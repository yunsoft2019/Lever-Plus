# RL训练失败分析报告（2025-12-16）

## 问题概述

使用 `hard_plus_gtprob_plus_rel` reward模式训练的v3模型（`v3_RandSampler_v4`）在200样本测试中，**平均性能下降4.57%**，多Shot场景下降尤其明显。

---

## 推理结果对比

### 200样本测试结果

| Shot | Baseline (12-13) | 新模型 (v3_RandSampler_v4) | 差异 | 变化率 |
|------|------------------|---------------------------|------|--------|
| **1** | 57.2% | 57.1% | -0.1% | -0.17% |
| **2** | 56.3% | 51.7% | **-4.6%** | **-8.17%** |
| **3** | 54.9% | 47.1% | **-7.8%** | **-14.21%** |
| **4** | 54.9% | 49.1% | **-5.8%** | **-10.56%** |

**平均下降：-4.57%**

### 关键发现

1. **Shot 1基本持平**：单候选选择能力未受影响
2. **Shot 2-4显著下降**：多候选排序能力严重退化
3. **下降幅度随Shot数增加**：Shot 3下降最严重（-14.2%）

---

## 训练过程回顾

### RCE阶段（正常）

| Epoch | Train Loss | Val Loss | 状态 |
|-------|-----------|----------|------|
| 1 | 17.54 | 17.48 | ✓ |
| 5 | 15.74 | 15.15 | ✓ |

- 训练和验证损失同步下降
- 无过拟合迹象

### GRPO阶段（问题）

| Epoch | Train Loss | Val Loss | PPO Loss | KL | Beta | 状态 |
|-------|-----------|----------|----------|-----|------|------|
| 1 | 199.63 | 17.15 | 1.61 | 20.68 | 10.0 | ⚠️ |
| 5 | 0.007 | 17.47 | 0.007 | 0.029 | 0.001 | ❌ |
| 10 | -0.001 | 17.50 | -0.001 | 0.030 | 0.001 | ❌ |

**核心问题**：
- ✅ 训练损失：199.6 → -0.001（过度优化）
- ❌ 验证损失：17.15 → 17.50（**上升0.35**，过拟合）
- ❌ KL散度：20.68 → 0.03（**太小**，学习不充分）
- ❌ Beta权重：10.0 → 0.001（**约束失效**）

---

## 根本原因分析

### 1. 严重过拟合

**现象**：
- 训练损失接近0，但验证损失上升
- 训练/验证损失差距巨大（-0.001 vs 17.50）

**原因**：
- GRPO训练10个epoch可能过多
- 模型过度拟合训练数据的reward分布
- 在测试集上泛化能力下降

**证据**：
- 验证损失在第2个epoch就开始上升（17.15 → 17.40）
- 后续8个epoch持续上升，但训练损失持续下降

### 2. KL约束失效

**现象**：
- KL散度从20.68降到0.03（**下降99.85%**）
- Beta权重从10.0降到0.001（**下降99.99%**）

**原因**：
- `kl_target_min=0.01` 设置太小
- 自适应调整机制认为KL已经足够小，不断降低Beta
- 模型几乎没有偏离SFT模型，没有学到新策略

**影响**：
- 模型可能只是在训练集上"记住"了reward分布
- 没有学到真正有用的策略改进
- 在测试集上表现变差

### 3. Reward设计问题

**使用的Reward模式**：
```python
reward = 2.0 * hard + 1.0 * gt_prob + 0.1 * (1-hard) * rel
```

**可能的问题**：
1. **权重不平衡**：hard权重2.0可能过大，导致模型过度关注correctness
2. **Relevance权重太小**：0.1的权重可能无法有效shaping负样本
3. **多Shot场景**：Reward设计可能没有体现候选间的相对排序关系
4. **Reward分布**：如果RL数据中reward分布不合理，模型可能学到错误策略

### 4. 多Shot能力退化

**为什么Shot 2-4下降更明显？**

1. **单样本优化 vs 全局排序**：
   - GRPO训练时，模型针对每个候选独立优化reward
   - 可能忽略了候选间的相对关系
   - Shot 2-4需要模型有更好的排序能力

2. **Reward信号不足**：
   - 当前reward设计可能没有体现"相对质量"
   - 模型可能无法区分"好"和"更好"的候选

3. **训练数据分布**：
   - 如果训练数据中多候选场景的reward分布不合理
   - 模型可能学到错误的排序策略

---

## 对比历史实验

| 实验 | 平均差异 | 结论 |
|------|---------|------|
| Balanced (12-13) | -0.27% | 基本持平 |
| **本次实验** | **-4.57%** | **显著下降** |

**关键差异**：
- Balanced实验：只做RCE训练（5 epochs），未进行GRPO
- 本次实验：RCE + GRPO（10 epochs）

**结论**：GRPO训练可能引入了额外的过拟合问题

---

## 改进建议

### 🔴 立即尝试（优先级最高）

#### 1. 测试RCE-only模型

```bash
# 使用RCE阶段的checkpoint进行推理
bash scripts/inference_v3_gpu4.sh 200
# 但修改checkpoint路径为：rce_epoch5.pt
```

**原因**：
- RCE阶段训练正常，无过拟合
- 可能性能比GRPO模型更好

#### 2. 使用早停策略

**最佳停止点**：第2个epoch（验证损失最低：17.15）

```bash
# 修改训练脚本，GRPO只训练2个epoch
--grpo_epochs 2
```

---

### 🟡 短期优化（1-2天）

#### 3. 调整KL约束参数

**当前设置**：
- `kl_target_min=0.01`（太小）
- `kl_target_max=0.1`

**建议修改**：
```python
kl_target_min=0.05  # 从0.01提高到0.05
kl_target_max=0.2    # 从0.1提高到0.2
```

**原因**：
- 允许模型有更大的策略变化
- 避免Beta权重过早降到0

#### 4. 减少GRPO训练轮数

**当前**：10 epochs

**建议**：3-5 epochs

**原因**：
- 验证损失在第2个epoch就开始上升
- 3-5个epoch可能已经足够

#### 5. 简化Reward模式

**当前**：`hard_plus_gtprob_plus_rel`（复杂）

**建议尝试**：
- `hard_plus_soft`：更简单，历史表现稳定
- `hard_plus_gtprob`：去掉relevance，减少复杂度

---

### 🟢 中期优化（1周）

#### 6. 分析Reward分布

```python
# 检查RL数据中reward的分布
python scripts/analyze_reward_distribution.py \
    --rl_data results/okvqa/generated_data/rl_data_RandSampler_v4_strictEval.json
```

**检查点**：
- Reward分布是否合理？
- 正负样本reward是否有明显区分？
- 多Shot场景的reward分布是否合理？

#### 7. 改进Reward设计

**考虑引入相对排序reward**：
- 不仅看单个候选的reward
- 还要考虑候选间的相对关系
- 例如：使用rank-based reward

#### 8. 数据质量检查

**检查RL数据**：
- `vqa_correct`标注是否准确？
- `vqa_acc_score`计算是否正确？
- `vqa_gt_prob`和`vqa_rel_score`是否合理？

---

## 具体行动计划

### 第一步：验证RCE模型性能（今天）

```bash
# 1. 修改推理脚本，使用RCE checkpoint
export LEVER_LM_CHECKPOINT_PATH="./results/okvqa/model_cpk/v3_RandSampler_v4/rce_epoch5.pt"
bash scripts/inference.sh vqa okvqa_local 4 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B v3 200
```

**预期**：如果RCE模型性能更好，说明GRPO训练有问题

### 第二步：重新训练（使用改进配置）

```bash
# 修改训练脚本
bash scripts/train_v3_with_rl_data_v4.sh 4 RandSampler v4_strictEval hard_plus_soft
# 但需要修改：
# - grpo_epochs: 10 → 3
# - kl_target_min: 0.01 → 0.05
# - reward_mode: hard_plus_gtprob_plus_rel → hard_plus_soft
```

### 第三步：对比分析

- RCE-only vs GRPO (3 epochs) vs GRPO (10 epochs)
- 找出最佳配置

---

## 经验教训

### 1. 过拟合是RL训练的最大敌人

- **现象**：训练损失下降但验证损失上升
- **解决**：早停、更强的正则化、更少的训练轮数

### 2. KL约束的重要性

- **现象**：KL散度过小，Beta权重失效
- **解决**：合理设置KL目标范围，避免过早降低Beta

### 3. Reward设计的复杂性

- **现象**：复杂的reward模式可能引入噪声
- **解决**：从简单开始，逐步增加复杂度

### 4. 多Shot场景的特殊性

- **现象**：单Shot表现正常，多Shot明显下降
- **解决**：Reward设计需要考虑候选间的相对关系

---

## 结论

本次RL训练失败的主要原因是：

1. **严重过拟合**：GRPO训练10个epoch过多，验证损失持续上升
2. **KL约束失效**：KL散度过小，Beta权重过早降到0
3. **Reward设计可能不合理**：复杂的reward模式可能引入噪声
4. **多Shot能力退化**：Reward设计没有体现候选间的相对关系

**建议**：
1. 立即测试RCE-only模型
2. 使用早停策略（2-3个epoch）
3. 调整KL约束参数
4. 简化Reward模式

---

---

## 🔴 重要更新：RCE-only模型测试结果（2025-12-16 09:32）

### RCE-only模型推理结果（200样本）

| Shot | Baseline | RCE-only | GRPO | RCE vs Base | GRPO vs Base | GRPO vs RCE |
|------|----------|----------|------|-------------|--------------|-------------|
| **1** | 57.2% | 50.6% | 57.1% | **-6.6%** | -0.1% | **+6.5%** |
| **2** | 56.3% | 46.4% | 51.7% | **-9.9%** | -4.6% | **+5.3%** |
| **3** | 54.9% | 45.7% | 47.1% | **-9.2%** | -7.8% | **+1.4%** |
| **4** | 54.9% | 43.0% | 49.1% | **-11.9%** | -5.8% | **+6.1%** |

**平均差异**：
- RCE-only vs Baseline: **-9.45%** ❌
- GRPO vs Baseline: **-4.57%** ❌
- GRPO vs RCE-only: **+4.83%** ✅

### 🔥 关键发现

**问题的根源在RCE阶段，而不是GRPO阶段！**

1. **RCE-only模型比Baseline差很多**（平均-9.45%）
   - 说明RCE训练本身就有严重问题
   - 不是GRPO训练导致的性能下降

2. **GRPO模型比RCE-only模型好**（平均+4.83%）
   - GRPO训练虽然引入了过拟合，但至少比RCE-only好
   - 说明GRPO训练有一定效果，但不足以弥补RCE阶段的问题

3. **两个模型都比Baseline差**
   - RCE-only: -9.45%
   - GRPO: -4.57%
   - 说明整个RL训练流程都有问题

### 修正后的原因分析

#### 1. RCE训练阶段的问题（主要问题）

**可能的原因**：
- **RL数据质量问题**：
  - Reward分布可能不合理
  - `vqa_correct`、`vqa_acc_score`、`vqa_gt_prob`、`vqa_rel_score`标注可能有问题
  - 训练数据与测试数据分布不匹配

- **RCE训练策略问题**：
  - Reward归一化可能有问题
  - 温度调度（1.7 → 0.5）可能不合适
  - 训练轮数（5 epochs）可能不够或过多

- **模型初始化问题**：
  - 从SFT模型加载权重可能有问题
  - 模型架构可能不适合当前任务

#### 2. GRPO训练阶段的问题（次要问题）

**虽然GRPO比RCE-only好，但仍然存在过拟合**：
- 验证损失上升（17.15 → 17.50）
- KL约束失效（Beta权重降到0.001）
- 但至少比RCE-only模型好4.83%

### 修正后的改进建议

#### 🔴 优先级最高：检查RCE训练

1. **检查RL数据质量**
   ```bash
   # 分析RL数据的reward分布
   python scripts/analyze_rl_data_quality.py \
       --rl_data results/okvqa/generated_data/rl_data_RandSampler_v4_strictEval.json
   ```

2. **对比RCE不同epoch的性能**
   ```bash
   # 测试rce_epoch1-5的性能，找出最佳epoch
   for epoch in 1 2 3 4 5; do
       export LEVER_LM_CHECKPOINT_PATH="./results/okvqa/model_cpk/v3_RandSampler_v4/rce_epoch${epoch}.pt"
       bash scripts/inference.sh vqa okvqa_local 4 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B v3 200
   done
   ```

3. **检查Baseline模型配置**
   - 对比Baseline（v2）和RCE（v3）的训练配置差异
   - 确认是否有配置错误

#### 🟡 优先级中等：调整RCE训练策略

4. **调整Reward归一化**
   - 检查RCE训练时reward的归一化方式
   - 尝试不同的归一化策略

5. **调整温度调度**
   - 当前：1.7 → 0.5（可能变化太快）
   - 尝试：更温和的温度调度

6. **调整训练轮数**
   - 当前：5 epochs
   - 尝试：3 epochs或7 epochs

---

## 更新记录

- **2025-12-16 09:32**：
  - ✅ 完成RCE-only模型推理测试
  - 🔥 **重要发现**：问题的根源在RCE阶段，而不是GRPO阶段
  - RCE-only模型比Baseline差9.45%，GRPO模型比RCE-only好4.83%
  - 修正原因分析和改进建议
- **2025-12-16 08:00**：
  - 完成v3_RandSampler_v4模型推理测试
  - 发现性能显著下降（平均-4.57%）
  - 分析训练过程，找出过拟合和KL约束失效问题
  - 提出改进建议和行动计划

