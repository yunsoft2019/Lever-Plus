# 方案六训练说明（方案 5 + 方案 6）

> 训练日期：2025-12-23  
> 训练配置：方案五（关闭 Rank Normalization）+ 方案六（增大学习率）

---

## 一、训练配置

### 1.1 核心参数

| 参数 | 值 | 说明 |
|------|-----|------|
| **USE_RANK_ADVANTAGE** | `false` | 方案五：关闭 Rank Normalization，使用 Z-score 归一化 |
| **GRPO_LR** | `5e-5` | 方案六：从默认 5e-6 提升 **10 倍** |
| **KL_BETA** | `0.1` | 保持较小的 KL 约束 |
| **GRPO_EPOCHS** | `50` | 训练 50 个 epochs |
| **RCE_EPOCHS** | `5` | RCE 预热（默认） |

### 1.2 训练命令

```bash
conda activate lever_env
export USE_RANK_ADVANTAGE=false
export GRPO_LR=5e-5              # 从 5e-6 提升 10 倍
export GRPO_EPOCHS=50
export KL_BETA=0.1
bash scripts/train_v3.sh vqa okvqa_local 0 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B 2>&1 | tee train_plan5_plan6_50epochs.log
```

### 1.3 预期效果

根据 `方案五优化分析.md`：

| 指标 | 方案五（当前） | 方案六（预期） | 提升 |
|------|---------------|---------------|------|
| **平均提升** | +0.38% | **1-2%** | +0.62-1.62% |
| **PPO Loss** | 0.00007 | **0.01-0.1** | 提升 100-1000 倍 |
| **Adv Std** | 0.0062 | 可能仍然较小 | 但学习率增大后效果更好 |

---

## 二、训练监控

### 2.1 日志文件

- **训练日志**：`train_plan5_plan6_50epochs.log`
- **输出目录**：`./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/`

### 2.2 关键指标监控

训练过程中需要关注：

1. **PPO Loss**：
   - 方案五：0.00007（太小）
   - 方案六预期：0.01-0.1（提升 100-1000 倍）

2. **Adv Std**：
   - 方案五：0.0062（仍然很小）
   - 方案六：可能仍然较小，但学习率增大后能产生更大的参数更新

3. **Val Loss**：
   - 用于选择最优 checkpoint

4. **KL Divergence**：
   - 应该保持在合理范围内（< 0.1）

### 2.3 监控命令

```bash
# 查看训练进度
tail -f train_plan5_plan6_50epochs.log

# 查看训练指标（每行格式：epoch train_loss val_loss ppo_loss kl adv_std adv_max beta）
grep -E "^\s+\d+\s+" train_plan5_plan6_50epochs.log | tail -20

# 检查训练是否还在运行
ps aux | grep grpo_post_train | grep -v grep
```

---

## 三、训练完成后

### 3.1 查找最优 Checkpoint

使用 `scripts/find_best_checkpoint.py`：

```bash
python scripts/find_best_checkpoint.py \
    --checkpoint_dir ./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct \
    --log_file train_plan5_plan6_50epochs.log \
    --strategy val_loss \
    --show_all
```

### 3.2 转换 Checkpoint 格式

最优 checkpoint 需要转换为 v2 格式用于推理：

```bash
# 假设最优 checkpoint 是 grpo_epochX.pt
python scripts/convert_v3_to_v2_format.py \
    --v3_ckpt ./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/grpo_epochX.pt
```

### 3.3 运行推理

使用最优 checkpoint 进行推理（800 samples）：

```bash
export LEVER_LM_CHECKPOINT_PATH=./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/grpo_epochX_v2format.ckpt
bash scripts/inference.sh vqa okvqa_local 0 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B v3 800
```

---

## 四、与方案五对比

### 4.1 方案五结果（参考）

**训练指标**：
- Val Loss: 5.14132 (Epoch 2)
- PPO Loss: -0.00007
- KL: 0.04286
- Adv Std: 0.0062

**推理结果（800 samples）**：
- Shot 1: 50.15% (vs Baseline 48.55%) → **+1.60%**
- Shot 2: 48.33% (vs Baseline 47.75%) → **+0.58%**
- Shot 3: 47.40% (vs Baseline 48.15%) → -0.75%
- Shot 4: 47.52% (vs Baseline 47.45%) → +0.07%
- **平均提升：+0.38%**

### 4.2 方案六预期改进

1. **PPO Loss 显著增大**：
   - 从 0.00007 提升到 0.01-0.1
   - 说明参数更新更有效

2. **平均提升进一步提升**：
   - 从 +0.38% 提升到 **1-2%**
   - 预期 Shot 1-4 都有更好的表现

3. **训练更稳定**：
   - 学习率增大后，即使 advantage 信号弱，也能产生更大的参数更新

---

## 五、后续计划

### 5.1 如果方案六效果良好（平均提升 > 1%）

✅ **继续优化**：
- 尝试方案 5 + 6 + 8（Reward Shaping）
- 预期进一步提升到 2-5%

### 5.2 如果方案六效果不够（平均提升 < 1%）

🔄 **尝试其他方案**：
- 方案 5 + 6 + 8（Reward Shaping）
- 方案 5 + 6 + 9（Curriculum Learning）

### 5.3 如果方案六效果很差（甚至下降）

⚠️ **重新评估**：
- 检查训练指标（PPO Loss, KL, Adv Std）
- 考虑调整学习率（如 1e-4 或 2e-5）
- 或考虑方案十（专注优化 RCE）

---

## 六、更新记录

- **2025-12-23**：
  - 启动方案六训练（方案 5 + 方案 6）
  - 配置：USE_RANK_ADVANTAGE=false, GRPO_LR=5e-5, GRPO_EPOCHS=50, KL_BETA=0.1
  - 预期平均提升：1-2%（vs Baseline）

---

*训练启动时间：2025-12-23*


