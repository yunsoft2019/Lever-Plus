# Lever-Plus 模型正确率对比

## 测试配置
- **数据集**: OKVQA
- **推理模型**: Qwen2.5-VL-3B-Instruct
- **采样器**: RandSampler

---

## 1. Shot 100（测试100条数据）

### v0 vs v1 vs v2 vs v2_lora vs v3 推理结果对比

| Shot Num | v0 结果 | v1 结果 | v2 结果 | v2_lora 结果 | v3 结果 | 最佳版本 | v3 vs v0 | v3 vs v1 | v3 vs v2 |
|----------|---------|---------|---------|--------------|---------|----------|----------|---------|---------|
| **1** | **66.6%** | 64.8% | 63.8% | 63.8% | **65.0%** | **v0** | -1.6% | +0.2% | **+1.2%** |
| **2** | 60.6% | 64.4% | 63.8% | 63.8% | 57.0% | **v1** | -3.6% | -7.4% | -6.8% |
| **3** | 55.4% | 59.8% | **62.8%** | **62.8%** | 58.2% | **v2/v2_lora** | +2.8% | -1.6% | -4.6% |
| **4** | 55.8% | 60.8% | **61.4%** | **61.4%** | 58.4% | **v2/v2_lora** | +2.6% | -2.4% | -3.0% |

### 版本演进趋势

| Shot Num | v0 → v1 变化 | v1 → v2 变化 | 总体趋势 |
|----------|-------------|-------------|----------|
| **1** | -1.8% | -1.0% | 下降 |
| **2** | +3.8% | -0.6% | 先升后略降 |
| **3** | +4.4% | **+3.0%** | **持续提升** |
| **4** | +5.0% | +0.6% | **持续提升** |

### 关键发现

1. **v0 在 shot_num=1 时表现最好**
   - shot_num=1: v0 66.6% 最高，v3 65.0%（+1.2% vs v2），v1 64.8%，v2/v2_lora 63.8%

2. **v1 在 shot_num=2 时表现最好**
   - shot_num=2: v1 64.4% 最高，v2/v2_lora 63.8%，v0 60.6%，v3 57.0%

3. **v2/v2_lora 在 shot_num≥3 时表现最好**
   - shot_num=3: v2/v2_lora 62.8% vs v0 55.4%（+7.4%），vs v1 59.8%（+3.0%），vs v3 58.2%（+4.6%）
   - shot_num=4: v2/v2_lora 61.4% vs v0 55.8%（+5.6%），vs v1 60.8%（+0.6%），vs v3 58.4%（+3.0%）

4. **v2_lora 与 v2 性能完全一致**
   - v2_lora 在所有 shot_num 下的结果与 v2 完全相同
   - 说明 LoRA 微调 CLIP 不影响推理性能，同时减少了可训练参数（仅训练 1.22% 的文本编码器参数和 1.33% 的图像编码器参数）

5. **v3 的 GRPO 强化学习效果**
   - v3 在 shot_num=1 时表现较好（65.0%），优于 v2/v2_lora（+1.2%），但低于 v0（-1.6%）
   - v3 在 shot_num=2 时表现较差（57.0%），低于所有其他版本
   - v3 在 shot_num≥3 时表现低于 v2/v2_lora，说明 GRPO 训练在中等 shot 数时效果有限

6. **版本演进特点**
   - v0 → v1：在 shot_num≥2 时提升明显
   - v1 → v2：在 shot_num≥3 时进一步提升
   - v2 → v3：在 shot_num=1 时略有提升（+1.2%），但在 shot_num≥2 时表现下降
   - v2 的多层 Cross-Attention 在更多 shot 时更有效
   - v3 的 GRPO 强化学习在 shot_num=1 时有一定效果，但在 shot_num≥2 时效果不佳

### 结论

- **如果主要使用 shot_num=1**：选择 **v0**（66.6%）或 **v3**（65.0%）
- **如果主要使用 shot_num=2**：选择 **v1**（64.4%）或 **v2/v2_lora**（63.8%）
- **如果主要使用 shot_num≥3**：选择 **v2** 或 **v2_lora**（shot_num=3: 62.8%，shot_num=4: 61.4%）
  - **推荐 v2_lora**：性能与 v2 完全相同，但训练效率更高（仅训练约 1.2% 的 CLIP 参数）

**版本选择建议**：
- **v2/v2_lora**：多层 Cross-Attention（3层）在 shot 数较多时能更好地建模 query 与 candidates 的交互，因此在 shot_num≥3 时表现最好。
- **v3**：GRPO 强化学习训练在 shot_num=1 时有一定效果（65.0%，优于 v2），但在 shot_num≥2 时表现不佳，需要进一步优化。
- **v2_lora**：性能与 v2 完全一致，但训练效率更高，推荐在生产环境中使用。

**v2_lora 优势**：
- 性能与 v2 完全一致
- 训练参数大幅减少（CLIP 仅训练 1.22% 文本编码器参数和 1.33% 图像编码器参数）
- 训练速度更快，内存占用更少
- 推荐在生产环境中使用 v2_lora 版本

---

## 2. Shot 200（测试200条数据）

### v0 vs v1 vs v2 vs v2_lora vs v3_1layer 推理结果对比

| Shot Num | v0 结果 | v1 结果 | v2 结果 | v2_lora 结果 | v3_1layer 结果 | 最佳版本 | v3_1layer vs v0 | v3_1layer vs v1 | v3_1layer vs v2 |
|----------|---------|---------|---------|--------------|----------------|----------|-----------------|-----------------|-----------------|
| **1** | 57.2% | 57.8% | 56.7% | 56.9% | **59.3%** | **v3_1layer** | **+2.1%** | **+1.5%** | **+2.6%** |
| **2** | 52.1% | 55.9% | 56.1% | 56.3% | **57.1%** | **v3_1layer** | **+5.0%** | **+1.2%** | **+1.0%** |
| **3** | 48.6% | 53.6% | **55.5%** | **55.5%** | 53.6% | **v2/v2_lora** | +5.0% | 0.0% | -1.9% |
| **4** | 46.9% | 54.9% | 54.7% | **54.9%** | 54.2% | **v2_lora** | +7.3% | -0.7% | -0.5% |

**🎉 重大突破：v3_1layer 在 shot_num=1,2 时超越所有版本！**
- shot_num=1: v3_1layer **59.3%**，比 v2 高 **+2.6%**，比 v1 高 **+1.5%** ✓
- shot_num=2: v3_1layer **57.1%**，比 v2 高 **+1.0%**，比 v2_lora 高 **+0.8%** ✓
- shot_num=3: v3_1layer 53.6%，比 v2 低 -1.9%（v2/v2_lora 仍最佳）
- shot_num=4: v3_1layer 54.2%，比 v2_lora 低 -0.7%（v2_lora 仍最佳）

### v0 vs v1 对比分析

| Shot Num | v0 结果 | v1 结果 | v1 vs v0 | 提升幅度 |
|----------|---------|---------|----------|----------|
| **1** | 57.2% | **57.8%** | +0.6% | 小幅提升 |
| **2** | 52.1% | **55.9%** | **+3.8%** | **明显提升** |
| **3** | 48.6% | **53.6%** | **+5.0%** | **显著提升** |
| **4** | 46.9% | **54.9%** | **+8.0%** | **大幅提升** |

**关键发现**：
- v1 在所有 shot_num 下都优于 v0
- shot_num 越大，v1 相对于 v0 的优势越明显（shot_num=4 时提升 8.0%）
- v1 的 Bi-Encoder 架构在更大测试集上表现更稳定

### v1 vs v2 对比分析

| Shot Num | v1 结果 | v2 结果 | v2 vs v1 | 提升幅度 |
|----------|---------|---------|----------|----------|
| **1** | **57.8%** | 56.7% | -1.1% | 略降 |
| **2** | 55.9% | **56.1%** | +0.2% | 小幅提升 |
| **3** | 53.6% | **55.5%** | **+1.9%** | **明显提升** |
| **4** | **54.9%** | 54.7% | -0.2% | 略降 |

**关键发现**：
- v2 在 shot_num=2 和 shot_num=3 时表现最好
- v2 在 shot_num=3 时超过 v1（55.5% vs 53.6%，+1.9%）
- v2 的多层 Cross-Attention 在 shot_num=2-3 时效果最佳
- v1 在 shot_num=1 和 shot_num=4 时略优于 v2

### v2 vs v2_lora 对比分析

| Shot Num | v2 结果 | v2_lora 结果 | v2_lora vs v2 | 差异 |
|----------|---------|--------------|---------------|------|
| **1** | 56.7% | **56.9%** | +0.2% | v2_lora 略优 |
| **2** | 56.1% | **56.3%** | +0.2% | v2_lora 略优 |
| **3** | **55.5%** | **55.5%** | 0.0% | 完全相同 |
| **4** | 54.7% | **54.9%** | +0.2% | v2_lora 略优 |

**关键发现**：
- v2_lora 与 v2 性能非常接近，在所有 shot_num 下差异不超过 0.2%
- shot_num=3 时两者完全相同（55.5%）
- v2_lora 在 shot_num=1, 2, 4 时略优于 v2（+0.2%）
- 说明 LoRA 微调 CLIP 不影响推理性能，甚至略有提升
- v2_lora 的优势：性能与 v2 相当或略优，但训练参数大幅减少（仅训练约 1.2% 的 CLIP 参数）

### v2 vs v3 对比分析

| Shot Num | v2 结果 | v3 结果（优化前） | v3 结果（优化后） | v3优化后 vs v2 | 差异 |
|----------|---------|------------------|------------------|---------------|------|
| **1** | 56.7% | 57.3% | 51.6% | -5.1% | v2 明显优于 v3 |
| **2** | **56.1%** | 50.2% | 48.0% | -8.1% | v2 明显优于 v3 |
| **3** | **55.5%** | 48.3% | 47.2% | -8.3% | v2 明显优于 v3 |
| **4** | **54.7%** | 49.6% | 46.0% | -8.7% | v2 明显优于 v3 |

**关键发现**：
- **优化后的 v3 表现更差**，在所有 shot_num 下都低于优化前的 v3 和 v2
- v3 优化后（RCE_EPOCHS=3, GRPO_EPOCHS=8）在 shot_num=1 时从 57.3% 降到 51.6%（-5.7%）
- v3 优化后在 shot_num≥2 时表现进一步下降，与 v2 的差距扩大到 8%+
- **说明减少训练轮数（3+8 epochs）可能不够，或者 GRPO 训练方法本身存在问题**
- 可能的原因：
  1. 训练轮数太少，模型没有充分学习
  2. Reward 设计可能不适合这个任务
  3. GRPO 方法可能不适合 pointer selector 这种场景
  4. 训练数据质量或分布可能有问题

### v0 vs v1 vs v2 vs v2_lora vs v3 综合对比

| Shot Num | v0 结果 | v1 结果 | v2 结果 | v2_lora 结果 | v3 结果 | 最佳版本 | v3 vs v0 | v3 vs v1 | v3 vs v2 |
|----------|---------|---------|---------|--------------|---------|----------|----------|---------|---------|
| **1** | 57.2% | **57.8%** | 56.7% | 56.9% | 51.6% | **v1** | -5.6% | -6.2% | -5.1% |
| **2** | 52.1% | **55.9%** | 56.1% | 56.3% | 48.0% | **v1** | -4.1% | -7.9% | -8.1% |
| **3** | 48.6% | 53.6% | **55.5%** | **55.5%** | 47.2% | **v2/v2_lora** | -1.4% | -6.4% | -8.3% |
| **4** | 46.9% | **54.9%** | 54.7% | **54.9%** | 46.0% | **v1/v2_lora** | -0.9% | -8.9% | -8.7% |

**关键发现**：
- **v1 在 shot_num=1, 2, 4 时表现最佳**
  - shot_num=1: v1 57.8% 最高
  - shot_num=2: v1 55.9% 最高，超过 v2_lora（+0.4%）、v2（+0.2%）、v3（+5.7%）
  - shot_num=4: v1/v2_lora 54.9% 最高
- **v2/v2_lora 在 shot_num=3 时表现最佳**（55.5%），超过 v1（+1.9%）、v3（+7.2%）
- **v3 表现很差**，在所有 shot_num 下都远低于 v1 和 v2/v2_lora
  - shot_num=1: v3 优化后 51.6%，比 v1 低 6.2%，比 v2 低 5.1%
  - shot_num≥2: v3 优化后表现进一步下降，与 v1/v2 的差距扩大到 8%+
- **优化后的 v3（RCE_EPOCHS=3, GRPO_EPOCHS=8）表现比优化前更差**
- v3 的 GRPO 强化学习训练在当前配置下效果很差，可能需要：
  1. 重新审视 GRPO 方法是否适合 pointer selector 任务
  2. 检查 reward 设计和训练数据质量
  3. 尝试完全不同的训练策略或参数
- **强烈建议：优先使用 v1 或 v2/v2_lora 版本，v3 需要进一步研究和优化**

---

## 3. Shot 400（测试400条数据）

### v0 vs v1 vs v2 vs v2_lora vs v3 推理结果对比

| Shot Num | v0 结果 | v1 结果 | v2 结果 | v2_lora 结果 | v3 结果 | 最佳版本 | v2_lora vs v0 | v2_lora vs v1 | v2_lora vs v2 | v2_lora vs v3 |
|----------|---------|---------|---------|--------------|---------|----------|---------------|--------------|--------------|--------------|
| **1** | 52.75% | **53.55%** | 53.1% | 52.8% | 52.95% | **v1** | +0.05% | -0.75% | -0.3% | -0.15% |
| **2** | 47.65% | 50.15% | **51.3%** | 51.4% | 47.45% | **v2_lora** | **+3.75%** | +1.25% | +0.1% | **+3.95%** |
| **3** | 45.1% | 49.2% | **51.75%** | **51.75%** | 45.5% | **v2/v2_lora** | **+6.65%** | **+2.55%** | 0.0% | **+6.25%** |
| **4** | 44.0% | **50.0%** | 50.35% | 50.45% | 45.25% | **v2_lora** | **+6.45%** | +0.45% | +0.1% | **+5.2%** |

### v0 vs v1 对比分析

| Shot Num | v0 结果 | v1 结果 | v1 vs v0 | 提升幅度 |
|----------|---------|---------|----------|----------|
| **1** | 52.75% | **53.55%** | +0.8% | 小幅提升 |
| **2** | 47.65% | **50.15%** | **+2.5%** | **明显提升** |
| **3** | 45.1% | **49.2%** | **+4.1%** | **显著提升** |
| **4** | 44.0% | **50.0%** | **+6.0%** | **大幅提升** |

**关键发现**：
- v1 在所有 shot_num 下都优于 v0
- shot_num 越大，v1 相对于 v0 的优势越明显（shot_num=4 时提升 6.0%）
- v1 的 Bi-Encoder 架构在 400 条测试数据上表现更稳定
- 与 200 条测试数据的结果趋势一致，v1 的优势在更大测试集上依然保持

### v1 vs v2 对比分析

| Shot Num | v1 结果 | v2 结果 | v2 vs v1 | 提升幅度 |
|----------|---------|---------|----------|----------|
| **1** | **53.55%** | 53.1% | -0.45% | 略降 |
| **2** | 50.15% | **51.3%** | **+1.15%** | **明显提升** |
| **3** | 49.2% | **51.75%** | **+2.55%** | **显著提升** |
| **4** | 50.0% | **50.35%** | +0.35% | 小幅提升 |

**关键发现**：
- v2 在 shot_num≥2 时表现优于 v1
- v2 在 shot_num=3 时表现最佳（51.75% vs v1 49.2%，+2.55%）
- v2 的多层 Cross-Attention 在 shot_num≥2 时效果最佳
- v1 在 shot_num=1 时略优于 v2（53.55% vs 53.1%）

### v2 vs v2_lora 对比分析

| Shot Num | v2 结果 | v2_lora 结果 | v2_lora vs v2 | 差异 |
|----------|---------|--------------|---------------|------|
| **1** | **53.1%** | 52.8% | -0.3% | v2 略优于 v2_lora |
| **2** | 51.3% | **51.4%** | +0.1% | v2_lora 略优于 v2 |
| **3** | **51.75%** | **51.75%** | 0.0% | 完全相同 |
| **4** | 50.35% | **50.45%** | +0.1% | v2_lora 略优于 v2 |

**关键发现**：
- v2_lora 与 v2 性能非常接近，在所有 shot_num 下差异不超过 0.3%
- shot_num=3 时两者完全相同（51.75%）
- v2_lora 在 shot_num=2 和 shot_num=4 时略优于 v2（+0.1%）
- 说明 LoRA 微调 CLIP 不影响推理性能，甚至略有提升
- v2_lora 的优势：性能与 v2 相当或略优，但训练参数大幅减少（仅训练约 1.2% 的 CLIP 参数）

### v2 vs v3 对比分析

| Shot Num | v2 结果 | v3 结果 | v3 vs v2 | 差异 |
|----------|---------|---------|----------|------|
| **1** | 53.1% | 52.95% | -0.15% | v2 略优于 v3 |
| **2** | **51.3%** | 47.45% | -3.85% | v2 明显优于 v3 |
| **3** | **51.75%** | 45.5% | -6.25% | v2 明显优于 v3 |
| **4** | **50.35%** | 45.25% | -5.1% | v2 明显优于 v3 |

**关键发现**：
- v3 在所有 shot_num 下都低于 v2
- v3 在 shot_num=1 时略低于 v2（-0.15%）
- v3 在 shot_num≥2 时表现明显低于 v2（shot_num=2: -3.85%，shot_num=3: -6.25%，shot_num=4: -5.1%）
- v3 的 GRPO 强化学习训练在当前配置下效果不佳，需要进一步优化训练参数和流程

### v0 vs v1 vs v2 vs v2_lora vs v3 综合对比

| Shot Num | v0 结果 | v1 结果 | v2 结果 | v2_lora 结果 | v3 结果 | 最佳版本 | v2_lora vs v0 | v2_lora vs v1 | v2_lora vs v2 | v2_lora vs v3 |
|----------|---------|---------|---------|--------------|---------|----------|---------------|--------------|--------------|--------------|
| **1** | 52.75% | **53.55%** | 53.1% | 52.8% | 53.3% | **v1** | +0.05% | -0.75% | -0.3% | -0.5% |
| **2** | 47.65% | 50.15% | 51.3% | **51.4%** | 50.9% | **v2_lora** | **+3.75%** | +1.25% | +0.1% | +0.5% |
| **3** | 45.1% | 49.2% | **51.75%** | **51.75%** | 49.7% | **v2/v2_lora** | **+6.65%** | **+2.55%** | 0.0% | **+2.05%** |
| **4** | 44.0% | 50.0% | 50.35% | **50.45%** | 50.25% | **v2_lora** | **+6.45%** | +0.45% | +0.1% | +0.2% |

**关键发现**：
- **v2_lora 在 shot_num=2 和 shot_num=4 时表现最佳**，超过所有其他版本
- **v2/v2_lora 在 shot_num=3 时表现最佳**（51.75%），超过 v0、v1 和 v3
- **v1 在 shot_num=1 时表现最佳**（53.55%）
- v2_lora 与 v2 性能非常接近，在所有 shot_num 下差异不超过 0.3%
- v2_lora 相对于 v0 的提升幅度随 shot_num 增大而增大（shot_num=3 时提升 6.65%）
- v2_lora 的优势：性能与 v2 相当或略优，但训练参数大幅减少（仅训练约 1.2% 的 CLIP 参数）
- v3 的 GRPO 强化学习训练在当前配置下效果不佳，在所有 shot_num 下都低于 v2/v2_lora
- v3 在 shot_num≥2 时表现明显下降，说明 GRPO 训练参数或流程需要优化
- 建议：优先使用 v1 或 v2/v2_lora 版本，或重新调整 GRPO 训练参数

---

## 4. Shot 800（测试800条数据）

### v0 vs v1 vs v2 vs v2_lora vs v3 推理结果对比

| Shot Num | v0 结果 | v1 结果 | v2 结果 | v2_lora 结果 | v3 结果 | 最佳版本 | v2_lora vs v0 | v2_lora vs v1 | v2_lora vs v2 | v2_lora vs v3 |
|----------|---------|---------|---------|--------------|---------|----------|---------------|--------------|--------------|--------------|
| **1** | **50.95%** | 50.7% | 49.0% | 49.05% | 50.4% | **v0** | -1.9% | -1.65% | +0.05% | -1.35% |
| **2** | 47.08% | 47.33% | 48.08% | **48.38%** | 47.7% | **v2_lora** | **+1.3%** | +1.05% | +0.3% | +0.68% |
| **3** | 44.95% | 46.35% | **48.33%** | 48.52% | 46.6% | **v2_lora** | **+3.57%** | **+2.17%** | +0.19% | **+1.92%** |
| **4** | 43.77% | **47.4%** | 47.62% | 47.55% | 47.52% | **v2** | **+3.78%** | +0.15% | -0.07% | +0.03% |

### v0 vs v1 对比分析

| Shot Num | v0 结果 | v1 结果 | v1 vs v0 | 提升幅度 |
|----------|---------|---------|----------|----------|
| **1** | **50.95%** | 50.7% | -0.25% | 略降 |
| **2** | 47.08% | **47.33%** | +0.25% | 小幅提升 |
| **3** | 44.95% | **46.35%** | **+1.4%** | **明显提升** |
| **4** | 43.77% | **47.4%** | **+3.63%** | **显著提升** |

**关键发现**：
- v1 在 shot_num≥2 时表现优于 v0
- shot_num 越大，v1 相对于 v0 的优势越明显（shot_num=4 时提升 3.63%）
- v1 的 Bi-Encoder 架构在更大测试集上表现更稳定
- v0 在 shot_num=1 时略优于 v1（50.95% vs 50.7%）
- 与 200、400 条测试数据的结果趋势一致，v1 的优势在更大测试集上依然保持

### v1 vs v2 对比分析

| Shot Num | v1 结果 | v2 结果 | v2 vs v1 | 差异 |
|----------|---------|---------|----------|------|
| **1** | **50.7%** | 49.0% | -1.7% | v1 优于 v2 |
| **2** | 47.33% | **48.08%** | +0.75% | v2 优于 v1 |
| **3** | 46.35% | **48.33%** | **+1.98%** | v2 明显优于 v1 |
| **4** | **47.4%** | 47.62% | +0.22% | v2 略优于 v1 |

**关键发现**：
- v2 在 shot_num≥2 时表现优于 v1
- v2 在 shot_num=3 时表现最佳（48.33% vs v1 46.35%，+1.98%）
- v2 的多层 Cross-Attention 在 shot_num≥2 时效果最佳
- v1 在 shot_num=1 时优于 v2（50.7% vs 49.0%）
- v2 相对于 v0 的提升幅度随 shot_num 增大而增大（shot_num=4 时提升 3.85%）

### v0 vs v2 对比分析

| Shot Num | v0 结果 | v2 结果 | v2 vs v0 | 提升幅度 |
|----------|---------|---------|----------|----------|
| **1** | **50.95%** | 49.0% | -1.95% | 略降 |
| **2** | 47.08% | **48.08%** | **+1.0%** | **明显提升** |
| **3** | 44.95% | **48.33%** | **+3.38%** | **显著提升** |
| **4** | 43.77% | **47.62%** | **+3.85%** | **大幅提升** |

**关键发现**：
- v2 在 shot_num≥2 时表现优于 v0
- shot_num 越大，v2 相对于 v0 的优势越明显（shot_num=4 时提升 3.85%）
- v2 的多层 Cross-Attention 架构在更大测试集上表现更稳定
- v0 在 shot_num=1 时优于 v2（50.95% vs 49.0%）
- 与 200、400 条测试数据的结果趋势一致，v2 的优势在更大测试集上依然保持

### v2 vs v3 对比分析

| Shot Num | v2 结果 | v3 结果 | v3 vs v2 | 差异 |
|----------|---------|---------|----------|------|
| **1** | 49.0% | **50.4%** | +1.4% | v3 优于 v2 |
| **2** | **48.08%** | 47.7% | -0.38% | v2 优于 v3 |
| **3** | **48.33%** | 46.6% | -1.73% | v2 明显优于 v3 |
| **4** | **47.62%** | 47.52% | -0.1% | v2 略优于 v3 |

**关键发现**：
- v3 在 shot_num=1 时优于 v2（50.4% vs 49.0%，+1.4%）
- v2 在 shot_num≥2 时表现优于 v3
- v3 在 shot_num=3 时表现明显低于 v2（-1.73%）
- v3 的 GRPO 强化学习训练在 shot_num=1 时效果最佳，但在 shot_num≥2 时效果不如 v2
- 与 100、200、400 条测试数据的结果趋势一致，v3 在 shot_num=1 时表现较好，但在 shot_num≥2 时不如 v2

### v2 vs v2_lora 对比分析

| Shot Num | v2 结果 | v2_lora 结果 | v2_lora vs v2 | 差异 |
|----------|---------|--------------|---------------|------|
| **1** | **49.0%** | 49.05% | +0.05% | 几乎相同 |
| **2** | 48.08% | **48.38%** | +0.3% | v2_lora 略优于 v2 |
| **3** | 48.33% | **48.52%** | +0.19% | v2_lora 略优于 v2 |
| **4** | **47.62%** | 47.55% | -0.07% | v2 略优于 v2_lora |

**关键发现**：
- v2_lora 与 v2 性能非常接近，在所有 shot_num 下差异不超过 0.3%
- v2_lora 在 shot_num=2 和 shot_num=3 时略优于 v2（+0.3% 和 +0.19%）
- v2 在 shot_num=4 时略优于 v2_lora（+0.07%）
- 说明 LoRA 微调 CLIP 不影响推理性能，甚至略有提升
- v2_lora 的优势：性能与 v2 相当或略优，但训练参数大幅减少（仅训练约 1.2% 的 CLIP 参数）
- 与 400 条测试数据的结果趋势一致，v2_lora 与 v2 性能非常接近

### v0 vs v1 vs v2 vs v2_lora vs v3 综合对比

| Shot Num | v0 结果 | v1 结果 | v2 结果 | v2_lora 结果 | v3 结果 | 最佳版本 | v2_lora vs v0 | v2_lora vs v1 | v2_lora vs v2 | v2_lora vs v3 |
|----------|---------|---------|---------|--------------|---------|----------|---------------|--------------|--------------|--------------|
| **1** | **50.95%** | 50.7% | 49.0% | 49.05% | 50.4% | **v0** | -1.9% | -1.65% | +0.05% | -1.35% |
| **2** | 47.08% | 47.33% | 48.08% | **48.38%** | 47.7% | **v2_lora** | **+1.3%** | +1.05% | +0.3% | +0.68% |
| **3** | 44.95% | 46.35% | 48.33% | **48.52%** | 46.6% | **v2_lora** | **+3.57%** | **+2.17%** | +0.19% | **+1.92%** |
| **4** | 43.77% | **47.4%** | **47.62%** | 47.55% | 47.52% | **v2** | **+3.78%** | +0.15% | -0.07% | +0.03% |

**关键发现**：
- **v2_lora 在 shot_num=2 和 shot_num=3 时表现最佳**，超过所有其他版本
- **v2/v2_lora 在 shot_num=4 时表现最佳**（v2 47.62% vs v2_lora 47.55%）
- **v0 在 shot_num=1 时表现最佳**（50.95%）
- v2_lora 与 v2 性能非常接近，在所有 shot_num 下差异不超过 0.3%
- v2_lora 相对于 v0 的提升幅度随 shot_num 增大而增大（shot_num=4 时提升 3.78%）
- v2_lora 的优势：性能与 v2 相当或略优，但训练参数大幅减少（仅训练约 1.2% 的 CLIP 参数）
- v3 的 GRPO 强化学习训练在 shot_num=1 时效果最佳（50.4%，优于 v2），但在 shot_num≥2 时效果不如 v2/v2_lora
- v3 在 shot_num=3 时表现明显低于 v2_lora（-1.92%），说明 GRPO 训练在中等 shot 数时效果有限
- 与 100、200、400 条测试数据的结果趋势一致，v2_lora 在 shot_num≥2 时表现最佳，v3 在 shot_num=1 时表现较好

---

## 备注

- 所有结果基于相同的测试集（随机种子=42）
- v0: GPT2自回归语言模型
- v1: Bi-Encoder指针网络架构
- v2: v1 + 多层Cross-Attention（3层）
- v2_lora: v2 + LoRA微调CLIP（r=16, alpha=32，仅训练约1.2%的CLIP参数）
- v3: v2 + 离线强化学习（RCE预热 + GRPO后训练）

## 更新记录

- **2025-12-07**: 更新 v3 在 100 条测试数据上的推理结果（RandSampler, Qwen2.5-VL-3B-Instruct）
  - shot_num=1: 65.0%（之前: 63.8%）
  - shot_num=2: 57.0%（之前: 65.4%）
  - shot_num=3: 58.2%（之前: 60.8%）
  - shot_num=4: 58.4%（之前: 60.8%）
  - 检查点: `grpo_epoch25.pt`

- **2025-12-07**: 更新 v3 在 200 条测试数据上的推理结果（RandSampler, Qwen2.5-VL-3B-Instruct）
  - **优化前**（RCE_EPOCHS=25, GRPO_EPOCHS=25）:
    - shot_num=1: 57.3%（之前: 57.8%）
    - shot_num=2: 50.2%（之前: 56.9%）
    - shot_num=3: 48.3%（之前: 54.1%）
    - shot_num=4: 49.6%（之前: 54.9%）
    - 检查点: `grpo_epoch25.pt`
  - **优化后**（RCE_EPOCHS=3, GRPO_EPOCHS=8）:
    - shot_num=1: 51.6%（比优化前 -5.7%，比 v2 -5.1%）
    - shot_num=2: 48.0%（比优化前 -2.2%，比 v2 -8.1%）
    - shot_num=3: 47.2%（比优化前 -1.1%，比 v2 -8.3%）
    - shot_num=4: 46.0%（比优化前 -3.6%，比 v2 -8.7%）
    - 检查点: `grpo_epoch8.pt`
  - **重要发现**：优化后的参数表现**更差**，说明减少训练轮数不是正确的优化方向，GRPO 训练方法本身可能存在问题

- **2025-12-07**: 更新 v3 在 400 条测试数据上的推理结果（RandSampler, Qwen2.5-VL-3B-Instruct）
  - shot_num=1: 52.95%（之前: 53.3%）
  - shot_num=2: 47.45%（之前: 50.9%）
  - shot_num=3: 45.5%（之前: 49.7%）
  - shot_num=4: 45.25%（之前: 50.25%）
  - 检查点: `grpo_epoch25.pt`
  - 注意：v3 在 400 条数据上的表现进一步下降，所有 shot_num 下都低于 v2/v2_lora，说明当前 GRPO 训练配置存在问题
  - 优化建议：参见 `GRPO训练优化建议.md`

- **2025-12-08**: 🎉 **重大突破！v3_1layer 超越 v2！**
  - **问题根因**：之前 v3 训练时 v2 权重没有正确加载（参数名不匹配，`strict=False` 让错误静默）
  - **修复方案**：
    1. 修复权重加载映射：`cross_attn.xxx` → `cross_attn_layers.0.xxx`，`attn_norm.xxx` → `attn_norms.0.xxx`
    2. 添加 `--num_layers=1` 参数，使 v3 架构与 v2 一致（1层 Cross-Attention）
    3. 使用 v2 推理流程（`icl_inference.py` + `PointerSelectorAdapter`）确保公平对比
  - **v3_1layer 完整结果**（200条数据，使用 v2 推理流程）：
    - shot_num=1: **59.3%**（v2: 56.7%，**+2.6%**）✓ 超越 v2
    - shot_num=2: **57.1%**（v2: 56.1%，**+1.0%**）✓ 超越 v2
    - shot_num=3: 53.6%（v2: 55.5%，-1.9%）✗ 低于 v2
    - shot_num=4: 54.2%（v2: 54.7%，-0.5%）✗ 略低于 v2
  - 检查点: `./results/okvqa/model_cpk/v3_1layer/grpo_epoch10.pt`
  - v2格式: `./results/okvqa/model_cpk/v3_1layer/grpo_epoch10_v2format.ckpt`
  - **结论**：
    - GRPO 强化学习在正确加载 v2 权重后确实有效
    - v3_1layer 在 shot_num=1,2 时超越 v2（最高提升 +2.6%）
    - v3_1layer 在 shot_num=3,4 时略低于 v2（可能因为 GRPO 训练数据是 shot_num=2 生成的）
    - **推荐**：shot_num≤2 时使用 v3_1layer，shot_num≥3 时使用 v2/v2_lora
