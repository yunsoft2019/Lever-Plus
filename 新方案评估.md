# v3 RL Reward 重新设计方案评估

> 本文档对 `Lever-Plus-v3-RL-Reward-Redesign.md` 中提出的"从增益得分改为绝对正确率"方案进行评估分析。

---

## 1. 方案核心思路回顾

### 1.1 当前实现（增益 reward）

```
reward = InfoScore = P(y|x,c) - P(y|x)
```

- 衡量的是"加了这个 ICD 之后答案好/坏了多少"
- 是一个**局部增量信号**

### 1.2 新方案（绝对正确率 reward）

```
reward = vqa_acc_score ∈ [0, 1]
```

- 衡量的是"整条 pointer 序列下的最终 VQA 正确率"
- 是一个**全局目标信号**

---

## 2. 方案诊断的问题是否准确？

### ✅ 核心问题诊断是正确的

文档准确指出了当前 v3 的三个核心问题：

| 问题 | 描述 | 影响 |
|------|------|------|
| **目标与 reward 不对齐** | 当前用的是 InfoScore（增益），而真正目标是 2-shot 下的绝对正确率 | RL 优化方向偏离 |
| **Qwen 场景的特殊性** | 1-shot 已经很强，加第二条 ICD 几乎都是负增益 | reward 空间塌缩，梯度消失 |
| **基线依赖 action** | 增益 = P(y\|x,c) - P(y\|x)，基线和 action 耦合 | 可能颠倒最优解的排序 |

### ⚠️ 但有一个关键事实需要考虑

**当前 v3_1layer 已经在 shot_num=1,2 时超越 v2！**

这说明之前 v3 效果差的**根本原因是权重没有正确加载**，而不完全是 reward 设计的问题。

---

## 3. v3_1layer 当前测试结果

### 3.1 v3_1layer vs v2 对比（所有数据规模）

| 数据规模 | shot_num=1 | shot_num=2 | shot_num=3 | shot_num=4 |
|----------|------------|------------|------------|------------|
| **100条** | +1.0% | **+2.0%** ✓ | -2.6% | -1.6% |
| **200条** | **+2.6%** ✓ | **+1.0%** ✓ | -1.9% | -0.5% |
| **400条** | **+1.25%** ✓ | **+0.6%** ✓ | -1.25% | -0.4% |
| **800条** | **+1.6%** ✓ | 持平 | -1.38% | -0.45% |

### 3.2 关键发现

1. **v3_1layer 在 shot_num=1 时始终超越 v2**（+1.0% ~ +2.6%）
2. **v3_1layer 在 shot_num=2 时表现优秀**（100/200/400条超越v2，800条持平）
3. **v3_1layer 在 shot_num≥3 时低于 v2**（-0.4% ~ -2.6%）

### 3.3 结论

- **GRPO 强化学习在正确加载 v2 权重后确实有效**
- **当前方案在低 shot 数场景已经取得了显著提升**
- **shot_num≥3 的下降可能是因为 GRPO 训练数据是 shot_num=2 生成的**

---

## 4. 新方案的潜在收益

如果实施"绝对正确率 reward"方案，理论上可能带来的改进：

### 4.1 shot_num≥3 的表现可能提升

- 当前 v3_1layer 在 shot_num≥3 时低于 v2
- 如果用绝对正确率作为 reward，可能更好地学习多 shot 场景
- 因为 reward 直接反映"最终是否答对"，而不是"相对上一步的增益"

### 4.2 更稳定的训练

- correctness 是 [0,1] 区间的清晰信号
- 不会出现"全负增益"导致的梯度消失
- reward 的动态范围更合理

### 4.3 更直接的目标对齐

- 直接优化"最终是否答对"
- 不再依赖"增益"的符号和幅度
- 即使所有 InfoScore 为负，reward 仍然是有效信号

---

## 5. 新方案的潜在风险

### 5.1 需要重新生成 RL 数据

- 当前的 RL 数据可能没有 `vqa_correct` / `vqa_acc_score` 字段
- 需要对每个 pointer 序列调用 VQA 模型评估
- **计算成本高**：每个 query 有多个 beam，每个 beam 都需要调用 VQA 模型推理

### 5.2 可能丢失有用的信息

- InfoScore 虽然有问题，但它包含了"概率分布变化"的信息
- 纯 correctness（0/1）可能太稀疏，梯度信号不够丰富
- 可能需要使用 `vqa_acc_score`（连续值）而不是 `vqa_correct`（0/1）

### 5.3 实验周期长

- 重新生成数据：需要对所有 query 的所有 beam 进行 VQA 评估
- 重新训练：RCE + GRPO 完整流程
- 重新评估：100/200/400/800 条数据，shot_num=1,2,3,4

### 5.4 不确定性

- 理论上应该更好，但实际效果需要实验验证
- 可能需要调整超参数（temperature、learning rate 等）

---

## 6. 实施建议

### 6.1 短期策略（推荐）

**先不急着实施新方案**，原因：

1. v3_1layer 已经在 shot_num=1,2 时超越 v2
2. 当前的主要问题（权重加载）已经修复
3. 可以先用现有方案跑更多实验，验证稳定性

**建议的下一步**：
- 在其他采样器（TextSimSampler、ImgSimSampler、MixSampler）上测试 v3_1layer
- 收集更多数据点，确认当前方案的稳定性

### 6.2 中期策略（可选）

如果想进一步提升 shot_num≥3 的表现，可以尝试：

1. **用更高 shot 数的数据生成 RL 训练数据**
   - 当前 GRPO 训练数据是 shot_num=2 生成的
   - 可以尝试用 shot_num=3 或 shot_num=4 的数据

2. **混合 reward 模式**
   - 实施文档中的 `correctness_plus_info` 模式
   - `reward = β * correctness + α * info_score_normalized`
   - 保留 InfoScore 的信息，同时引入 correctness 作为主信号

### 6.3 长期策略（值得尝试）

如果有充足的时间和计算资源，完整实施新方案：

1. **数据层改动**
   - 修改 `rl_data_generation.py`，确保每条 pointer 都有 `vqa_correct` 和 `vqa_acc_score`
   - 重新生成 RL 数据

2. **reward 层改动**
   - 实现 `build_reward()` 函数
   - 支持多种模式：`correctness_only`、`correctness_plus_info`、`binary_correctness`

3. **训练层改动**
   - RCE 和 GRPO 的 `beam_rewards` 改为 correctness-based
   - 可选：调整 `compute_advantage` 的归一化方式

4. **对比实验**
   - v2（纯 SFT）
   - v3_1layer（当前增益 reward）
   - v3_new（新版 correctness reward）

---

## 7. 决策矩阵

| 方案 | 预期收益 | 实施成本 | 风险 | 推荐优先级 |
|------|----------|----------|------|------------|
| **保持现状** | 低（已有提升） | 无 | 无 | ⭐⭐⭐⭐⭐ |
| **用更高 shot 数据训练** | 中（可能提升 shot≥3） | 低 | 低 | ⭐⭐⭐⭐ |
| **混合 reward 模式** | 中 | 中 | 中 | ⭐⭐⭐ |
| **完整实施新方案** | 高（理论上） | 高 | 高 | ⭐⭐ |

---

## 8. 总结

### 8.1 新方案的分析是正确的

- 目标与 reward 不对齐的问题确实存在
- 用绝对正确率作为 reward 理论上更合理

### 8.2 但当前时机可能不是最佳

- v3_1layer 已经在 shot_num=1,2 时超越 v2
- 之前效果差的根本原因是权重加载问题，已修复
- 新方案需要大量重新生成数据和实验的成本

### 8.3 建议

1. **短期**：先用现有 v3_1layer 方案，在更多场景下验证效果
2. **中期**：如果 shot_num≥3 的提升是刚需，尝试用更高 shot 数据训练或混合 reward
3. **长期**：如果有资源，完整实施新方案并做对比实验

---

## 更新记录

- **2025-12-08**：创建文档，基于 v3_1layer 测试结果评估新方案
