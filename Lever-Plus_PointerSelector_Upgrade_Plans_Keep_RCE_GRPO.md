# Lever-Plus PointerSelector å‡çº§æ–¹æ¡ˆæ¸…å•ï¼ˆä¿æŒ RCE / GRPO ä¸å˜ï¼‰

> åŸºäºä½ æä¾›çš„é¡¹ç›®åŒ…ï¼ˆ`Lever-Plus-main`ï¼‰åˆ†æï¼š  
> - å½“å‰çº¿ä¸Šä½¿ç”¨ï¼š`lever_lm/models/v3/pointer_selector_v3.py`ï¼ˆRLï¼šRCE + GRPOï¼‰  
> - æ ¸å¿ƒé€‰æ‹©å™¨æ¥è‡ªï¼š`lever_lm/models/v2/pointer_selector_v2.py`ï¼ˆBi-Encoder + å¤šå±‚ Cross-Attn + é™æ€ä½™å¼¦æ‰“åˆ†ï¼‰  
> - ä½ æŠ¥å‘Šï¼ˆ`2025-12-25æµ‹è¯•æŠ¥å‘Š.md`ï¼‰é‡Œä¹Ÿæåˆ°ï¼šGRPO â€œç­–ç•¥å‡ ä¹æ²¡æœ‰å˜åŒ–â€ã€shotâ‰¥3 ä¸ç¨³å®š/ä¸‹é™ï¼Œè¿™å’Œ V2 çš„â€œé™æ€æ’åº top-kâ€æœºåˆ¶é«˜åº¦ä¸€è‡´ã€‚

æœ¬æ–‡æŠŠâ€œåªæ”¹æ¨¡å‹ï¼ˆforward/ç»“æ„ï¼‰ï¼Œ**RCE/GRPO ä»£ç ä¿æŒä¸å˜**â€ä½œä¸ºç¡¬çº¦æŸï¼Œå¹¶æŠŠæ‰€æœ‰å¯è¡Œå‡çº§æ–¹æ¡ˆæŒ‰**æˆåŠŸæ¦‚ç‡ï¼ˆé¢„è®¡å¸¦æ¥ç¨³å®šæå‡çš„å¯èƒ½æ€§ï¼‰ä»é«˜åˆ°ä½**æ’åºï¼Œæ–¹ä¾¿ä½ é€æ­¥æ¢ç´¢ã€‚

---

## 0. ç¡¬çº¦æŸï¼ˆç¡®ä¿ä¸æ”¹ RCE / GRPOï¼‰

ä½ çš„ `PointerSelectorV3` çš„ RCE/GRPO é€»è¾‘ä¾èµ–è¿™äº›äº‹å®ï¼ˆå¿…é¡»ä¿æŒï¼‰ï¼š

1. **forward æ¥å£ä¸å˜**  
   `forward(query_emb, cand_emb, labels=None, return_loss=True) -> dict`  
   è¿”å›è‡³å°‘åŒ…å«ï¼š  
   - `logits: [B, S, K_actual]`
   - `predictions: [B, S]`
   - å¯é€‰ `loss`

2. **logits çš„æœ€åä¸€ç»´å¿…é¡»ç­‰äº `cand_emb.shape[1]`**  
   å› ä¸º `compute_rce_loss()` é‡Œç”¨ `actual_K = cand_emb.shape[1]` å» reshape logitsï¼š  
   ```py
   logits_for_loss = logits.reshape(-1, actual_K)
   ```
   æ‰€ä»¥å¦‚æœä½ åœ¨æ¨¡å‹å†…éƒ¨å·å·æŠŠ K å˜æˆ K+1ï¼Œä¼šç›´æ¥ç‚¸ï¼ˆé™¤éä½ åŒæ—¶æŠŠ cand_emb ä¹Ÿæ‰©æˆ K+1ï¼‰ã€‚

3. **Teacher Forcing çš„è¯­ä¹‰ä¸å˜**  
   - è®­ç»ƒ/è®¡ç®— logprob æ—¶ï¼šæ¯ä¸€æ­¥ç”¨ `labels[:, step]` æ›´æ–° maskï¼ˆä»¥åŠä½ æ–°å¼•å…¥çš„â€œçŠ¶æ€â€ï¼‰
   - æ¨ç†æ—¶ï¼šç”¨ `argmax` çš„ pred æ›´æ–° maskï¼ˆä»¥åŠçŠ¶æ€ï¼‰

4. **æ”¯æŒ per-query åŠ¨æ€ K**  
   ä½ é¡¹ç›®é‡Œå·²ç»ä¿®å¤äº† per-query å€™é€‰æ± ï¼ˆV2/V3 å¤šå¤„ç”¨ `actual_K = cand_emb.shape[1]`ï¼‰ï¼Œå‡çº§æ–¹æ¡ˆå¿…é¡»ç»§ç»­éµå®ˆã€‚

---

## 1. ç°çŠ¶å¤ç›˜ï¼šV2 å…¶å®ä¸æ˜¯â€œçœŸæ­£çš„ pointer netâ€ï¼ˆä¸ºä»€ä¹ˆ shotâ‰¥3 å®¹æ˜“æ‰ï¼‰

ä½ ç°åœ¨çš„ `PointerSelectorV2.forward()` çš„æ ¸å¿ƒå¾ªç¯æ˜¯ï¼š

- æ¯ä¸ª step éƒ½ç”¨**åŒä¸€ä¸ª** `query_proj` è®¡ç®— `scores = query_proj @ cand_proj^T`
- åªç”¨ `selected_mask` ç¦æ­¢é‡å¤

å› æ­¤å®ƒåœ¨æ•°å­¦ä¸Šéå¸¸æ¥è¿‘ï¼š**é™æ€æ‰“åˆ† + é€æ­¥å»é‡çš„ top-k æ’åº**ã€‚  
è¿™ä¼šå¯¼è‡´å…¸å‹é—®é¢˜ï¼š

- shot=1/2ï¼šé€‰æœ€ç›¸ä¼¼çš„å‡ ä¸ªé€šå¸¸æ²¡æ¯›ç—…  
- shotâ‰¥3ï¼šå¼€å§‹å¤§é‡é€‰åˆ°â€œäº’ç›¸å¾ˆåƒâ€çš„ demoï¼ˆå†—ä½™ï¼‰ï¼Œç”šè‡³æŠŠå™ªå£°/é”™è¯¯ demo å¡è¿› prompt â†’ æ­£ç¡®ç‡ä¸‹é™  
- GRPOï¼šå°±ç®—ä½ ç”¨ RL è®­ç»ƒï¼Œæ¨¡å‹çš„â€œåŠ¨ä½œç©ºé—´â€å®é™…ä¸Šå¾ˆéš¾è¡¨è¾¾ç»„åˆäº’è¡¥ï¼ˆå› ä¸ºæ¯ä¸€æ­¥åˆ†æ•°æ²¡éšå†å²æ›´æ–°ï¼‰

> æ‰€ä»¥æœ€æ ¸å¿ƒçš„å‡çº§æ–¹å‘ï¼š**è®© step t çš„æ‰“åˆ†æ˜¾å¼ä¾èµ–å·²é€‰å†å²ï¼ˆhistory-aware / set-awareï¼‰**ã€‚

---

## 2. æ–¹æ¡ˆæ€»è§ˆï¼ˆæŒ‰æˆåŠŸæ¦‚ç‡æ’åºï¼‰

| æ’å | æ–¹æ¡ˆä»£å· | é¢„è®¡æˆåŠŸæ¦‚ç‡ | å®ç°éš¾åº¦ | ä¸»è¦æ”¶ç›Šç‚¹ | æ˜¯å¦ä¿æŒ RCE/GRPO ä¸å˜ |
|---:|---|---|---|---|---|
| 1 | V4-1ï¼šCross-Attn + **Query çŠ¶æ€æ›´æ–°ï¼ˆV1 æ€è·¯å›å½’ï¼‰** | å¾ˆé«˜ | å¾ˆä½ | ç«‹åˆ»è®©å¤šæ­¥é€‰æ‹©â€œæœ‰è®°å¿†â€ | âœ… |
| 2 | V4-2ï¼šCross-Attn + **GRU Pointer Decoder** | å¾ˆé«˜ | ä½-ä¸­ | æ›´å¼ºçš„ history-aware ç»„åˆèƒ½åŠ› | âœ… |
| 3 | V4-3ï¼šåœ¨ V4-1/2 ä¸ŠåŠ  **Learnable MMR å¤šæ ·æ€§æ®‹å·®** | é«˜ | ä½ | ä¸“æ²» shotâ‰¥3 å†—ä½™ | âœ… |
| 4 | V4-4ï¼š**Candidate Set Encoderï¼ˆSelf-Attnï¼‰** + V4-2 | ä¸­-é«˜ | ä¸­ | å€™é€‰ä¹‹é—´å…ˆâ€œäº’ç›¸çœ‹ä¸€çœ¼â€ï¼Œæ›´ä¼šå»é‡ | âœ… |
| 5 | V4-5ï¼šæŠŠâ€œç‚¹ç§¯â€æ¢æˆ **Additive/Bilinear Attention æ‰“åˆ†å¤´** | ä¸­-é«˜ | ä¸­ | è§£å†³ embedding ä¸å®Œå…¨åŒç©ºé—´çš„é—®é¢˜ | âœ… |
| 6 | V4-6ï¼š**Coverage / Topic åŸå‹è¦†ç›–**ï¼ˆè‡ªç›‘ç£ã€æ— éœ€é¢å¤–æ ‡ç­¾ï¼‰ | ä¸­ | ä¸­ | å¼ºåŒ–äº’è¡¥è¦†ç›–ã€å‡å°‘é‡å¤ | âœ… |
| 7 | V4-7ï¼š**(N)DPP / log-det é£æ ¼çš„é›†åˆå¢ç›Š**ï¼ˆè¿‘ä¼¼ï¼‰ | ä¸­-ä½ | é«˜ | å¼ºé›†åˆå»ºæ¨¡ï¼Œä½†æ•°å€¼/å·¥ç¨‹æ›´éš¾ | âœ… |
| 8 | V4-8ï¼š**Slot/Set Decoderï¼ˆå¹¶è¡Œ slots ååŒï¼‰** + å…¼å®¹è¾“å‡º logits | ä¸­-ä½ | é«˜ | ç›´æ¥åšâ€œé›†åˆé¢„æµ‹â€ï¼Œæœ‰æ½œåŠ› | âœ… |
| 9 | V4-9ï¼šTwo-Stage **Coarse-to-Fine TopM ç²¾æ’**ï¼ˆé€Ÿåº¦+å¯èƒ½æ›´ç¨³ï¼‰ | ä¸­-ä½ | ä¸­ | ä¸»è¦åˆ©å¥½æ•ˆç‡ï¼Œä¹Ÿå¯èƒ½æå‡é²æ£’ | âœ… |
| 10 | V4-10ï¼š**STOP è‡ªé€‚åº” shot**ï¼ˆéœ€æŠŠ cand_pool æ‰©æˆ K+1ï¼‰ | ä¸ç¡®å®š | ä¸­-é«˜ | è§£å†³â€œshot å¤šåè€Œä¼¤â€çš„æ ¹å›  | âœ…ï¼ˆä½†éœ€è¦ä¸Šæ¸¸æ”¹ cand_embï¼‰ |

> æ¨èæ¢ç´¢è·¯å¾„ï¼šä» V4-1 â†’ V4-2 â†’ V4-3 èµ°ä¸‰æ­¥ï¼Œä½ å¤§æ¦‚ç‡å°±èƒ½çœ‹åˆ°â€œshotâ‰¥3 ä¸å†æ˜æ˜¾æ‰â€çš„è¶‹åŠ¿ï¼›å†å¾€åé€æ­¥åŠ  set encoder / coverageã€‚

---

# æ–¹æ¡ˆ 1ï¼šV4-1ï¼ˆæœ€æ¨èï¼‰Cross-Attn + Query çŠ¶æ€æ›´æ–°ï¼ˆV1 æ€è·¯å›å½’ï¼‰

## ä¸ºä»€ä¹ˆæˆåŠŸæ¦‚ç‡å¾ˆé«˜ï¼ˆåŸºäºä½ é¡¹ç›®ï¼‰
- ä½ é¡¹ç›®é‡Œ **V1 å°±æœ‰â€œé€‰å®Œä¸€ä¸ª demo æ›´æ–° queryâ€çš„æœºåˆ¶**ï¼ˆ`PointerSelectorV1.forward()` é‡Œ `current_query = alpha * current_query + (1-alpha) * next_icd`ï¼‰
- ä½† V2 å¼•å…¥ Cross-Attn åæŠŠâ€œçŠ¶æ€æ›´æ–°â€ä¸¢äº† â†’ å¤šæ­¥é€‰æ‹©é€€åŒ–æˆé™æ€ top-k
- æ‰€ä»¥æœ€å°æ”¹åŠ¨ï¼š**ä¿ç•™ V2 çš„ Cross-Attn å¢å¼ºï¼Œå†åŠ å›â€œéšå·²é€‰ demo æ›´æ–° query_stateâ€**  
  è¿™èƒ½ç›´æ¥æŠŠ multi-shot é€‰æ‹©å˜æˆæ¡ä»¶æ¦‚ç‡é“¾ï¼š`p(a1|q) p(a2|q,a1) ...`

## æ”¹åŠ¨ç‚¹ï¼ˆä»…æ”¹æ¨¡å‹ï¼Œä¸ç¢° V3 çš„ RCE/GRPOï¼‰
æ–‡ä»¶ï¼š`lever_lm/models/v2/pointer_selector_v2.py`

- åœ¨ `__init__` æ–°å¢ä¸€ä¸ªå¯å­¦ä¹  gateï¼ˆæ ‡é‡æˆ–å‘é‡éƒ½è¡Œï¼‰
- åœ¨ `forward()` çš„ step loop é‡Œï¼Œé€‰å®Œä¸€ä¸ª idx åæ›´æ–° `query_state`

## ä¼ªä»£ç ï¼ˆä¸¥æ ¼è´´åˆä½  V2 çš„å˜é‡/shapeï¼‰

```py
class PointerSelectorV2(nn.Module):
    def __init__(...):
        ...
        # âœ… æ–°å¢ï¼šå¯å­¦ä¹ çš„èåˆæƒé‡ï¼ˆå»ºè®®åšæˆå‘é‡ gatingï¼Œæ›´å¼ºï¼‰
        # æ–¹æ¡ˆAï¼šæ ‡é‡ gateï¼ˆæœ€ç®€å•ï¼‰
        self.query_update_weight = nn.Parameter(torch.tensor(0.6))
        # æ–¹æ¡ˆBï¼šå‘é‡ gateï¼ˆæ›´å¼ºï¼Œæ¨èï¼‰
        # self.query_update_gate = nn.Linear(hidden_dim * 2, hidden_dim)

    def forward(query_emb, cand_emb, labels=None, return_loss=True):
        B = query_emb.shape[0]
        device = query_emb.device
        actual_K = cand_emb.shape[1]
        input_dim = query_emb.shape[-1]

        # 1) input_proj
        query_reduced = self.input_proj(query_emb)                     # [B,H]
        cand_reduced  = self.input_proj(cand_emb.reshape(-1,input_dim)).reshape(B,actual_K,self.hidden_dim)

        # 2) å¤šå±‚ Cross-Attn åªå¢å¼ºä¸€æ¬¡ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        query_for_attn = query_reduced.unsqueeze(1)                    # [B,1,H]
        for l in range(self.num_layers):
            attn_out,_ = self.cross_attn_layers[l](query_for_attn, cand_reduced, cand_reduced)
            query_for_attn = self.attn_norms[l](attn_out + query_for_attn)
        query_enhanced = query_for_attn.squeeze(1)                     # [B,H]

        # 3) æŠ•å½± + dropout + L2 normalize
        query_state = F.normalize(self.dropout(self.query_proj(query_enhanced)), p=2, dim=-1)  # [B,H]
        cand_proj   = F.normalize(self.dropout(self.cand_proj(cand_reduced)),   p=2, dim=-1)  # [B,K,H]

        selected_mask = torch.zeros(B, actual_K, dtype=torch.bool, device=device)
        all_logits, predictions = [], []

        for step in range(self.shot_num):
            # 4) ç”¨â€œå½“å‰ query_stateâ€æ‰“åˆ†ï¼ˆâœ…ä¸å†æ˜¯é™æ€ queryï¼‰
            scores = (query_state.unsqueeze(1) @ cand_proj.transpose(1,2)).squeeze(1)         # [B,K]
            scores = scores / self.temperature.to(device)
            scores = scores.masked_fill(selected_mask, -100.0)

            all_logits.append(scores)
            pred = scores.argmax(dim=-1)                                                      # [B]
            predictions.append(pred)

            # 5) Teacher forcing çš„ idxï¼ˆè®­ç»ƒç”¨ labelï¼Œæ¨ç†ç”¨ predï¼‰
            if labels is not None and step < labels.shape[1]:
                idx = labels[:, step]                                                         # [B]
            else:
                idx = pred

            # 6) æ›´æ–° maskï¼ˆé¿å… inplaceï¼‰
            selected_mask = selected_mask.scatter(1, idx.unsqueeze(1), True)

            # 7) å–å‡ºè¢«é€‰ demo çš„ embedding
            chosen = cand_proj.gather(1, idx.view(B,1,1).expand(-1,1,self.hidden_dim)).squeeze(1)  # [B,H]

            # 8) âœ…æ›´æ–° query_stateï¼ˆè®©åç»­ step æ¡ä»¶åŒ–ï¼‰
            # æ–¹æ¡ˆAï¼šæ ‡é‡ gateï¼ˆä¸ V1 ä¸€è‡´ï¼‰
            alpha = torch.sigmoid(self.query_update_weight)                                   # scalar
            query_state = F.normalize(alpha * query_state + (1 - alpha) * chosen, p=2, dim=-1)

            # æ–¹æ¡ˆBï¼šå‘é‡ gateï¼ˆæ›´å¼ºï¼‰
            # g = torch.sigmoid(self.query_update_gate(torch.cat([query_state, chosen], dim=-1)))  # [B,H]
            # query_state = F.normalize(g*query_state + (1-g)*chosen, p=2, dim=-1)

        logits = torch.stack(all_logits, dim=1)        # [B,S,K]
        preds  = torch.stack(predictions, dim=1)       # [B,S]
        out = {"logits": logits, "predictions": preds}
        if return_loss and labels is not None:
            out["loss"] = self.compute_loss(logits, labels)
        return out
```

## ä½ åº”è¯¥é‡ç‚¹çœ‹ä»€ä¹ˆæŒ‡æ ‡ï¼ˆå¿«é€Ÿåˆ¤æ–­æ˜¯å¦æœ‰æ•ˆï¼‰
- **shot3/shot4 æ˜¯å¦ä¸å†æ˜¾è‘—ä¸‹é™**ï¼ˆä½ æŠ¥å‘Šé‡Œ shot3 ç»å¸¸æ‰ï¼‰
- GRPO æ—¥å¿—é‡Œï¼š`Adv Std` æ˜¯å¦å˜å¤§ã€`mean_ratio` æ˜¯å¦æ›´å®¹æ˜“åç¦» 1ï¼ˆè¯´æ˜ç­–ç•¥çœŸçš„åœ¨å˜ï¼‰
- æ¨ç†æ—¶ top-k çš„â€œé›†åˆé‡å¤ç‡â€æ˜¯å¦é™ä½ï¼š  
  `mean_{batch} mean_{i<j} cosine(cand[idx_i], cand[idx_j])` åº”è¯¥æ›´ä½

---

# æ–¹æ¡ˆ 2ï¼šV4-2 Cross-Attn + GRU Pointer Decoderï¼ˆçœŸæ­£çš„â€œæœ‰çŠ¶æ€æŒ‡é’ˆç½‘ç»œâ€ï¼‰

## ä¸ºä»€ä¹ˆæˆåŠŸæ¦‚ç‡å¾ˆé«˜
V4-1 çš„ gated update æ˜¯çº¿æ€§èåˆï¼›GRU å¯ä»¥å­¦æ›´å¤æ‚çš„â€œè®°å¿†/é—å¿˜/ç»„åˆç­–ç•¥â€ï¼Œåœ¨å¤šæ­¥é€‰æ‹©é‡Œé€šå¸¸æ›´å¼ºã€‚

## æ”¹åŠ¨ç‚¹
ä»ç„¶åªæ”¹ `PointerSelectorV2`ï¼ˆæˆ–å¤åˆ¶ä¸€ä¸ª `PointerSelectorV4Core`ï¼‰ï¼ŒV3 çš„ RL ä»£ç æ— éœ€åŠ¨ã€‚

æ–°å¢æ¨¡å—ï¼š
- `self.decoder = nn.GRUCell(hidden_dim, hidden_dim)`

## ä¼ªä»£ç ï¼ˆforward æ ¸å¿ƒï¼‰

```py
class PointerSelectorV2(nn.Module):
    def __init__(...):
        ...
        self.decoder_gru = nn.GRUCell(self.hidden_dim, self.hidden_dim)
        # å¯é€‰ï¼šstep embeddingï¼Œè®©ä¸åŒ step å­¦åˆ°ä¸åŒç­–ç•¥
        self.step_emb = nn.Embedding(self.shot_num, self.hidden_dim)

    def forward(...):
        ...  # åŒ V2ï¼šå¾—åˆ° query_enhanced, cand_proj

        # åˆå§‹ hidden state
        h = F.normalize(self.dropout(self.query_proj(query_enhanced)), p=2, dim=-1)  # [B,H]
        cand_proj = F.normalize(self.dropout(self.cand_proj(cand_reduced)), p=2, dim=-1)

        selected_mask = zeros([B,K])
        for step in range(self.shot_num):

            h_step = h + self.step_emb(step)     # [B,H]  (å¯é€‰ä½†æ¨è)
            h_step = F.normalize(h_step, p=2, dim=-1)

            # ç”¨å½“å‰çŠ¶æ€æŒ‡å‘ candidates
            scores = (h_step.unsqueeze(1) @ cand_proj.transpose(1,2)).squeeze(1) / temperature
            scores = scores.masked_fill(selected_mask, -100.0)
            ...

            idx = labels[:,step] if training else pred
            selected_mask = selected_mask.scatter(1, idx.unsqueeze(1), True)

            chosen = cand_proj.gather(1, idx.view(B,1,1).expand(-1,1,H)).squeeze(1)  # [B,H]

            # âœ… GRU æ›´æ–° hidden stateï¼ˆhistory-awareï¼‰
            h = self.decoder_gru(chosen, h)        # [B,H]
            h = F.normalize(h, p=2, dim=-1)
```

## å·¥ç¨‹å»ºè®®
- GRU åˆæœŸå¯èƒ½ä¸ç¨³å®šï¼šå¯ä»¥å…ˆåªç”¨ RCE é¢„çƒ­ 1~2 epochï¼Œå†å¼€ GRPOï¼ˆä½ å·²æœ‰æµç¨‹ï¼‰ã€‚
- å¦‚æœæ‹…å¿ƒè¿‡æ‹Ÿåˆï¼šä¿ç•™ä½ å½“å‰çš„ dropoutï¼ˆ0.1~0.5ï¼‰+ weight decayã€‚

---

# æ–¹æ¡ˆ 3ï¼šV4-3 åœ¨ V4-1/2 ä¸ŠåŠ  Learnable MMR å¤šæ ·æ€§æ®‹å·®ï¼ˆshotâ‰¥3 å¿…åšï¼‰

## ä¸ºä»€ä¹ˆæˆåŠŸæ¦‚ç‡é«˜
ä½ ç°è±¡é‡Œâ€œshot1/2 æå‡æ›´æ˜æ˜¾ï¼Œshot3 ä¸‹é™â€éå¸¸åƒå†—ä½™å¯¼è‡´ã€‚MMRï¼ˆMaximum Marginal Relevanceï¼‰çš„æ€æƒ³å°±æ˜¯ï¼š  
> é€‰ç¬¬ t ä¸ªæ—¶ï¼Œä¸åªçœ‹â€œå’Œ query çš„ç›¸å…³æ€§â€ï¼Œè¿˜è¦æƒ©ç½šâ€œå’Œå·²é€‰é›†åˆçš„ç›¸ä¼¼åº¦â€ã€‚

è¿™å¯ä»¥åªåœ¨æ¨¡å‹ forward é‡Œåšï¼Œä¸æ”¹ RCE/GRPOã€‚

## æ”¹åŠ¨ç‚¹
- forward é‡Œç»´æŠ¤ `selected_embs`ï¼ˆå·²é€‰ cand_projï¼‰
- æ¯æ­¥æŠŠå†—ä½™é¡¹ä» scores é‡Œå‡æ‰
- Î» åšæˆå¯å­¦ä¹ ï¼ˆper-stepï¼‰

æ–°å¢å‚æ•°ï¼š
```py
self.div_lambda = nn.Parameter(torch.zeros(self.shot_num))  # åˆå§‹åŒ–ä¸º 0ï¼Œç­‰ä»·äºåŸæ¨¡å‹ï¼Œå¯å¹³æ»‘è¿ç§»
```

## ä¼ªä»£ç ï¼ˆåœ¨ step loop å†…åŠ å…¥ï¼‰

```py
selected_embs = []  # list of [B,H]

for step in range(S):
    base = (state.unsqueeze(1) @ cand_proj.transpose(1,2)).squeeze(1) / temperature  # [B,K]

    if step > 0:
        sel = torch.stack(selected_embs, dim=1)     # [B,step,H]
        # cosineï¼Œå› ä¸ºéƒ½ normalize äº†ï¼Œç‚¹ç§¯å°±æ˜¯ cosine
        sim = torch.einsum("bkh,bth->bkt", cand_proj, sel)   # [B,K,step]
        redundancy = sim.max(dim=-1).values                   # [B,K] (ä¹Ÿå¯ç”¨ mean)
        base = base - torch.relu(self.div_lambda[step]) * redundancy

    scores = base.masked_fill(selected_mask, -100.0)

    idx = labels[:,step] if training else argmax(scores)
    chosen = cand_proj.gather(1, idx.view(B,1,1).expand(-1,1,H)).squeeze(1)
    selected_embs.append(chosen)
    ...
```

## å°æŠ€å·§ï¼ˆæ›´ä¼˜é›…ã€æ›´å¼ºï¼‰
- `redundancy` ç”¨ `max` å¾€å¾€æ¯” `mean` æ›´åƒâ€œå»é‡â€
- `div_lambda` ç”¨ `softplus` æˆ– `relu` ä¿è¯éè´Ÿï¼Œé¿å…æ¨¡å‹å­¦å‡ºâ€œé¼“åŠ±é‡å¤â€çš„å¥‡æ€ªè¡Œä¸º

---

# æ–¹æ¡ˆ 4ï¼šV4-4 Candidate Set Encoderï¼ˆSelf-Attnï¼‰+ V4-2ï¼ˆæ›´ä¼šå¤„ç†å€™é€‰é‡å¤ï¼‰

## ç›´è§‰
V2 åªæœ‰ queryâ†’cand çš„ cross-attnï¼Œæ²¡æœ‰ candâ†”cand çš„äº¤äº’ã€‚  
ä½†â€œé‡å¤/å†—ä½™â€æœ¬è´¨æ˜¯å€™é€‰ä¹‹é—´çš„å…³ç³»ï¼Œæ‰€ä»¥è®© candidates å…ˆ self-attn ä¸€æ¬¡é€šå¸¸æ›´ç¨³ã€‚

## æ”¹åŠ¨ç‚¹
åœ¨ `cand_reduced` ä¸ŠåŠ ä¸€åˆ°ä¸¤å±‚ self-attn encoderï¼š

æ–°å¢æ¨¡å—ï¼š
```py
self.cand_encoder = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(
        d_model=hidden_dim, nhead=1,
        dim_feedforward=hidden_dim*4,
        dropout=dropout, batch_first=True
    ),
    num_layers=1 or 2
)
```

## ä¼ªä»£ç ï¼ˆæ’å…¥åœ¨ cand_reduced åï¼‰

```py
cand_ctx = self.cand_encoder(cand_reduced)   # [B,K,H]
cand_proj = F.normalize(self.dropout(self.cand_proj(cand_ctx)), p=2, dim=-1)

# query_enhanced ä»ç„¶ç”¨ cross-attnï¼ˆå¯é€‰ï¼šè®© query ä¹Ÿ attend cand_ctxï¼‰
...
```

> æ¨èæ­é… V4-2ï¼ˆGRU decoderï¼‰ï¼šcand_ctx è¡¨è¾¾æ›´å¼ºï¼Œstateful decoder å†³ç­–æ›´å¼ºã€‚

---

# æ–¹æ¡ˆ 5ï¼šV4-5 æŠŠ dot-product æ¢æˆ Additive / Bilinear Attention æ‰“åˆ†å¤´ï¼ˆæå‡å¯è¡¨è¾¾æ€§ï¼‰

## é€‚ç”¨åœºæ™¯
å½“ä½ è§‰å¾—ï¼š
- query embeddingï¼ˆæ¥è‡ªæŸä¸ª adapter/CLIP åˆ†æ”¯ï¼‰å’Œ candidate embedding çš„â€œå¯çº¿æ€§å¯¹é½æ€§â€ä¸å¤Ÿ  
- çº¯ dot-product è¿‡äºåˆšæ€§

å¯ä»¥æ¢ scoring headï¼Œè€Œ**ä¸æ”¹å˜æ•´ä½“ç®¡çº¿**ã€‚

## 5.1 Additiveï¼ˆBahdanauï¼‰Attention ä¼ªä»£ç 

æ–°å¢æ¨¡å—ï¼š
```py
self.attn_Wq = nn.Linear(H, H, bias=True)
self.attn_Wc = nn.Linear(H, H, bias=False)
self.attn_v  = nn.Linear(H, 1, bias=False)
```

step å†…æ‰“åˆ†ï¼š
```py
# h: [B,H], cand_proj: [B,K,H]
q = self.attn_Wq(h).unsqueeze(1)             # [B,1,H]
c = self.attn_Wc(cand_proj)                  # [B,K,H]
scores = self.attn_v(torch.tanh(q + c)).squeeze(-1)   # [B,K]
scores = scores.masked_fill(selected_mask, -100.0)
```

## 5.2 Bilinear Attention ä¼ªä»£ç 

æ–°å¢æ¨¡å—ï¼š
```py
self.bilinear = nn.Bilinear(H, H, 1, bias=False)   # è¾“å‡ºæ ‡é‡
```

æ‰“åˆ†ï¼š
```py
# å¹¿æ’­åˆ° [B,K]
scores = self.bilinear(h.unsqueeze(1).expand(-1,K,-1), cand_proj).squeeze(-1)
```

---

# æ–¹æ¡ˆ 6ï¼šV4-6 Coverage / Topic åŸå‹è¦†ç›–ï¼ˆè‡ªç›‘ç£ã€æ— éœ€é¢å¤–æ ‡ç­¾ï¼‰

## ç›®æ ‡
è®©æ¨¡å‹å­¦ä¼šâ€œäº’è¡¥è¦†ç›–â€ï¼šåç»­ demo æ›´å€¾å‘äºè¦†ç›–å‰é¢æ²¡è¦†ç›–åˆ°çš„â€œåŸå‹/ç°‡â€ã€‚

## ä¸ºä»€ä¹ˆä¸éœ€è¦é¢å¤–è¾“å…¥ï¼ˆä¸¥æ ¼å…¼å®¹ä½  forward ç­¾åï¼‰
æˆ‘ä»¬åœ¨æ¨¡å‹é‡Œå¼•å…¥ M ä¸ªâ€œåŸå‹å‘é‡â€ï¼ˆlearnable prototypesï¼‰ï¼Œç”¨ soft cluster çš„æ–¹å¼ç»™æ¯ä¸ª candidate ä¸€ä¸ª topic åˆ†å¸ƒï¼›å†è®© query é¢„æµ‹è‡ªå·±éœ€è¦å“ªäº› topicsã€‚

## æ–°å¢æ¨¡å—
```py
M = 16  # topics/prototypes æ•°
self.topic_prototypes = nn.Parameter(torch.randn(M, H))
self.query_topic_head = nn.Linear(H, M, bias=True)
self.cover_lambda = nn.Parameter(torch.tensor(0.0))  # å…ˆä»0å¼€å§‹
```

## ä¼ªä»£ç ï¼ˆstep loop å†…åŠ  coverage gainï¼‰

```py
# é¢„è®¡ç®—ï¼šæ¯ä¸ª candidate çš„ topic åˆ†å¸ƒ
# cand_proj: [B,K,H], prototypes: [M,H]
proto = F.normalize(self.topic_prototypes, p=2, dim=-1)                 # [M,H]
topic_logits = cand_proj @ proto.t()                                     # [B,K,M]
topic_probs  = F.softmax(topic_logits, dim=-1)                           # [B,K,M]

# query éœ€è¦çš„ topics
need = F.softmax(self.query_topic_head(h0), dim=-1)                      # [B,M]

covered = torch.zeros(B, M, device=device)                               # [B,M]
for step in range(S):
    base = score(h, cand_proj)                                           # [B,K]

    # coverage gainï¼šå€¾å‘é€‰æ‹©èƒ½è¦†ç›–æœªè¦†ç›– topic çš„å€™é€‰
    uncovered = (1.0 - covered).clamp(min=0.0, max=1.0)                  # [B,M]
    gain = torch.einsum("bm,bkm->bk", need * uncovered, topic_probs)      # [B,K]

    scores = base + torch.relu(self.cover_lambda) * gain
    scores = scores.masked_fill(selected_mask, -100.0)

    idx = labels[:,step] if training else argmax(scores)
    selected_mask = selected_mask.scatter(1, idx.unsqueeze(1), True)

    # æ›´æ–° coveredï¼ˆTeacher forcing ä¹Ÿç”¨ idxï¼‰
    chosen_topic = topic_probs.gather(1, idx.view(B,1,1).expand(-1,1,M)).squeeze(1)  # [B,M]
    covered = (covered + chosen_topic).clamp(max=1.0)

    # æ›´æ–° stateï¼ˆå¯ç”¨ V4-1 æˆ– V4-2ï¼‰
    ...
```

## æ³¨æ„
- M ä¸è¦å¤ªå¤§ï¼ˆ16/32 è¶³å¤Ÿï¼‰ï¼Œå¦åˆ™ä¸ç¨³å®š
- `cover_lambda` å»ºè®®åˆå§‹åŒ– 0ï¼Œå…ˆè®©æ¨¡å‹å­¦ç›¸å…³æ€§ï¼Œå†æ…¢æ…¢å­¦è¦†ç›–

---

# æ–¹æ¡ˆ 7ï¼šV4-7 (N)DPP / log-det é£æ ¼é›†åˆå¢ç›Šï¼ˆè¿‘ä¼¼å®ç°ï¼‰

> è¿™æ˜¯â€œæ›´å­¦æœ¯ã€æ›´å¼ºé›†åˆå»ºæ¨¡â€çš„è·¯çº¿ï¼Œä½†å®ç°å’Œæ•°å€¼ç¨³å®šæ€§æ›´æŒ‘æˆ˜ï¼ŒæˆåŠŸæ¦‚ç‡ä¸­ç­‰åä½ã€‚

## ç®€åŒ–ç‰ˆå¯è½åœ°æ€è·¯ï¼šç”¨ä½ç§©ç‰¹å¾åš logdet è¿‘ä¼¼å¢ç›Š
æ–°å¢ï¼š
```py
r = 32
self.dpp_proj = nn.Linear(H, r, bias=False)
self.dpp_lambda = nn.Parameter(torch.tensor(0.0))
```

æ¯ä¸ª candidate çš„ DPP ç‰¹å¾ï¼š
```py
B = F.normalize(self.dpp_proj(cand_proj), p=2, dim=-1)   # [B,K,r]
```

å¢é‡å¤šæ ·æ€§ï¼ˆè¿‘ä¼¼ï¼‰ï¼š  
ç”¨ â€œä¸å·²é€‰é›†åˆçš„æœ€å¤§ç›¸ä¼¼åº¦â€ è¿‘ä¼¼ logdet å¢ç›Šï¼š
```py
sim = einsum("bkr,btr->bkt", B, B_selected)      # [B,K,t]
div_gain = torch.log(1e-6 + 1 - sim.max(-1).values.pow(2))  # [B,K]
scores = base + relu(dpp_lambda) * div_gain
```

> å¦‚æœä½ æƒ³åšæ›´â€œçœŸâ€çš„ logdetï¼šéœ€è¦ç»´æŠ¤ Cholesky åˆ†è§£/é€†çŸ©é˜µï¼Œå·¥ç¨‹é‡æ˜æ˜¾æ›´å¤§ï¼›å»ºè®®å…ˆç”¨ä¸Šé¢çš„è¿‘ä¼¼ç‰ˆæœ¬éªŒè¯æ–¹å‘ã€‚

---

# æ–¹æ¡ˆ 8ï¼šV4-8 Slot/Set Decoderï¼ˆå¹¶è¡Œ slots ååŒï¼‰ä½†ä»è¾“å‡º [B,S,K] logits

## ç›´è§‰
ä¸å…¶è‡ªå›å½’ä¸€æ­¥æ­¥æŒ‘ï¼Œä¸å¦‚åŒæ—¶ç»´æŠ¤ S ä¸ªâ€œslotâ€ï¼Œslots ä¹‹é—´ self-attn ååŒåˆ†å·¥ï¼Œç„¶åæ¯ä¸ª slot ç”Ÿæˆä¸€è¡Œ logitsã€‚

ä¸ºäº†å…¼å®¹ä½ ç°æœ‰è®­ç»ƒï¼ˆlabels æ˜¯åºåˆ—ï¼‰ï¼Œæˆ‘ä»¬ä»ç„¶æŒ‰ step è¾“å‡º logitsï¼Œä½† logits ç”± slot äº§ç”Ÿã€‚

## æ–°å¢æ¨¡å—
```py
self.slot_emb = nn.Embedding(self.shot_num, H)     # æ¯ä¸ª slot ä¸€ä¸ª learnable embedding
self.slot_self_attn = nn.MultiheadAttention(H, 1, dropout=attn_dropout, batch_first=True)
self.slot_norm = nn.LayerNorm(H)
```

## ä¼ªä»£ç 

```py
# åˆå§‹åŒ– slots
slots = query_proj.unsqueeze(1).expand(-1,S,-1) + self.slot_emb.weight.unsqueeze(0)  # [B,S,H]

# slots è‡ªèº«ååŒ
attn_out,_ = self.slot_self_attn(slots, slots, slots)
slots = self.slot_norm(slots + attn_out)                                           # [B,S,H]
slots = F.normalize(slots, p=2, dim=-1)

# æ¯ä¸ª slot å¯¹ candidates æ‰“åˆ†ï¼šå¾—åˆ° [B,S,K]
logits = torch.einsum("bsh,bkh->bsk", slots, cand_proj) / temperature

# ä¸ºäº†ä¿è¯â€œä¸é‡å¤â€ï¼Œæ¨ç†æ—¶ä»ç„¶å¯ç”¨ä½ ç°åœ¨çš„ greedy maskï¼š
# step0 ç”¨ logits[:,0], mask idx0
# step1 ç”¨ logits[:,1] ä½†æŠŠ idx0 mask æ‰
# ...
```

> è¿™æ¡è·¯çº¿æ½œåŠ›å¤§ï¼Œä½†ä½ éœ€è¦ä»”ç»†å¤„ç† â€œslot è¾“å‡ºçš„é¡ºåºâ€ ä¸ labels åºåˆ—å¯¹é½çš„é—®é¢˜ï¼ˆå¦åˆ™ç›‘ç£ä¼šæ··ä¹±ï¼‰ã€‚  
> æ¨èï¼šè®­ç»ƒæ—¶ labels å°±æŒ‰ reward/beam æ’å¥½å›ºå®šé¡ºåºï¼ˆä½  RL æ•°æ®é‡Œé€šå¸¸æ˜¯æŒ‰ reward é™åºï¼‰ï¼Œslot ä¹Ÿå›ºå®šå¯¹åº” stepã€‚

---

# æ–¹æ¡ˆ 9ï¼šV4-9 Two-Stage Coarse-to-Fineï¼ˆTopM ç²¾æ’ï¼Œä¿æŒè¾“å‡º K ä¸å˜ï¼‰

## ç›®æ ‡
- ä¸»è¦ï¼šæå‡é€Ÿåº¦ã€ç¨³å®šæ€§ï¼ˆæŠŠå¤æ‚è®¡ç®—é›†ä¸­åœ¨ topMï¼‰  
- æ¬¡è¦ï¼šæœ‰æ—¶ä¹Ÿèƒ½æå‡è´¨é‡ï¼ˆå‡å°‘å™ªå£°å€™é€‰å¹²æ‰°ï¼‰

## ä¼ªä»£ç ï¼ˆæ¯ stepï¼‰

```py
# cheap scoreï¼ˆç‚¹ç§¯ï¼‰
cheap = dot(h, cand_proj)                           # [B,K]
cheap = cheap.masked_fill(selected_mask, -100.0)

# é€‰ topM åšç²¾æ’
top_val, top_idx = cheap.topk(M, dim=-1)           # [B,M]
cand_sub = cand_proj.gather(1, top_idx[...,None].expand(-1,-1,H))  # [B,M,H]

# heavy refineï¼ˆæ¯”å¦‚ä¸€ä¸ªå° cross-attn / MLPï¼‰
refined_sub = refine(h, cand_sub)                  # [B,M]  è¾“å‡º refined åˆ†æ•°

# scatter å›å…¨é‡ K
scores = torch.full([B,K], -100.0, device=device)
scores = scores.scatter(1, top_idx, refined_sub)

# logits ä»æ˜¯ [B,K]ï¼Œå®Œå…¨å…¼å®¹ RCE/GRPO
```

---

# æ–¹æ¡ˆ 10ï¼šV4-10 STOP è‡ªé€‚åº” shotï¼ˆéœ€è¦æŠŠ cand_pool æ‰©æˆ K+1ï¼Œä½† RCE/GRPO ä»ä¸å˜ï¼‰

> ä½ æŠ¥å‘Šé‡Œ shot3 å¸¸æ‰ï¼Œè¿™ä¸ªæ–¹æ¡ˆç›´æ¥è§£å†³â€œå¤šé€‰åè€Œä¼¤â€çš„æ ¹å› ã€‚  
> ä½†å®ƒä¸æ˜¯çº¯æ¨¡å‹æ”¹åŠ¨ï¼š**éœ€è¦ä¸Šæ¸¸æ„é€  cand_emb æ—¶è¿½åŠ ä¸€ä¸ª STOP å€™é€‰**ï¼Œå¦åˆ™ä¼šè¿å `logits.last_dim == cand_emb.shape[1]` çš„ç¡¬çº¦æŸã€‚

## ä¸Šæ¸¸æ”¹åŠ¨ï¼ˆæœ€å°ï¼‰
åœ¨ç”Ÿæˆ cand_emb çš„åœ°æ–¹ï¼ˆä¾‹å¦‚ embedding export / samplerï¼‰ï¼š
```py
stop_vec = stop_token.expand(B,1,d_model)                 # stop_token: learnable æˆ–å¸¸é‡
cand_emb = torch.cat([cand_emb, stop_vec], dim=1)         # [B, K+1, d]
# labels / beam_labels ä¹Ÿå…è®¸å‡ºç° index = K ä»£è¡¨ STOP
# è‹¥æå‰ STOPï¼Œåˆ™åç»­ step å…¨å¡« STOPï¼ˆä¿æŒåºåˆ—é•¿åº¦ S ä¸å˜ï¼‰
```

## æ¨¡å‹ forward ä¼ªä»£ç ï¼ˆå…³é”®æ˜¯â€œé‡åˆ° STOP åå†»ç»“â€ï¼‰

```py
ended = torch.zeros(B, dtype=torch.bool, device=device)

for step in range(S):
    scores = score(h, cand_proj)                           # [B,K+1]
    scores = scores.masked_fill(selected_mask, -100.0)

    # å¦‚æœå·²ç» endedï¼šåªå…è®¸é€‰ STOP
    # stop_idx = actual_K-1 (å› ä¸º cand_emb å·²ç»æ‰©æˆ K+1)
    stop_idx = actual_K - 1
    scores = torch.where(
        ended.unsqueeze(1),
        torch.full_like(scores, -100.0).scatter(1, torch.full([B,1], stop_idx, device=device), 0.0),
        scores
    )

    idx = labels[:,step] if training else argmax(scores)

    ended = ended | (idx == stop_idx)

    selected_mask = selected_mask.scatter(1, idx.unsqueeze(1), True)
    update_state(...)
```

---

## æœ€åï¼šæˆ‘å»ºè®®ä½ æ€ä¹ˆâ€œé€æ­¥æ¢ç´¢â€ï¼ˆæœ€ç¨³ï¼‰

1) **å…ˆåš V4-1**ï¼ˆæœ€å°æ”¹åŠ¨ï¼Œé«˜æ¦‚ç‡ç«‹ç«¿è§å½±ï¼‰  
2) å¦‚æœ shotâ‰¥3 ä»æ‰ï¼šç›´æ¥å  **V4-3ï¼ˆMMR å¤šæ ·æ€§æ®‹å·®ï¼‰**  
3) éœ€è¦æ›´å¼ºï¼šæŠŠçŠ¶æ€æ›´æ–°ä» gate å‡çº§ä¸º **V4-2ï¼ˆGRUï¼‰**  
4) å†è¿½æ±‚ç¨³å®šä¸ä¸Šé™ï¼šåŠ  **V4-4ï¼ˆcand set encoderï¼‰** æˆ– **V4-6ï¼ˆcoverageï¼‰**  
5) å¦‚æœä½ ç¡®è®¤â€œshot è¶Šå¤šè¶Šä¼¤â€æ˜¯ç³»ç»Ÿæ€§ç°è±¡ï¼šå†åš **V4-10 STOP**

---

## é™„ï¼šä½ æ”¹å®Œä»¥åæœ€å®¹æ˜“è¸©çš„å‘ï¼ˆä¸ç°æœ‰ä»£ç ä¸€è‡´ï¼‰

- **ä¸è¦ in-place æ”¹ selected_mask**ï¼šç»§ç»­ç”¨ `selected_mask = selected_mask.scatter(...)`  
- **mask å€¼å»ºè®®ä¿æŒ -100.0**ï¼šä½  `compute_loss` é‡Œä¼š clamp åˆ° `min=-100`ï¼Œè¿™å¥—æ•°å€¼ç¨³å®šæ˜¯é…å¥—çš„  
- **teacher forcing æ—¶ï¼ŒçŠ¶æ€æ›´æ–°ä¹Ÿå¿…é¡»ç”¨ label idx**ï¼šå¦åˆ™ `compute_log_probs()` ä¼šç®—é”™æ¡ä»¶æ¦‚ç‡  
- **åŠ¨æ€ K**ï¼šæ‰€æœ‰åœ°æ–¹éƒ½ç”¨ `actual_K = cand_emb.shape[1]`ï¼Œä¸è¦ç”¨ `self.K`

---

ï¼ˆå®Œï¼‰


---

# æ–¹æ¡ˆ 5 å®éªŒç»“æœï¼šV4-5 Additive/Bilinear Attentionï¼ˆ2025-12-27 æ›´æ–°ï¼‰

## 5.3 è®­ç»ƒé…ç½®

| é…ç½®é¡¹ | V4-5 | è¯´æ˜ |
|--------|------|------|
| **RL_DATA** | rl_data_k64_v3_balanced.json | ä¸æ–¹æ¡ˆäº”ç›¸åŒ |
| **RCE_EPOCHS** | **15** | å¢åŠ åˆ°15ï¼ˆå› ä¸ºAttentionå‚æ•°éšæœºåˆå§‹åŒ–éœ€è¦æ›´å¤šè®­ç»ƒï¼‰ |
| GRPO_EPOCHS | 50 | ä¸æ–¹æ¡ˆäº”ç›¸åŒ |
| KL_BETA | 0.1 | ä¸æ–¹æ¡ˆäº”ç›¸åŒ |
| GRPO_LR | 5e-6 | ä¸æ–¹æ¡ˆäº”ç›¸åŒ |

## 5.4 å®éªŒç»“æœï¼ˆ800 samplesï¼‰

### V4-5 Additive Attentionï¼ˆEpoch 1ï¼‰

| Shot | Baseline | æ–¹æ¡ˆäº” Epoch 2 | **V4-5 Additive Epoch 1** | V4-5 vs Baseline | V4-5 vs æ–¹æ¡ˆäº” |
|------|----------|----------------|---------------------------|------------------|----------------|
| **1** | 48.55% | 50.15% | **50.05%** | **+1.50%** â¬†ï¸ | -0.10% â¬‡ï¸ |
| **2** | 47.75% | 48.33% | **48.65%** | **+0.90%** â¬†ï¸ | **+0.32%** â¬†ï¸ |
| **3** | 48.15% | 47.40% | 47.48% | -0.67% â¬‡ï¸ | +0.08% â¬†ï¸ |
| **4** | 47.45% | 47.52% | **47.77%** | **+0.32%** â¬†ï¸ | **+0.25%** â¬†ï¸ |
| **å¹³å‡** | 47.98% | 48.35% | **48.49%** | **+0.51%** â¬†ï¸ | **+0.14%** â¬†ï¸ |

### V4-5 Bilinear Attentionï¼ˆEpoch 2ï¼‰

| Shot | Baseline | æ–¹æ¡ˆäº” Epoch 2 | **V4-5 Bilinear Epoch 2** | V4-5 vs Baseline | V4-5 vs æ–¹æ¡ˆäº” |
|------|----------|----------------|---------------------------|------------------|----------------|
| **1** | 48.55% | 50.15% | **50.15%** | **+1.60%** â¬†ï¸ | **0.00%** â¡ï¸ |
| **2** | 47.75% | 48.33% | 47.55% | -0.20% â¬‡ï¸ | -0.78% â¬‡ï¸ |
| **3** | 48.15% | 47.40% | 47.10% | -1.05% â¬‡ï¸ | -0.30% â¬‡ï¸ |
| **4** | 47.45% | 47.52% | 47.15% | -0.30% â¬‡ï¸ | -0.37% â¬‡ï¸ |
| **å¹³å‡** | 47.98% | 48.35% | 47.99% | +0.01% â¬†ï¸ | -0.36% â¬‡ï¸ |

### Additive vs Bilinear å¯¹æ¯”

| Shot | V4-5 Additive | V4-5 Bilinear | å·®å¼‚ | æ›´ä¼˜ |
|------|---------------|---------------|------|------|
| **1** | 50.05% | **50.15%** | +0.10% | Bilinear |
| **2** | **48.65%** | 47.55% | -1.10% | **Additive** ğŸ† |
| **3** | **47.48%** | 47.10% | -0.38% | **Additive** ğŸ† |
| **4** | **47.77%** | 47.15% | -0.62% | **Additive** ğŸ† |
| **å¹³å‡** | **48.49%** | 47.99% | -0.50% | **Additive** ğŸ† |

## 5.5 ç»“è®º

1. **V4-5 Additive Attention æ˜¯ç›®å‰æœ€ä¼˜æ–¹æ¡ˆ**ï¼šå¹³å‡å‡†ç¡®ç‡ 48.49%ï¼Œè¶…è¿‡æ–¹æ¡ˆäº” 0.14%
2. **Additive æ˜æ˜¾ä¼˜äº Bilinear**ï¼šå¹³å‡å·®è· 0.50%
3. **Additive åœ¨ Shot 2/4 ä¸Šè¡¨ç°æœ€å¥½**ï¼š
   - Shot 2: 48.65%ï¼ˆè¶…è¿‡æ–¹æ¡ˆäº” 0.32%ï¼‰
   - Shot 4: 47.77%ï¼ˆè¶…è¿‡æ–¹æ¡ˆäº” 0.25%ï¼‰
4. **Bilinear ä¸æ¨è**ï¼šä»… Shot 1 ä¸æ–¹æ¡ˆäº”æŒå¹³ï¼Œå…¶ä»– shot å‡è¾ƒå·®
5. **è®­ç»ƒæ³¨æ„äº‹é¡¹**ï¼šéœ€è¦å¢åŠ  RCE epochs åˆ° 15ï¼Œå› ä¸º Attention å‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„

## 5.6 Checkpoint ä½ç½®

- V4-5 Additive: `results/okvqa/model_cpk/v3_plan_v4_5_additive/grpo_epoch1.pt`
- V4-5 Bilinear: `results/okvqa/model_cpk/v3_plan_v4_5_bilinear/grpo_epoch2.pt`

## 5.7 æ¨èä½¿ç”¨

```bash
# è®­ç»ƒ V4-5 Additiveï¼ˆæ¨èï¼‰
bash scripts/train_v3_plan_v4_5.sh [gpu_id] additive

# æ¨ç†
bash scripts/inference_v4_5.sh [gpu_id] 1 additive
```


---

# æ–¹æ¡ˆ 7 å®ç°ï¼šV4-7 (N)DPP / log-det é£æ ¼é›†åˆå¢ç›Šï¼ˆ2025-12-27 å®ç°ï¼‰

## 7.1 å®ç°æ¦‚è¿°

V4-7 æ–¹æ¡ˆå·²å®Œæˆå®ç°ï¼Œæ ¸å¿ƒç‰¹ç‚¹ï¼š
- ä½¿ç”¨ä½ç§©ç‰¹å¾åš logdet è¿‘ä¼¼å¢ç›Š
- ç”¨ "ä¸å·²é€‰é›†åˆçš„æœ€å¤§ç›¸ä¼¼åº¦" è¿‘ä¼¼ logdet å¢ç›Š
- å¼ºé›†åˆå»ºæ¨¡ï¼Œå¢å¼ºå¤šæ ·æ€§é€‰æ‹©èƒ½åŠ›

## 7.2 æ ¸å¿ƒæ”¹åŠ¨

### æ–°å¢æ¨¡å—
```python
# DPP ä½ç§©æŠ•å½±çŸ©é˜µï¼šå°† hidden_dim æŠ•å½±åˆ° dpp_rank ç»´
self.dpp_proj = nn.Linear(hidden_dim, dpp_rank, bias=False)

# å¯å­¦ä¹ çš„ DPP å¢ç›Šæƒé‡
self.dpp_lambda = nn.Parameter(torch.tensor(dpp_lambda_init if dpp_lambda_init != 0.0 else -2.0))
```

### æ ¸å¿ƒç®—æ³•ï¼ˆæ¯æ­¥è®¡ç®— diversity gainï¼‰
```python
# é¢„è®¡ç®— DPP ç‰¹å¾
dpp_features = F.normalize(self.dpp_proj(cand_proj_norm), p=2, dim=-1)  # [B, K, r]

# è®¡ç®—æ¯ä¸ªå€™é€‰ä¸å·²é€‰é›†åˆçš„ç›¸ä¼¼åº¦
if step > 0 and len(selected_dpp_features) > 0:
    selected_stack = torch.stack(selected_dpp_features, dim=1)  # [B, step, r]
    sim = torch.einsum("bkr,btr->bkt", dpp_features, selected_stack)  # [B, K, step]
    max_sim = sim.max(dim=-1).values  # [B, K]
    
    # DPP diversity gain: log(1 - sim^2) çš„è¿‘ä¼¼
    diversity_gain = torch.log(1e-6 + 1.0 - max_sim.pow(2).clamp(max=0.999))
    scores = base_scores + F.softplus(self.dpp_lambda) * diversity_gain
```

## 7.3 æ–‡ä»¶ç»“æ„

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `lever_lm/models/v2/pointer_selector_v4_7.py` | V4-7 åŸºç¡€æ¨¡å‹ |
| `lever_lm/models/v3/pointer_selector_v4_7_rl.py` | V4-7 RL ç‰ˆæœ¬ï¼ˆRCE + GRPOï¼‰ |
| `lever_lm/workflows/grpo_post_train_v4_7.py` | V4-7 è®­ç»ƒ workflow |
| `scripts/train_v3_plan_v4_7.sh` | è®­ç»ƒè„šæœ¬ |
| `scripts/inference_v4_7.sh` | æ¨ç†è„šæœ¬ |
| `scripts/convert_v4_7_to_v2_format.py` | Checkpoint è½¬æ¢è„šæœ¬ |

## 7.4 è®­ç»ƒé…ç½®

| é…ç½®é¡¹ | V4-7 | è¯´æ˜ |
|--------|------|------|
| **RL_DATA** | rl_data_k64_v3_balanced.json | ä¸å…¶ä»–æ–¹æ¡ˆç›¸åŒ |
| **RCE_EPOCHS** | **15** | å¢åŠ åˆ°15ï¼ˆå› ä¸º DPP å‚æ•°éœ€è¦æ›´å¤šè®­ç»ƒï¼‰ |
| GRPO_EPOCHS | 50 | ä¸å…¶ä»–æ–¹æ¡ˆç›¸åŒ |
| KL_BETA | 0.1 | ä¸å…¶ä»–æ–¹æ¡ˆç›¸åŒ |
| GRPO_LR | 5e-6 | ä¸å…¶ä»–æ–¹æ¡ˆç›¸åŒ |
| **dpp_rank** | 32 | DPP ä½ç§©æŠ•å½±ç»´åº¦ï¼ˆå¯è°ƒæ•´ï¼š16/32/64ï¼‰ |

## 7.5 ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒ
```bash
# ä½¿ç”¨é»˜è®¤ dpp_rank=32
bash scripts/train_v3_plan_v4_7.sh [gpu_id]

# æŒ‡å®š dpp_rank
bash scripts/train_v3_plan_v4_7.sh [gpu_id] 64
```

### æ¨ç†
```bash
# ä½¿ç”¨ RCE epoch 2
bash scripts/inference_v4_7.sh [gpu_id] rce 2 32

# ä½¿ç”¨ GRPO epoch 1
bash scripts/inference_v4_7.sh [gpu_id] grpo 1 32
```

## 7.6 é¢„æœŸæ•ˆæœ

æ ¹æ®æ–‡æ¡£åˆ†æï¼ŒV4-7 çš„é¢„æœŸæ•ˆæœï¼š
- **æˆåŠŸæ¦‚ç‡**ï¼šä¸­-ä½ï¼ˆå› ä¸ºæ•°å€¼/å·¥ç¨‹æ›´éš¾ï¼‰
- **ä¸»è¦æ”¶ç›Š**ï¼šå¼ºé›†åˆå»ºæ¨¡ï¼Œå¢å¼ºå¤šæ ·æ€§
- **é€‚ç”¨åœºæ™¯**ï¼šå½“ shotâ‰¥3 å‡ºç°æ˜æ˜¾å†—ä½™æ—¶

## 7.7 å®éªŒç»“æœ

ï¼ˆå¾…è®­ç»ƒå®Œæˆåæ›´æ–°ï¼‰

| Shot | Baseline | V4-5 Additive | **V4-7 DPP** | V4-7 vs Baseline | V4-7 vs V4-5 |
|------|----------|---------------|--------------|------------------|--------------|
| **1** | 48.55% | 50.05% | - | - | - |
| **2** | 47.75% | 48.65% | - | - | - |
| **3** | 48.15% | 47.48% | - | - | - |
| **4** | 47.45% | 47.77% | - | - | - |
| **å¹³å‡** | 47.98% | 48.49% | - | - | - |

## 7.8 æ³¨æ„äº‹é¡¹

1. **dpp_rank é€‰æ‹©**ï¼š
   - é»˜è®¤ 32ï¼Œä¸ hidden_dim=256 é…åˆ
   - å¤ªå°ï¼ˆ<16ï¼‰å¯èƒ½è¡¨è¾¾èƒ½åŠ›ä¸è¶³
   - å¤ªå¤§ï¼ˆ>64ï¼‰å¯èƒ½è¿‡æ‹Ÿåˆ

2. **dpp_lambda åˆå§‹åŒ–**ï¼š
   - é»˜è®¤ 0.0ï¼ˆå®é™…ä½¿ç”¨ softplus(-2.0) â‰ˆ 0.127ï¼‰
   - è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨å­¦ä¹ 

3. **æ•°å€¼ç¨³å®šæ€§**ï¼š
   - diversity_gain ä½¿ç”¨ `log(1e-6 + 1.0 - sim^2.clamp(max=0.999))` é¿å…æ•°å€¼é—®é¢˜
   - å½“ sim æ¥è¿‘ 1 æ—¶ï¼Œgain ä¸ºè´Ÿï¼ˆæƒ©ç½šé‡å¤ï¼‰
