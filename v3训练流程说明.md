# v3 训练流程说明

## v3 训练的三个阶段

v3 的训练流程包含三个阶段，按顺序执行：

### 阶段1：加载 SFT 模型并计算 old_log_probs

- **目的**：为后续的 GRPO 训练准备参考模型
- **操作**：
  1. 加载 v2 SFT checkpoint（作为基线模型）
  2. 创建 SFT 模型的副本
  3. 在训练数据上计算 `old_log_probs`（用于 GRPO 训练中的 PPO ratio 计算）

### 阶段2：RCE 预热（Reward-weighted Cross-Entropy）

- **目的**：平滑过渡，将模型从"贴近人工标签"拉到"贴近高 reward 的 beam/sample"
- **特点**：
  - RCE = Reward-weighted Cross-Entropy
  - 在监督学习的 CE loss 基础上，加上 reward 的权重
  - reward 高的样本在 loss 中权重大，reward 低的样本影响被压低
- **训练方式**：
  ```python
  loss = RCE_loss(query_emb, cand_emb, labels, rewards, temperature)
  ```
- **推荐设置**（来自强化学习.md）：
  - `rce_epochs = 1~2`（我们当前用 3 或 25）
  - `rce_temperature` 从 2.0 线性降到 0.5
  - 学习率比 SFT 小 3~10 倍（如 1e-5）
- **输出**：保存 `rce_epoch*.pt` checkpoint

### 阶段3：GRPO 训练（Group-Relative Policy Optimization）

- **目的**：使用强化学习进一步优化模型，最大化 reward
- **特点**：
  - GRPO = Group-Relative Policy Optimization（带 PPO ratio）
  - 使用组内 advantage（group-relative advantage）
  - 使用 PPO clip 机制防止策略更新过大
  - 添加 KL 散度惩罚，保持与 SFT 模型的相似度
- **训练方式**：
  ```python
  # 计算 advantage（组内归一化）
  advantage = normalize_reward_in_group(rewards) - baseline
  
  # PPO loss
  ratio = exp(current_log_probs - old_log_probs)
  policy_loss = -mean(min(ratio * advantage, clip(ratio, 1-eps, 1+eps) * advantage))
  
  # KL penalty
  kl_loss = kl_beta * mean(ratio - 1 - log(ratio))
  
  total_loss = policy_loss + kl_loss
  ```
- **推荐设置**：
  - `grpo_epochs = 3~10`（我们当前用 8 或 25）
  - `grpo_lr = 1e-5`（比 RCE 稍小）
  - `kl_beta = 0.1`（控制与 SFT 模型的偏离程度）
- **输出**：保存 `grpo_epoch*.pt` checkpoint

## 完整训练流程

```
SFT 模型 (v2)
    ↓
[阶段1] 加载 SFT checkpoint，计算 old_log_probs
    ↓
[阶段2] RCE 预热（1-3 epochs）
    ↓ 保存 rce_epoch*.pt
[阶段3] GRPO 训练（8-25 epochs）
    ↓ 保存 grpo_epoch*.pt
最终模型 (v3)
```

## 关键理解

1. **RCE 是必需的预热阶段**：
   - 不是可选的，而是 v3 训练流程的一部分
   - 它帮助模型从监督学习平滑过渡到强化学习
   - 如果跳过 RCE，直接进行 GRPO，可能会导致训练不稳定

2. **RCE 和 GRPO 的关系**：
   - RCE：更像有监督学习，使用 reward 加权
   - GRPO：真正的强化学习，使用 advantage 和 PPO
   - RCE 为 GRPO 打好基础

3. **当前问题**：
   - 优化后的参数（RCE_EPOCHS=3, GRPO_EPOCHS=8）表现更差
   - 可能原因：
     - RCE 预热不够（3 epochs 可能太少）
     - GRPO 训练不够（8 epochs 可能太少）
     - 或者 GRPO 方法本身不适合这个任务

## 建议

如果 RCE 预热不够，可以尝试：
1. 增加 RCE epochs（如 5-10）
2. 确保 RCE 阶段的 loss 充分下降
3. 然后再进行 GRPO 训练

或者，如果 RCE 已经足够，但 GRPO 效果不好，可以考虑：
1. 只使用 RCE（不进行 GRPO），看看效果如何
2. 调整 GRPO 的参数（如降低 kl_beta，调整 reward 权重）




