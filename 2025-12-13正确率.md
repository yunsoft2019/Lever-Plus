# Lever-Plus v3 正确率对比报告（2025-12-13）

## 测试配置

- **数据集**: OKVQA
- **推理模型**: Qwen2.5-VL-3B-Instruct
- **采样器**: RandSampler
- **测试日期**: 2025-12-13

---

## 本次实验：Balanced 数据 + Separated Reward 模式

### 实验背景

根据 `2025-12-23需求.md` 的建议，本次实验尝试：

1. **候选平衡（Candidate Balancing）**：
   - 对每个 query 保留 top-P 正样本（按 vqa_acc_score 排序）
   - 保留 top-N hard negatives（vqa_correct=0 但 vqa_acc_score 最高）
   - 过滤正样本不足 3 个的 query

2. **Separated Reward 模式**：
   - 正样本 reward: `2.0 + vqa_acc_score` ∈ [2, 3]
   - 负样本 reward: `vqa_acc_score` ∈ [0, 1]
   - 正负样本之间有 1.0 的硬 gap

### 数据统计

| 指标 | 原始 merged 数据 | balanced 数据 | 变化 |
|------|-----------------|---------------|------|
| Query 总数 | 800 | 552 | -31% |
| 候选总数 | 9,594 | 5,520 | -42% |
| 正样本比例 | 33.36% | 57.48% | +24% |
| 每 query 候选数 | ~12 | 10 | 固定 |

### 训练配置

```bash
python -m lever_lm.workflows.grpo_post_train \
    --beam_data ./results/okvqa/generated_data/rl_data_balanced.json \
    --sft_ckpt ./results/okvqa_bak/model_cpk/v2/Qwen2_5_VL_3B_Instruct_RandSampler_*.ckpt \
    --rce_epochs 5 --grpo_epochs 0 \
    --reward_mode separated \
    --rce_use_raw_reward \
    --require_positive_query \
    --output_dir ./results/okvqa/model_cpk/v3_balanced_separated
```

### 模型检查点

- **Balanced 模型**: `results/okvqa/model_cpk/v3_balanced_separated/rce_epoch5_v2format.ckpt`
- **Baseline 模型**: `results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/rce_epoch5_v2format.ckpt`

---

## 推理结果对比

### Balanced 模型 vs Baseline 完整对比

| 测试数据量 | Shot | Baseline (12-12) | Balanced (12-13) | 差异 | 评价 |
|-----------|------|------------------|------------------|------|------|
| **100** | 1 | 64.2% | 63.2% | -1.0% | ⬇️ |
| **100** | 2 | 64.8% | 65.8% | **+1.0%** | ⬆️ |
| **100** | 3 | 62.8% | 61.2% | -1.6% | ⬇️ |
| **100** | 4 | 61.4% | 60.8% | -0.6% | ⬇️ |
| **200** | 1 | 57.2% | 56.5% | -0.7% | ⬇️ |
| **200** | 2 | 56.3% | 56.6% | +0.3% | ⬆️ |
| **200** | 3 | 54.9% | 53.6% | -1.3% | ⬇️ |
| **200** | 4 | 54.9% | 54.2% | -0.7% | ⬇️ |
| **400** | 1 | 52.6% | 53.25% | +0.65% | ⬆️ |
| **400** | 2 | 51.2% | 51.0% | -0.2% | ➡️ |
| **400** | 3 | 51.25% | 49.5% | -1.75% | ⬇️ |
| **400** | 4 | 49.75% | 50.45% | +0.7% | ⬆️ |
| **800** | 1 | 48.55% | 49.75% | **+1.2%** | ⬆️ |
| **800** | 2 | 47.75% | 47.8% | +0.05% | ➡️ |
| **800** | 3 | 48.15% | 47.0% | -1.15% | ⬇️ |
| **800** | 4 | 47.45% | 48.12% | +0.67% | ⬆️ |

### 统计汇总

| 指标 | 值 |
|------|-----|
| 平均差异 | **-0.27%** |
| 最大下降 | -1.75% (400 samples, shot 3) |
| 最大提升 | +1.2% (800 samples, shot 1) |
| 提升的配置数 | 7/16 (43.75%) |
| 下降的配置数 | 7/16 (43.75%) |
| 持平的配置数 | 2/16 (12.5%) |

### 按 Shot 分析

| Shot | 平均差异 | 趋势 |
|------|---------|------|
| Shot 1 | +0.04% | 基本持平 |
| Shot 2 | +0.29% | 略有提升 |
| Shot 3 | **-1.45%** | 明显下降 |
| Shot 4 | +0.01% | 基本持平 |

### 按样本量分析

| 样本量 | 平均差异 | 趋势 |
|--------|---------|------|
| 100 | -0.55% | 略有下降 |
| 200 | -0.60% | 略有下降 |
| 400 | -0.15% | 基本持平 |
| 800 | +0.19% | 略有提升 |

---

## 三个模型完整对比

| 测试数据量 | Shot | Baseline | Merged | Balanced | 最佳 |
|-----------|------|----------|--------|----------|------|
| **100** | 1 | 64.2% | 63.2% | 63.2% | Baseline |
| **100** | 2 | 64.8% | 64.8% | **65.8%** | Balanced |
| **100** | 3 | 62.8% | 63.2% | 61.2% | Merged |
| **100** | 4 | 61.4% | 61.8% | 60.8% | Merged |
| **200** | 1 | **57.2%** | 56.7% | 56.5% | Baseline |
| **200** | 2 | 56.3% | 55.6% | **56.6%** | Balanced |
| **200** | 3 | **54.9%** | 55.4% | 53.6% | Baseline/Merged |
| **200** | 4 | **54.9%** | 54.7% | 54.2% | Baseline |
| **400** | 1 | 52.6% | 52.45% | **53.25%** | Balanced |
| **400** | 2 | **51.2%** | 50.2% | 51.0% | Baseline |
| **400** | 3 | **51.25%** | 50.2% | 49.5% | Baseline |
| **400** | 4 | 49.75% | **50.25%** | **50.45%** | Balanced |
| **800** | 1 | 48.55% | 49.17% | **49.75%** | Balanced |
| **800** | 2 | **47.75%** | 47.35% | 47.8% | Balanced |
| **800** | 3 | **48.15%** | 47.48% | 47.0% | Baseline |
| **800** | 4 | 47.45% | **48.08%** | 48.12% | Balanced |

### 各模型最佳配置数

| 模型 | 最佳配置数 | 占比 |
|------|-----------|------|
| Baseline | 8 | 50% |
| Balanced | 7 | 43.75% |
| Merged | 3 | 18.75% |

（注：部分配置有并列最佳）

---

## 原因分析

### 1. 为什么 Balanced 模型没有显著提升？

#### 1.1 数据量减少

- Balanced 数据只有 552 个 query（原来 800 个）
- 过滤掉了 31% 的 query（正样本不足 3 个的）
- **训练数据量减少可能导致模型泛化能力下降**

#### 1.2 正样本比例过高

- Balanced 数据正样本比例达到 57.48%
- 这可能导致模型"过于乐观"，对负样本的区分能力下降
- **理想的正负样本比例可能在 20-40% 之间**

#### 1.3 Separated Reward 的 Gap 效应

- Separated 模式在正负样本之间有 1.0 的硬 gap
- 当正样本比例很高时，这个 gap 可能导致模型过度偏向正样本
- **在高正样本比例下，hard_plus_soft 可能更合适**

### 2. 为什么 Shot 3 下降最明显？

- Shot 3 需要模型选择 3 个 ICD，对排序能力要求更高
- Balanced 数据过滤了很多"困难"的 query，模型可能没学到如何处理这些情况
- **训练数据的多样性下降影响了多 shot 场景**

### 3. 为什么大样本量（800）表现相对更好？

- 大样本量测试覆盖更多 query 类型
- Balanced 模型在"简单" query 上表现更好
- 小样本量测试可能恰好包含更多"困难" query

---

## 结论

### 本次实验结论

1. **Balanced 数据 + Separated Reward 没有带来显著提升**
   - 平均差异 -0.27%，基本持平
   - Shot 3 场景下降明显（-1.45%）

2. **候选平衡策略需要调整**
   - 当前 57.48% 的正样本比例可能过高
   - 建议尝试 30-40% 的正样本比例

3. **Separated Reward 在高正样本比例下可能不是最优**
   - 考虑在 balanced 数据上使用 hard_plus_soft 模式

### 当前最佳模型

**仍然是 Baseline 模型**（`v3_RandSampler_Qwen2_5-VL-3B-Instruct/rce_epoch5_v2format.ckpt`）

| 测试数据量 | Shot 1 | Shot 2 | Shot 3 | Shot 4 |
|-----------|--------|--------|--------|--------|
| **100** | 64.2% | 64.8% | 62.8% | 61.4% |
| **200** | 57.2% | 56.3% | 54.9% | 54.9% |
| **400** | 52.6% | 51.2% | 51.25% | 49.75% |
| **800** | 48.55% | 47.75% | 48.15% | 47.45% |

---

## 后续建议

### 短期优化（推荐优先尝试）

1. **调整正样本比例**
   - 在 balanced 数据基础上，随机采样减少正样本
   - 目标：30-40% 正样本比例
   ```bash
   python lever_lm/models/v3/mine_positive_samples.py balance \
       --input rl_data_merged.json \
       --output rl_data_balanced_v2.json \
       --max_pos_per_query 4 \
       --max_neg_per_query 6
   ```

2. **在 Balanced 数据上使用 hard_plus_soft**
   ```bash
   python -m lever_lm.workflows.grpo_post_train \
       --beam_data ./results/okvqa/generated_data/rl_data_balanced.json \
       --reward_mode hard_plus_soft \
       --rce_epochs 5 --grpo_epochs 0
   ```

3. **尝试 GRPO 微调**
   - 在 RCE baseline 基础上加 1-2 epochs GRPO
   - 使用较小学习率和强 KL 约束

### 中期优化

4. **不过滤 query，只做候选平衡**
   - 保留所有 800 个 query
   - 对每个 query 做候选平衡（即使正样本不足 3 个）

5. **混合 reward 策略**
   - 对正样本多的 query 用 separated
   - 对正样本少的 query 用 hard_plus_soft

---

## 更新记录

- **2025-12-13**:
  - 完成 Balanced 数据 + Separated Reward 模式实验
  - 在 GPU 5/6/7 上完成 200/400/800 样本推理
  - 结论：整体表现与 Baseline 持平，Shot 3 下降明显
  - 分析原因：正样本比例过高、训练数据量减少
  - 提出后续优化建议
