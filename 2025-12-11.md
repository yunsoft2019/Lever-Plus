# Lever-Plus v3 强化学习改进计划书（基于 2025-12-10 结果）

> 本计划书基于当前 GitHub 仓库代码（`Lever-Plus`）与仓库中的 RL 文档、实验记录，特别是  
> `2025_12_10正确率.md` 的最新结果，结合你之前的设计意图，给出一套**从现在代码出发**的改进方案。  
> 目标是：在不推翻现有 v3 设计的前提下，稳步放大 RL 带来的收益，并避免再次出现 v3 崩盘的情况。

---

## 0. 一页纸总结

### 现状

1. **数据与 Reward 设计**

   - RL 数据已经从「增益得分」完全切换到「整条 pointer 序列的绝对正确率」：

     - 每条候选 pointer 都有：

       - `vqa_correct ∈ {0, 1}`
       - `vqa_acc_score ∈ [0, 1]`

     - `reward_utils.compute_reward_for_candidate` 默认使用：

       ```python
       reward_mode = "hard_plus_soft"
       reward = vqa_correct + vqa_acc_score   # ∈ [0, 2]
       # 正样本：[1, 2]；负样本：[0, 1)
       ```

   - RL JSON 格式统一为：

     ```jsonc
     {
       "query_id_str": {
         "pointer_candidates": [
           {
             "pointer": [i, j],
             "beam_score": ...,
             "logprob_score": ...,
             "gen_method": "beam" | "sample" | "random",
             "vqa_pred_answer": "...",
             "vqa_correct": 0/1,
             "vqa_acc_score": 0.x,
             "vqa_eval_mode": "file" | "fallback"
           },
           ...
         ]
       },
       ...
     }
     ```

2. **当前最好结果：v3（RCE-only, 5 epoch, 无 GRPO）**

   - 基于最新的 `2025_12_10正确率.md`：
     - 在 **shot_num=2** 上，v3（RCE-only）在 100/200/400/800 条数据上 **全部优于 v2**，增益约 `+0.27% ~ +1.0%`。
     - 在 **shot_num=1** 上，v3 在小样本（100/200）略优于 v2，大样本（400/800）略弱 0.3% ～ 0.5%。
     - 在 **shot_num ≥ 3** 上，大多数情况下 v3 与 v2 接近或略低，一般在 `±0.7%` 范围内。
   - 总体评价：
     - ✅ 证明：**新 reward + RCE-only 是有效的**；
     - ❌ 提升幅度偏小，尤其对 1-shot / 3/4-shot 影响有限。

3. **GRPO 部分**

   - 历史实验（`GRPO训练优化建议*.md`）表明：
     - 在 v3 早期版本上强行使用 GRPO，会导致整体准确率明显下降（崩盘）。
   - 目前最新的 v3（RCE-only）实验中，**GRPO 是关闭的**（`GRPO_EPOCHS=0`），因此现有最优结果完全来自 RCE。

### 关键结论

- ✅ 新 reward（绝对正确率 hard+soft）是正确方向；
- ✅ RCE-only 已经提供了可观的稳定增益（尤其 2-shot）；
- ⚠️ 目前的限制：
  - RL 目标只在 2-shot 上有直接监督；
  - RCE 训练对 reward 做了强归一化，削弱了正负样本 gap；
  - GRPO 目前还处于「危险状态」，一旦打开容易把模型拉坏。

> **短期策略**：  
> 把「v3 RCE-only（5 epoch, hard_plus_soft）」作为稳定 baseline，  
> 通过更合理的 reward 使用方式（raw reward + 更强正负间隔 + 数据过滤）放大收益；  
> GRPO 只作为谨慎实验，先修掉双重归一化和训练范围问题。

---

## 1. 现有 RL 流水线回顾（基于最新代码）

### 1.1 数据与 embedding 导出

- 位置：`lever_lm/models/v3/export_embeddings.py`

- 逻辑：

  - 使用 OKVQA 训练集 `train_ds`；

  - 对每个样本提取：

    - `query_embeddings[i]`：query embedding；
    - `candidate_embeddings[i]`：candidate embedding（候选池=训练集本身）。

  - 最终保存：

    ```python
    query_embeddings.pt    # [N, d]
    candidate_embeddings.pt  # [N, d]
    ```

### 1.2 RL 数据生成

- 位置：`lever_lm/models/v3/generate_rl_data.py`

- 核心步骤：

  1. 加载 pointer SFT 模型（v2/v3）：

     ```python
     sft_model = load_sft_model(args.sft_ckpt, device)
     ```

  2. 对每个 `query_id`：

     - 从 `query_embeddings[query_id]` + 全部 `candidate_embeddings` 出发；

     - 调用：

       ```python
       generate_pointer_candidates_for_query(
           model=sft_model,
           query_emb=query_emb,
           cand_emb=cand_emb,
           num_beams=args.num_beams,
           temps=args.temps,
           num_samples_per_temp=args.num_samples_per_temp,
           num_random=args.num_random,
       )
       ```

     - 得到若干 pointer：

       ```python
       {
         "pointer": [i, j],       # index in [0..K-1]
         "beam_score": ...,
         "logprob_score": ...,
         "gen_method": "beam" / "sample" / "random",
         ...
       }
       ```

  3. 对每条 pointer：

     - 将 `[i, j]` 映射回原始样本（`train_ds[i]`, `train_ds[j]`）；

     - 构造 2-shot VQA prompt（2 条 ICD + query）；

     - 调用 Qwen2.5-VL-3B interface 做推理；

     - 使用 VQA 官方 metric + fallback 匹配计算：

       ```python
       vqa_correct   ∈ {0, 1}
       vqa_acc_score ∈ [0, 1]
       vqa_eval_mode = "file" | "fallback"
       ```

  4. 写入 RL JSON，后续供 RL 训练使用。

### 1.3 Reward 计算

- 位置：`lever_lm/utils/reward_utils.py`

核心接口：

```python
def compute_reward_for_candidate(
    beam_score=None,
    logprob_score=None,
    vqa_correct=None,
    vqa_acc_score=None,
    reward_mode="hard_plus_soft",
    hard_weight=1.0,
    soft_weight=1.0,
    alpha=0.0,
    beta=0.0,
    correctness_mode="01",
    use_logprob=False,
    reward_clip=(-5.0, 5.0),
) -> float:
    ...
```

- 当前实验主要使用：

  ```python
  reward_mode = "hard_plus_soft"
  hard_weight = soft_weight = 1.0
  
  hard = float(vqa_correct)    # 0 or 1
  soft = float(vqa_acc_score)  # [0, 1]
  reward = hard + soft         # [0, 2]
  ```

- 已实现的其他模式（未来可用）：

  - `"hard_plus_soft_v2"`：增加正负 gap；
  - `"separated"`：正样本 [2,3]，负样本 [0,1]；
  - `"hard_only"` / `"soft_only"`；
  - `"hybrid"`：结合 InfoScore 与 correctness；
  - `"legacy"`：完全沿用旧 InfoScore 方案。

### 1.4 RL 数据集（Dataset + Collate）

- 位置：`lever_lm/models/v3/dataset_v3.py`
- 类：`RLBeamDatasetWithEmbedding`

**主要逻辑：**

1. 从 RL JSON 中读取：

   ```python
   rl_data[query_id_str]["pointer_candidates"]
   ```

2. 对每个 pointer：

   ```python
   pointer = c["pointer"]     # [i, j]，位置索引
   mapped_pointer = [self.cand_idx_to_pos[idx] for idx in pointer]  # 严格映射
   
   reward = compute_reward_for_candidate(
       beam_score=c.get("beam_score"),
       logprob_score=c.get("logprob_score"),
       vqa_correct=c.get("vqa_correct"),
       vqa_acc_score=c.get("vqa_acc_score"),
       reward_mode=self.reward_mode,
       hard_weight=self.hard_weight,
       soft_weight=self.soft_weight,
       ...
   )
   ```

3. 存储：

   ```python
   self.samples.append({
       "query_id": query_id,
       "beam_labels": beam_labels,   # [num_candidates, shot_num]
       "beam_rewards": beam_rewards, # 原始 reward
       "beam_logprobs": beam_logprobs,
   })
   ```

4. 在 `__getitem__` 中做一次组内 Z-score：

   ```python
   beam_rewards_raw = torch.tensor(beam_rewards, dtype=torch.float32)
   beam_rewards_normalized = beam_rewards_raw.clone()
   mean = beam_rewards_raw.mean()
   std = beam_rewards_raw.std()
   if std > 1e-12:
       beam_rewards_normalized = (beam_rewards_raw - mean) / std
   
   return {
       "beam_rewards": beam_rewards_normalized if self.normalize_rewards else beam_rewards_raw,
       "beam_rewards_raw": beam_rewards_raw,
       ...
   }
   ```

5. `collate_fn_rl_v3` 简单地把每个 query 组装成一个 batch（batch_size=1）：

   ```python
   {
     "query_id": int,
     "query_emb": [1, d],
     "cand_emb": [1, K, d],
     "beam_labels": [1, num_candidates, shot_num],
     "beam_rewards": [1, num_candidates],
     "beam_rewards_raw": [1, num_candidates],
     "beam_logprobs": [1, num_candidates] (可选)
   }
   ```

### 1.5 训练脚本：RCE & GRPO

- 位置：`lever_lm/workflows/grpo_post_train.py`
- 主要逻辑（简化）：

#### 1.5.1 RCE 阶段

```python
def train_rce_epoch(...):
    for batch in train_loader:
        query_emb = batch["query_emb"].to(device)
        cand_emb = batch["cand_emb"].to(device)
        beam_labels = batch["beam_labels"].to(device)

        beam_rewards      = batch["beam_rewards"].to(device)      # 已 Z-score
        beam_rewards_raw  = batch["beam_rewards_raw"].to(device)  # 原始 reward

        if self.rce_use_raw_reward:
            reward_for_rce = beam_rewards_raw
        else:
            reward_for_rce = beam_rewards

        loss = model.compute_rce_loss(
            query_emb,
            cand_emb,
            beam_labels,
            reward_for_rce,
            temperature=current_temp,
        )
```

- 当前最佳模型（`rce_epoch5.pt`）是在：
  - `rce_use_raw_reward = False`（即使用归一化后的 reward）下训练得到的；
  - `grpo_epochs = 0`，纯 RCE-only。

#### 1.5.2 GRPO 阶段（目前关闭）

```python
def train_grpo_epoch(...):
    for batch in train_loader:
        query_emb = batch["query_emb"].to(device)
        cand_emb = batch["cand_emb"].to(device)
        beam_labels = batch["beam_labels"].to(device)
        beam_rewards = batch["beam_rewards"].to(device)      # 注意：已 Z-score
        old_log_probs = batch["beam_logprobs"].to(device)

        result = model.compute_grpo_loss(
            query_emb, cand_emb, beam_labels,
            beam_rewards, old_log_probs,
            use_top_k=...
        )
```

在 `PointerSelectorV3.compute_grpo_loss` 内部：

- 再次调用 `self.compute_advantage(beam_rewards, normalize=True)` 做 Z-score；
- 即：**GRPO 实际会对 reward 做两次 Z-score**。

> 目前 GRPO 没有在最新实验中启用，但如果未来再开，这里会是一个重要坑。

---

## 2. 结合最新结果：问题到底出在哪里？

### 2.1 正向结论：reward 与 RCE 设计是“正确方向”

从 `2025_12_10正确率.md` 可以看出：

- v3（RCE-only）在所有数据规模上，**2-shot 全面优于 v2**，增益约 `+0.27% ~ +1.0%`；
- 在 1/3/4-shot 上，v3 与 v2 基本持平，差异通常在 `±0.7%` 以内；
- 与 v0/v1 对比，v3 在 shot≥2 上大多数情况下**显著更好**，尤其 2-shot 可以拉开 4~7% 的差距。

这些事实说明：

1. **用“整条 pointer 序列的绝对正确率”作为 reward 是有效的**；
2. **RCE-only 作为一种“reward 加权的监督式 RL”是可靠的**；
3. v3 现在的主要问题是：**收益不够大 & 对 1/3/4-shot 泛化不足**，而不是“RL 完全没用”。

### 2.2 为什么收益不大？

综上代码和结果，可以归纳出几个结构性原因：

1. **训练目标只在 2-shot 上有直接监督**

   - RL 数据全部是「2-shot prompt 的 correctness」；
   - pointer 模型的 `shot_num=2`，所有 reward 都来自这个场景；
   - 评估时却看 1/2/3/4-shot：
     - 1-shot：从未被 RL 直接优化，只能从 2-shot 的表示“顺带受益”；
     - 3/4-shot：完全靠 2-shot 排序的泛化。
   - 这很自然导致：
     - 2-shot 最容易受益；
     - 1/3/4-shot 只能微弱变化，有时略好有时略坏。

2. **RCE 阶段对 reward 做了 Z-score，削弱了 gap**

   - 原始设计里：正样本 reward 在 `[1, 2]`，负样本在 `[0, 1)`，gap 非常直观；

   - Dataset 里先做了一次 group 内 Z-score：

     ```python
     reward -> (reward - mean) / std
     ```

   - RCE 再用标准化后的 reward 做 softmax 权重：

     ```python
     w_i = softmax(reward_normalized / τ)
     ```

   - 这会带来两点影响：

     1. 正负样本 gap 被压平，上下差别更多取决于「组内相对排序」，而非绝对正确率；
     2. 对于“全错 query”或“只有极少差异的 query”，Z-score 会放大噪声，RCE 仍然被迫对这些 query 学习。

3. **RL 数据中“有效差异”本来就有限**

   - 很多 query：所有 pointer 都错（`vqa_correct=0`），且 `vqa_acc_score` 也差不多；
   - 少数 query：有一两个 pointer 是对的，其余都错；
   - 离线 RL 的离策略性质决定了：**模型只能在这批“旧策略生成的候选”里做排序优化**，不可能突破太多。
   - 在这样的设置下，+0.3% ~ +1% 的提升本身就是合理量级。

4. **GRPO 当前的实现有“double Z-score”风险**

   - 早期实验（`GRPO训练优化建议*.md`）已经证明：
     - 一旦大力使用 GRPO（多 epoch + 较大 lr），极容易把整个模型拉崩；
   - 从代码细节看：
     - Dataset 里先做了一次 Z-score；
     - GRPO 又在 `compute_advantage` 中做了一次 Z-score；
   - 这会让 reward 本来的结构完全丢失，PPO 优势更新只剩下“模糊的相对差异”，十分脆弱。

---

## 3. 短期改造计划（1～3 天内可完成）

目标：在**不动 RL 数据**的前提下，对训练脚本和 reward 使用方式做轻量调整，放大 RCE 效果，并为未来的 GRPO 留好“安全阀”。

### 3.1 把 v3（RCE-only, hard_plus_soft）固化为当前 baseline

**目的**：避免误操作用错误的配置覆盖掉当前最优模型，同时为所有新实验提供「统一起点」。

**操作建议：**

1. 在 `grpo_post_train.py` 的 help 与 README 中明确写出推荐 baseline：

   ```bash
   python -m lever_lm.workflows.grpo_post_train \
     --beam_data <rl_data.json> \
     --img_emb <query_embeddings.pt> \
     --rce_epochs 5 \
     --grpo_epochs 0 \
     --reward_mode hard_plus_soft \
     --num_layers 1 \
     --output_dir results/rl/v3_rce_only/
   ```

2. 在代码中给出明显提示：

   ```python
   if self.grpo_epochs <= 0:
       print("[GRPOTrainer] GRPO_EPOCHS=0，仅进行 RCE 预热（RCE-only 模式）。")
       return
   ```

3. 当前 `rce_epoch5.pt` 作为 `v3_rce_only` 的命名固定下来，并在 `2025_12_10正确率.md` 中作为 baseline 标注。

---

### 3.2 新一轮重点实验：RCE 使用 **raw reward + 更强正负 gap**

**目的**：让 reward 的“绝对意义”发挥作用，避免被 Z-score 吃掉；特别是让正样本与负样本之间有更明显的间隔，从而更容易把权重压到真正正确的 pointer 上。

#### 3.2.1 启用 raw reward 训练 RCE

- 在 `grpo_post_train.py` 增加/确认参数：

  ```bash
  --rce_use_raw_reward
  ```

- 对应代码已经存在：

  ```python
  if self.rce_use_raw_reward:
      reward_for_rce = beam_rewards_raw
  else:
      reward_for_rce = beam_rewards
  ```

- 新实验建议配置：

  ```bash
  python -m lever_lm.workflows.grpo_post_train \
    --beam_data <rl_data.json> \
    --img_emb <query_embeddings.pt> \
    --rce_epochs 5 \
    --grpo_epochs 0 \
    --reward_mode separated \
    --hard_weight 1.0 \
    --soft_weight 1.0 \
    --rce_use_raw_reward \
    --skip_fallback_reward \
    --output_dir results/rl/v3_rce_raw_separated/
  ```

#### 3.2.2 改用更强 gap 的 reward_mode：`separated`

- 在 `reward_utils.compute_reward_for_candidate` 中，`separated` 模式已经实现，大致逻辑为：

  ```python
  if reward_mode == "separated":
      # hard: 0/1
      hard = float(vqa_correct) if vqa_correct is not None else 0.0
      soft = float(vqa_acc_score) if vqa_acc_score is not None else 0.0
  
      if hard >= 0.5:  # 正样本
          reward = 2.0 + soft   # ∈ [2, 3]
      else:             # 负样本
          reward = soft         # ∈ [0, 1]
  ```

- 这样，**正负样本间隔至少 1.0**，非常利于 softmax 权重把概率集中到正确样本上。

#### 3.2.3 预期效果与验证方式

- 预期：
  - shot_num=2：在 400/800 条数据上，v3 相对 v2 的优势有希望从 `+0.3% ~ +0.5%` 提升到 `+1% ~ +1.5%`；
  - shot_num=1/3/4：不求明显提升，先保证不明显退化。
- 验证：
  - 用同一套评测脚本，按「100/200/400/800 × shot=1/2/3/4」重新测一轮；
  - 把结果更新到新的 `2025_12_xx正确率_raw_separated.md` 中，与 `2025_12_10正确率.md` 对比。

---

### 3.3 RL 数据过滤：跳过 fallback 样本 & 无信号 query

**目的**：减少 reward 噪声，把 RCE 的“学习预算”集中在有可靠信号的样本上。

#### 3.3.1 跳过 fallback reward（代码已经支持）

- RL JSON 中已经写入了 `vqa_eval_mode = "file" | "fallback"` 字段；

- 在 `RLBeamDatasetWithEmbedding` 中有参数：

  ```python
  skip_fallback_reward: bool = False
  ```

- 确认初始化时传入：

  ```python
  train_dataset = RLBeamDatasetWithEmbedding(
      rl_data=train_rl_data,
      query_embeddings=query_emb,
      candidate_embeddings=cand_emb,
      candidate_indices=candidate_indices,
      shot_num=args.shot_num,
      normalize_rewards=True,      # 是否在 Dataset 内做 Z-score，后面可再调整
      reward_mode=args.reward_mode,
      hard_weight=args.hard_weight,
      soft_weight=args.soft_weight,
      skip_fallback_reward=True,   # ★ 建议置为 True
      ...
  )
  ```

- 这会去掉那些通过字符串匹配等方式得到的低置信度 reward 样本。

#### 3.3.2 按 query 粒度过滤“无信号组”

**建议新增逻辑**（在 `RLBeamDatasetWithEmbedding.__init__` 中）：

```python
for query_id_str, query_data in rl_data.items():
    ...
    beam_rewards = [...]  # 原始 reward 列表

    if len(beam_rewards) == 0:
        continue

    rewards_tensor = torch.tensor(beam_rewards, dtype=torch.float32)
    r_min, r_max = rewards_tensor.min().item(), rewards_tensor.max().item()

    # 1) 所有 reward 完全一样 ⇒ 没有排序信息
    if r_max == r_min:
        print(f"[RLBeamDataset] skip query_id={query_id} because all rewards equal={r_min:.4f}")
        continue

    # 2) 最好样本 reward 仍然很低（例如全错，soft 也很小）
    if r_max < 0.1:
        print(f"[RLBeamDataset] skip query_id={query_id} because rewards too small (max={r_max:.4f})")
        continue

    self.samples.append({
        "query_id": query_id,
        "beam_labels": beam_labels,
        "beam_rewards": beam_rewards,
        "beam_logprobs": beam_logprobs,
    })
```

**好处：**

- 对那些「全部 pointer 都错」「正确率完全随机」的 query 不再训练；
- RCE 的有效 batch 中，reward 差异更显著，更容易学到稳定排序。

---

### 3.4 GRPO：短期建议保持关闭，必要时只做“小步实验”

在短期内，**不建议直接在当前 v3_rce_only 上全量启用 GRPO**。如果你确实想继续探索 GRPO，可以按以下「安全模式」来做：

1. **修掉双重 Z-score**

   - 修改 `train_grpo_epoch` 中传入的 reward：

     ```python
     # 原来
     beam_rewards = batch["beam_rewards"].to(device)      # 已 Z-score
     
     # 建议改为
     beam_rewards = batch["beam_rewards_raw"].to(device)  # 原始 reward
     ```

   - 在 `PointerSelectorV3.compute_grpo_loss` 中，保留：

     ```python
     advantages = self.compute_advantage(beam_rewards, normalize=True)
     ```

     这样只做一次 group 内 Z-score。

2. **限制 GRPO 的“破坏力”**

   - 配置建议：

     - `grpo_epochs = 1` 或最多 2；
     - `grpo_lr` 比 RCE 再小一个量级（例如 RCE 用 `1e-4`，GRPO 用 `3e-5` 或 `1e-5`）；

   - 参数冻结（可选但强烈建议）：

     - 在 `grpo_post_train.py` 中增加选项：

       ```python
       if args.freeze_backbone_in_grpo:
           for name, param in model.named_parameters():
               if "pointer_head" in name or "mlp_out" in name:
                   param.requires_grad = True
               else:
                   param.requires_grad = False
       ```

   - CLI 调用示例：

     ```bash
     python -m lever_lm.workflows.grpo_post_train \
       --beam_data <rl_data.json> \
       --img_emb <query_embeddings.pt> \
       --rce_epochs 5 \
       --grpo_epochs 1 \
       --reward_mode separated \
       --rce_use_raw_reward \
       --skip_fallback_reward \
       --freeze_backbone_in_grpo \
       --output_dir results/rl/v3_rce_separated_grpo_safe/
     ```

3. **评估方式**

   - 每次尝试 GRPO，只在「一个固定的 RCE-only checkpoint」上做；
   - 控制变量：只改 GRPO 设置，不改 reward_mode / 数据集；
   - 用 400/800 条的评测结果来判定是否有实质性提升；
   - 一旦发现明显退化，立即停用该配置。

---

### 3.5 日志与监控：让每次实验都“可解释”

为了避免后面再出现“这轮实验到底用了啥配置”的困惑，建议：

1. 在 `grpo_post_train.py` 中定义一个配置结构体（你已有类似写法，可稍微整理）：

   ```python
   @dataclass
   class GRPOConfig:
       rce_epochs: int
       rce_lr: float
       grpo_epochs: int
       grpo_lr: float
       reward_mode: str
       rce_use_raw_reward: bool
       skip_fallback_reward: bool
       freeze_backbone_in_grpo: bool
       kl_beta_init: float
       kl_target_min: float
       kl_target_max: float
   ```

2. 在 `train()` 开头打印：

   ```python
   print(f"[GRPO] Config: {cfg}")
   ```

3. 在训练过程中周期性打印：

   ```python
   print(
       f"[GRPO] step={global_step} "
       f"reward_mean={beam_rewards_raw.mean().item():.4f} "
       f"reward_std={beam_rewards_raw.std().item():.4f} "
       f"adv_mean={advantages.mean().item():.4f} "
       f"adv_std={advantages.std().item():.4f} "
       f"KL={kl.item():.4f} kl_beta={kl_beta:.4f}"
   )
   ```

这样，未来任何一个 RL 实验都可以通过 log 和 md 报告完整复盘。

---

## 4. 中期改造计划（1～2 周）

短期方案跑通并验证后，可以考虑以下中期方向。

### 4.1 针对 1-shot 的专门 RL 数据与模型

**动机**：目前 1-shot 只有来自 2-shot RL 的“间接泛化”，对它进行专门优化可以明显改善「低 shot 场景」。

**方案：**

1. 在 `generate_rl_data.py` 中增加参数 `--shot_num`：
   - 当 `shot_num=1` 时，pointer 长度为 1，VQA prompt 只包含 1 个 ICD；
   - 生成一份 **专门的 1-shot RL 数据**（新的 JSON）。

2. 在 `grpo_post_train.py` 中允许指定对应的 `shot_num`：
   - 训练一个专门适配 shot_num=1 的 pointer 模型；
   - 使其 reward 完全来自「1-shot correctness」。

3. 评估：
   - 将该模型作为「1-shot 专家」；
   - 在多 shot 推理时，可以：
     - 1-shot 用这个专家模型；
     - 2/3/4-shot 用原来的 2-shot 优化模型（或未来的多 shot 模型）。

### 4.2 多 shot 统一 reward：1/2/3/4-shot 综合优化

这是更复杂但更理想的方案。

**思路：**

1. pointer 固定长度 4：`[i1, i2, i3, i4]`；

2. 对每条 pointer，计算 1/2/3/4-shot 的 correctness：

   ```python
   R1 = correctness([i1])
   R2 = correctness([i1, i2])
   R3 = correctness([i1, i2, i3])
   R4 = correctness([i1, i2, i3, i4])
   ```

3. 最终 reward 定义为加权和：

   ```python
   R_total = w1 * R1 + w2 * R2 + w3 * R3 + w4 * R4
   ```

   例如：`w1=0.2, w2=0.3, w3=0.2, w4=0.3`（可调）。

4. RL 数据 JSON 中为每个 pointer 保存：

   ```jsonc
   {
     "pointer": [i1, i2, i3, i4],
     "vqa_R_list": [R1, R2, R3, R4],
     "vqa_R_total": R_total,
     ...
   }
   ```

5. 在 `RLBeamDatasetWithEmbedding` 中优先使用 `vqa_R_total` 作为 reward：

   ```python
   if "vqa_R_total" in c:
       reward = float(c["vqa_R_total"])
   else:
       # fallback: 使用 2-shot correctness
       reward = compute_reward_for_candidate(...)
   ```

**好处：**

- RL 目标与最终评测的 1/2/3/4-shot 准确率真正对齐；
- 模型不会只为 2-shot 调整排序，而是综合考虑“前缀质量”。

> 这一块需要较多计算资源（VQA 要多跑几次），建议在短期方案稳定后再启动。

---

### 4.3 系统探索 reward_mode / hard_soft_权重

目前你的实验主要集中在 `hard_plus_soft` + hard_weight=soft_weight=1.0。  
中期可以设计一轮系统的 grid search：

- 维度 1：`reward_mode ∈ {hard_plus_soft, hard_plus_soft_v2, separated, hybrid}`
- 维度 2：`hard_weight / soft_weight ∈ {(1,1), (1,0.5), (0.5,1)}`
- 维度 3：`rce_use_raw_reward ∈ {True, False}`

**实验策略：**

1. 固定：
   - RCE-only（`grpo_epochs=0`）；
   - `skip_fallback_reward=True`；
   - 数据集与采样器设置；
2. 对上述组合跑小规模实验（例如只用 200 条测试）；
3. 从中选出 2~3 个表现最好的组合，再在 800 条上做完整评估；
4. 最终选出一个「**v3_RL_final**」配置写入文档，作为默认。

---

### 4.4 Hybrid：让 InfoScore 作为辅助信号回归

旧方案中，InfoScore 在 1/2-shot 上表现不错；  
新方案 correctness 在多 shot 上更稳。  
Hybrid 模式可以让二者互补。

**实现思路：**

1. 在 RL 数据生成中保留 `beam_score`（InfoScore）；

2. 在 `reward_utils.compute_reward_for_candidate` 中的 `"hybrid"` 模式：

   ```python
   elif reward_mode == "hybrid":
       # correctness 部分
       hard = float(vqa_correct) if vqa_correct is not None else 0.0
       soft = float(vqa_acc_score) if vqa_acc_score is not None else 0.0
       correctness_val = hard + soft  # [0, 2]
   
       # quality 部分（InfoScore 归一化到 [0, 1]）
       if beam_score is not None:
           quality = float(beam_score)
           # 可在组内做 min-max 归一化
           # quality_norm = (quality - min) / (max - min + 1e-8)
       else:
           quality = 0.0
   
       reward = alpha * quality + (1 - alpha) * correctness_val
   ```

3. 通过 alpha（例如 0.2, 0.5, 0.8）控制 InfoScore 在总 reward 中的比重。

---

## 5. 执行路线图与 Checklist

### 5.1 建议的执行顺序

1. **短期 Step 1：raw + separated + skip_fallback**
   - 跑一版 RCE-only：
     - `reward_mode=separated`
     - `rce_use_raw_reward=True`
     - `skip_fallback_reward=True`
     - `grpo_epochs=0`
   - 用 100/200/400/800 × shot=1/2/3/4 评估；
   - 写入 `2025_12_xx正确率_raw_separated.md`。

2. **短期 Step 2：query 过滤**
   - 在 `RLBeamDatasetWithEmbedding` 中接入“无信号 query 跳过”逻辑；
   - 在相同设置下再训练/评估一版，观察变化。

3. **短期 Step 3：安全 GRPO 小实验（可选）**
   - 修掉双重 Z-score，限制 epoch 与 lr，启用 `freeze_backbone_in_grpo`；
   - 从当前最佳 RCE-only checkpoint 出发，只做 1 epoch；
   - 评估是否有 +0.2% ~ +0.5% 的额外收益；
   - 如效果不稳定，可暂时放弃 GRPO。

4. **中期 Step 4：1-shot 专家模型**
   - 生成 1-shot RL 数据，训练专门的 pointer 模型；
   - 集成到推理 pipeline 中。

5. **中期 Step 5：多 shot 统一 reward（可选）**
   - 如资源允许，对 [i1,i2,i3,i4] pointer 计算 1~4-shot correctness，训练多 shot 模型。

6. **中期 Step 6：Hybrid 与 grid search**
   - 系统探索不同 reward 组合的效果，最终确定 v3_RL_final 默认配置。

---

### 5.2 Checklist（可以直接复制到你的 README 或 issue 中）

- [ ] 固化 v3（RCE-only, hard_plus_soft）为当前 baseline，记录完整训练参数与结果链接  
- [ ] 在 `grpo_post_train.py` 中规范打印 GRPOConfig & 日志  
- [ ] 在 RCE 阶段启用 `--rce_use_raw_reward` 并试验 `reward_mode=separated`  
- [ ] 在 `RLBeamDatasetWithEmbedding` 中增加按 query 粒度的“无信号过滤”  
- [ ] 在 RL JSON 中确认保存 `vqa_eval_mode`，并在训练中启用 `skip_fallback_reward=True`  
- [ ] 修复 GRPO 阶段 reward 传入，确保只做一次 Z-score  
- [ ] 增加 `--freeze_backbone_in_grpo`，对 GRPO “限权”  
- [ ] 设计并跑完一轮小规模 reward_mode × hard/soft × raw/normalized 的 grid search  
- [ ] 视资源情况，规划 1-shot 专家模型与多 shot 统一 reward 的实现与实验  

---

> 备注：  
> 这份计划书假定你继续以 OKVQA + Qwen2.5-VL-3B-Instruct + RandSampler 为主战场，  
> 如果后续切到其他 dataset / 模型，可以按相同结构复用，只需重新生成 RL 数据与 correctness。  
> 整体改造思路是“**巩固 RCE-only 的优势 → 清理 reward 噪声 → 小心扩展 GRPO → 再考虑多 shot 与 hybrid**”。