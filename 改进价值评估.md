# Lever-Plus v3 改进价值评估

> 基于 `2025_12_10正确率.md` 的实验结果，评估各项改进的实际价值

---

## 核心事实回顾

### ✅ 当前最佳方案：RCE-only baseline

- **配置**：`RCE_EPOCHS=5, GRPO_EPOCHS=0, reward_mode=hard_plus_soft`
- **表现**：在 shot_num=2 时，**稳定优于 v2/v1/v0**（100/200/400/800 条数据上都成立）
- **提升幅度**：vs v2（+0.27% ~ +1.0%），vs v1（+0.4% ~ +1.5%），vs v0（+1.27% ~ +4.5%）

### ❌ GRPO 阶段的问题

- 文档明确指出：**GRPO 目前更像是"把好模型拉坏"**
- 原因：在已经学好的 RCE 模型上继续用同一批离线数据做 PPO，容易过拟合 reward 噪声

---

## 改进价值评估（按实际价值排序）

### 🔴 **高价值：必须做（保护现有成果）**

#### 1. **3.1 把 RCE-only 明确固化成默认训练方式**

**价值：⭐⭐⭐⭐⭐（最高优先级）**

**为什么有意义：**
- ✅ **避免误操作**：当前默认 `--grpo_epochs 3`，如果用户不小心运行训练，会把已经很好的 RCE-only 模型覆盖
- ✅ **明确最佳实践**：代码即文档，默认参数应该反映"当前最佳方案"
- ✅ **降低风险**：保护已经验证有效的 `rce_epoch5.pt` 模型

**实际影响：**
- 如果误操作运行 GRPO，可能把 64.8% 的模型拉低到 60% 以下
- 这个改进是**防御性的**，成本低但收益高

**建议：立即实现**

---

#### 2. **3.2 SFT checkpoint 加载时增加一致性检查**

**价值：⭐⭐⭐⭐（高优先级）**

**为什么有意义：**
- ✅ **避免静默错误**：如果 checkpoint 和模型结构不匹配，`strict=False` 会静默丢失参数
- ✅ **早期发现问题**：在训练开始前就能发现配置错误
- ✅ **提高可调试性**：明确的错误信息比"模型效果不好但不知道为什么"要好得多

**实际影响：**
- 如果参数丢失，可能导致：
  - RL 数据生成时用的模型不对（影响数据质量）
  - 训练时模型初始化不对（影响训练效果）
- 这类问题很难从结果中看出来，但会严重影响实验可靠性

**建议：立即实现**

---

### 🟡 **中等价值：建议做（提升实验质量）**

#### 3. **3.3.1 添加 `vqa_eval_mode` 字段**

**价值：⭐⭐⭐（中等优先级）**

**为什么有意义：**
- ✅ **数据质量透明化**：知道哪些样本用的是官方 metric，哪些是 fallback
- ✅ **便于分析**：可以统计 fallback 样本的比例，评估数据质量
- ⚠️ **但可能不是最紧急的**：当前实验已经证明 RCE-only 有效，说明数据质量基本可靠

**实际影响：**
- 如果 fallback 样本比例很高（>50%），可能需要重新生成数据
- 如果 fallback 样本比例很低（<10%），这个改进主要是"锦上添花"

**建议：近期实现（在下一轮数据生成时加上）**

---

#### 4. **3.3.2 支持跳过 fallback 样本**

**价值：⭐⭐（低-中等优先级）**

**为什么有意义：**
- ✅ **理论上更干净**：只用官方 metric 计算的 reward
- ⚠️ **但实际价值待验证**：当前 RCE-only 已经表现很好，说明 fallback 样本可能影响不大
- ⚠️ **可能减少数据量**：如果 fallback 样本很多，过滤后数据量可能不够

**实际影响：**
- 需要先统计 fallback 样本比例
- 如果比例低，这个改进意义不大
- 如果比例高，可能需要重新生成数据而不是过滤

**建议：先实现 3.3.1，统计 fallback 比例后再决定是否实现 3.3.2**

---

#### 5. **3.4 增加 `rce_use_raw_reward` 开关**

**价值：⭐⭐（低优先级）**

**为什么有意义：**
- ✅ **便于对比实验**：可以对比 raw reward vs normalized reward 的效果
- ⚠️ **但当前已经很好**：当前使用 raw reward 的结果已经是最优的
- ⚠️ **实验性改进**：主要是为了探索，不是必须的

**实际影响：**
- 如果对比实验发现 normalized reward 更好，可以切换
- 如果对比实验发现 raw reward 确实更好（当前情况），这个开关主要是"记录最佳实践"

**建议：可以延后实现，等需要做对比实验时再加**

---

### 🟢 **低价值：可做可不做（GRPO 相关）**

#### 6. **3.5.2 支持冻结 backbone**

**价值：⭐（低优先级）**

**为什么价值较低：**
- ⚠️ **GRPO 本身效果不好**：文档明确指出 GRPO 目前更像是"把好模型拉坏"
- ⚠️ **可能治标不治本**：冻结 backbone 可能让 GRPO 更安全，但如果 GRPO 本身有问题，这个改进意义有限
- ⚠️ **当前不需要**：RCE-only 已经是最优方案，GRPO 暂时不需要

**实际影响：**
- 如果未来要探索 GRPO，这个改进可能有价值
- 但当前阶段，**优先保护 RCE-only 成果**更重要

**建议：等 GRPO 有明确改进方向后再实现**

---

### 🔵 **探索性：中期规划（不紧急）**

#### 7. **4.1-4.3 中期改进**

**价值：⭐（探索性）**

**为什么价值较低：**
- ⚠️ **当前方案已经很好**：RCE-only 在 shot_num=2 时已经是最优的
- ⚠️ **需要大量实验验证**：多 shot_num、reward_mode 探索都需要时间
- ⚠️ **优先级不高**：当前应该先巩固 RCE-only 成果

**建议：作为中期规划，不紧急**

---

## 综合建议

### 立即实现（1-2天）

1. ✅ **3.1 RCE-only 默认配置**
   - 修改默认参数：`--rce_epochs 5 --grpo_epochs 0`
   - 添加 `grpo_epochs <= 0` 时的保护逻辑
   - **价值：保护现有成果，避免误操作**

2. ✅ **3.2 Checkpoint 加载检查**
   - 检查 `missing`/`unexpected` 参数
   - 如果缺失过多，直接 raise 错误
   - **价值：避免静默错误，提高可调试性**

### 近期实现（1周内）

3. ⚠️ **3.3.1 添加 `vqa_eval_mode` 字段**
   - 在下次生成 RL 数据时加上
   - 统计 fallback 样本比例
   - **价值：数据质量透明化**

4. ⚠️ **3.3.2 支持跳过 fallback 样本**（可选）
   - 根据 fallback 比例决定是否实现
   - **价值：如果 fallback 比例高，可能有价值**

### 延后实现（等需要时）

5. ⚠️ **3.4 `rce_use_raw_reward` 开关**
   - 等需要做对比实验时再加
   - **价值：实验性改进**

6. ⚠️ **3.5.2 冻结 backbone**
   - 等 GRPO 有明确改进方向后再实现
   - **价值：GRPO 相关，当前不紧急**

### 中期规划（1-2周后）

7. ⚠️ **4.1-4.3 探索性改进**
   - 等 RCE-only 成果巩固后再规划
   - **价值：探索性，不紧急**

---

## 核心结论

### ✅ **有意义的改进（必须做）**

1. **3.1 RCE-only 默认配置**：保护现有成果，避免误操作
2. **3.2 Checkpoint 检查**：避免静默错误，提高可靠性

这两个改进是**防御性的**，成本低但收益高，**强烈建议立即实现**。

### ⚠️ **有价值的改进（建议做）**

3. **3.3 Reward 质量标记**：提升数据质量透明度
4. **3.4 rce_use_raw_reward 开关**：便于对比实验

这些改进能提升实验质量，但不是最紧急的。

### ❓ **价值待验证的改进（可做可不做）**

5. **3.5.2 冻结 backbone**：GRPO 相关，当前不紧急
6. **4.1-4.3 探索性改进**：中期规划

这些改进主要是为了探索，当前阶段优先级不高。

---

## 最终建议

**如果时间有限，只做前两项（3.1 + 3.2）就够了。**

这两项改进：
- ✅ **成本低**：代码改动小，1-2小时就能完成
- ✅ **收益高**：保护现有成果，避免常见错误
- ✅ **风险低**：不会影响现有功能

其他改进可以根据实际需要和时间安排逐步实现。

---

**评估时间：** 2025-12-10  
**基于文档：** `LeverPlus_v3_RL_next_steps_2025_12_10.md` + `2025_12_10正确率.md`





