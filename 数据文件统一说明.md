# 数据文件统一说明

## 修改内容

为了确保 KL_BETA=0.12、0.15、0.18 的公平对比，已将所有训练脚本统一使用相同的数据文件：

**统一数据文件**: `rl_data_k64_v3.json`

## 已修改的脚本

1. ✅ `scripts/train_v3_kl012.sh` - 完整训练脚本（RCE + GRPO）
2. ✅ `scripts/train_grpo_kl012_from_rce.sh` - 从 RCE checkpoint 继续训练 GRPO
3. ✅ `scripts/train_v3_kl018.sh` - 完整训练脚本（RCE + GRPO）
4. ✅ `scripts/train_grpo_kl018_from_rce.sh` - 从 RCE checkpoint 继续训练 GRPO

## 数据文件对比

### 之前使用的数据（已废弃）

- **KL_BETA=0.12/0.18**: `rl_data_RandSampler_Qwen2_5-VL-3B-Instruct.json`
  - Query 数: 800
  - Candidate 数: 6,400（每个 query 8 个）
  - 有正样本的 query: 5 / 800 (0.6%)
  - 平均 reward: 0.0020

### 现在统一使用的数据

- **所有 KL_BETA**: `rl_data_k64_v3.json`
  - Query 数: 800（与之前相同）
  - Candidate 数: 12,349（每个 query 8-44 个不等）
  - 有正样本的 query: 664 / 800 (83%)
  - 平均 reward: 1.0264

## 重要提示

⚠️ **之前训练的 KL_BETA=0.12 模型使用的是旧数据文件**，数据质量较低（只有 0.6% 的 query 有正样本）。

为了公平对比，建议：

### 选项 1：重新训练（推荐）

使用统一的数据文件重新训练所有 KL_BETA 的模型：

```bash
# 重新训练 KL_BETA=0.12
bash scripts/train_grpo_kl012_from_rce.sh 5 0 3 kl012

# 训练 KL_BETA=0.18（如果还未完成）
bash scripts/train_grpo_kl018_from_rce.sh 5 0 3 kl012
```

### 选项 2：保留之前的训练结果作为对比

如果已经训练完成并评估，可以：
- 保留之前的训练结果（标记为"使用旧数据"）
- 重新训练新的模型（标记为"使用统一数据"）
- 对比两种数据的效果差异

## 数据质量对比

| 指标 | 旧数据 (RandSampler) | 统一数据 (k64_v3) |
|------|---------------------|-------------------|
| Query 数 | 800 | 800 |
| Candidate 总数 | 6,400 | 12,349 |
| 平均 candidate 数/query | 8 | ~15.4 |
| 有正样本的 query 比例 | 0.6% | 83% |
| 平均 reward | 0.0020 | 1.0264 |
| Reward 信号强度 | ⚠️ 很弱 | ✅ 强 |

## 预期影响

使用统一数据后：
- ✅ Reward 信号更强（83% query 有正样本 vs 0.6%）
- ✅ 训练效果应该更好
- ✅ 不同 KL_BETA 的对比更公平
- ⚠️ 需要重新训练（之前的结果可能不准确）

## 下一步

1. **重新训练 KL_BETA=0.12**（使用统一数据）
2. **训练 KL_BETA=0.18**（使用统一数据）
3. **运行评估**，与 KL_BETA=0.15 进行公平对比

