# 新强化学习方案结果对比

## 实验配置

- **数据集**: OKVQA
- **推理模型**: Qwen2.5-VL-3B-Instruct
- **采样器**: RandSampler
- **RCE-only baseline (v3_rce_only)**: reward = hard + soft = vqa_correct + vqa_acc_score（范围 [0, 2]），**仅RCE训练（5 epochs），无GRPO**

---

## 100条数据对比

| Shot Num | v2 | v3_rce_only (RCE-only) | RCE-only vs v2 |
|----------|-----|------------------------|----------------|
| **1** | 63.8% | **64.2%** | **+0.4%** |
| **2** | 63.8% | **64.8%** | **+1.0%** |
| **3** | **62.8%** | 62.8% | 0.0% |
| **4** | **61.4%** | 61.4% | 0.0% |

**100条数据结论**：
- shot_num=1: RCE-only **优于 v2（+0.4%）**
- shot_num=2: RCE-only **优于 v2（+1.0%）**
- shot_num=3,4: RCE-only 与 v2 持平

---

## 200条数据对比

| Shot Num | v2 | v3_rce_only (RCE-only) | RCE-only vs v2 |
|----------|-----|------------------------|----------------|
| **1** | 56.7% | **57.0%** | **+0.3%** |
| **2** | 56.1% | **56.6%** | **+0.5%** |
| **3** | **55.5%** | 55.4% | -0.1% |
| **4** | **54.7%** | 54.5% | -0.2% |

**200条数据结论**：
- **RCE-only baseline 表现优秀**：
  - shot_num=1: 57.0%，**优于 v2（+0.3%）**
  - shot_num=2: 56.6%，**优于 v2（+0.5%）**
  - shot_num=3: 55.4%，接近 v2（-0.1%）
  - shot_num=4: 54.5%，接近 v2（-0.2%）
- **关键发现**：RCE-only baseline 在 shot_num≤2 时**稳定优于 v2**，在 shot_num≥3 时与 v2 接近

---

## 400条数据对比

| Shot Num | v2 | v3_rce_only (RCE-only) | RCE-only vs v2 |
|----------|-----|------------------------|----------------|
| **1** | 53.1% | 52.55% | -0.55% |
| **2** | 51.3% | **51.65%** | **+0.35%** |
| **3** | **51.75%** | 51.05% | -0.7% |
| **4** | **50.35%** | 49.6% | -0.75% |

**400条数据结论**：
- shot_num=1: RCE-only 略低于 v2（-0.55%）
- shot_num=2: RCE-only **优于 v2（+0.35%）**
- shot_num=3,4: RCE-only 略低于 v2（-0.7% ~ -0.75%）

---

## 800条数据对比

| Shot Num | v0 | v1 | v2 | v3_rce_only (RCE-only) | RCE-only vs v0 | RCE-only vs v1 | RCE-only vs v2 |
|----------|-----|-----|-----|------------------------|----------------|----------------|----------------|
| **1** | **50.95%** | 50.7% | 49.0% | 48.73% | -2.22% | -1.97% | -0.27% |
| **2** | 47.08% | 47.33% | 48.08% | **48.35%** | **+1.27%** | **+1.02%** | **+0.27%** |
| **3** | 44.95% | 46.35% | **48.33%** | 47.8% | +2.85% | +1.45% | -0.53% |
| **4** | 43.77% | 47.4% | **47.62%** | 47.17% | +3.4% | -0.23% | -0.45% |

**800条数据结论**：
- shot_num=1: RCE-only 低于所有版本（v0 50.95% > v1 50.7% > RCE-only 48.73% > v2 49.0%）
- shot_num=2: RCE-only **优于 v2（+0.27%）**，**优于 v0（+1.27%）**，**优于 v1（+1.02%）**
- shot_num=3: RCE-only 略低于 v2（-0.53%），但优于 v0（+2.85%）和 v1（+1.45%）
- shot_num=4: RCE-only 略低于 v2（-0.45%），但优于 v0（+3.4%）

---

## 总体结论

### RCE-only baseline (v3_rce_only) vs v0/v1/v2

| 指标 | RCE-only 表现 |
|------|--------------|
| shot_num=1 | **64.2%**（100条），优于 v2（+0.4%）；**57.0%**（200条），优于 v2（+0.3%）；52.55%（400条），略低于 v2（-0.55%）；48.73%（800条），低于 v0/v1/v2 |
| shot_num=2 | **64.8%**（100条），**优于 v2（+1.0%）**；**56.6%**（200条），**优于 v2（+0.5%）**；**51.65%**（400条），**优于 v2（+0.35%）**；**48.35%**（800条），**优于 v2（+0.27%）**，**优于 v0（+1.27%）**，**优于 v1（+1.02%）** |
| shot_num=3 | 62.8%（100条），与 v2 持平；55.4%（200条），接近 v2（-0.1%）；51.05%（400条），略低于 v2（-0.7%）；47.8%（800条），略低于 v2（-0.53%） |
| shot_num=4 | 61.4%（100条），与 v2 持平；54.5%（200条），接近 v2（-0.2%）；49.6%（400条），略低于 v2（-0.75%）；47.17%（800条），略低于 v2（-0.45%） |

### 关键发现

1. **RCE-only baseline 在 shot_num=2 时表现最佳**
   - 在所有数据规模（100/200/400/800条）上都**稳定优于 v2**
   - 提升幅度：+1.0%（100条）、+0.5%（200条）、+0.35%（400条）、+0.27%（800条）
   - **在 800 条数据上，RCE-only 同时优于 v0、v1、v2**

2. **RCE-only baseline 在 shot_num=1 时表现良好**
   - 在 100 条和 200 条数据上优于 v2（+0.4% 和 +0.3%）
   - 在 400 条和 800 条数据上略低于 v2（-0.55% 和 -0.27%）
   - 在 800 条数据上，v0 表现最好（50.95%），RCE-only 为 48.73%

3. **RCE-only baseline 在 shot_num≥3 时与 v2 接近**
   - 差异在 -0.75% 以内
   - 在 800 条数据上，shot_num=3 时 RCE-only（47.8%）优于 v0（+2.85%）和 v1（+1.45%）
   - 表现稳定，没有明显退化

4. **数据规模趋势**
   - 随着数据规模增加（100→200→400→800），RCE-only 相对于 v2 的优势在 shot_num=2 时保持稳定
   - 在 shot_num=1 时，小数据规模（100/200条）上 RCE-only 优于 v2，大数据规模（400/800条）上略低于 v2
   - 在 shot_num≥3 时，RCE-only 与 v2 的差距随数据规模增加而略有扩大

### 建议

1. **✅ 推荐使用 RCE-only baseline**：
   - 训练配置：`--rce_epochs 5 --grpo_epochs 0`
   - Reward：`hard_plus_soft`（默认）
   - **在 shot_num=2 时稳定优于 v2**

2. **🔬 后续探索方向**：
   - 探索不同的 reward_mode（separated、hard_plus_soft_v2 等）
   - 对比 RCE-only 在不同 reward_mode 下的表现
   - 优化 RCE 训练参数（epochs、learning rate 等）

---

## 更新记录

- **2025-12-10**: 新增 RCE-only baseline 结果（100/200/400/800条数据）
  - 训练配置：RCE Epochs=5, GRPO Epochs=0, Reward Mode=hard_plus_soft
  - 关键发现：
    - RCE-only 在 shot_num=2 时稳定优于 v2（+1.0% @ 100条，+0.5% @ 200条，+0.35% @ 400条，+0.27% @ 800条）
    - 在 800 条数据上，RCE-only 在 shot_num=2 时同时优于 v0、v1、v2
    - 添加了 v0、v1、v2 的完整对比（800条数据）
