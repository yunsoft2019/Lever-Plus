# 新强化学习方案结果对比

## 实验配置

- **数据集**: OKVQA
- **推理模型**: Qwen2.5-VL-3B-Instruct
- **采样器**: RandSampler
- **旧方案 (v3_1layer)**: reward = InfoScore（增益）
- **新方案 (v3_new)**: reward = hard + soft = vqa_correct + vqa_acc_score（范围 [0, 2]）

---

## 100条数据对比

| Shot Num | v2 | v3_1layer (旧) | v3_new (新) | 新 vs 旧 | 新 vs v2 |
|----------|-----|----------------|-------------|----------|----------|
| **1** | 63.8% | 64.8% | 62.8% | **-2.0%** | -1.0% |
| **2** | 63.8% | **65.8%** | 65.4% | -0.4% | +1.6% |
| **3** | **62.8%** | 60.2% | 62.2% | **+2.0%** | -0.6% |
| **4** | **61.4%** | 59.8% | 60.8% | **+1.0%** | -0.6% |

**100条数据结论**：
- shot_num=1: 新方案下降 2.0%
- shot_num=2: 新方案略降 0.4%，但仍优于 v2
- shot_num=3,4: 新方案提升 1.0%~2.0%，接近 v2

---

## 200条数据对比

| Shot Num | v2 | v3_1layer (旧) | v3_new (新) | 新 vs 旧 | 新 vs v2 |
|----------|-----|----------------|-------------|----------|----------|
| **1** | 56.7% | **59.3%** | 56.7% | **-2.6%** | 0.0% |
| **2** | 56.1% | **57.1%** | 55.9% | **-1.2%** | -0.2% |
| **3** | **55.5%** | 53.6% | 54.3% | +0.7% | -1.2% |
| **4** | **54.7%** | 54.2% | 54.1% | -0.1% | -0.6% |

**200条数据结论**：
- shot_num=1: 新方案下降 2.6%，与 v2 持平
- shot_num=2: 新方案下降 1.2%，略低于 v2
- shot_num=3,4: 新方案略有提升，但仍低于 v2

---

## 400条数据对比

| Shot Num | v2 | v3_1layer (旧) | v3_new (新) | 新 vs 旧 | 新 vs v2 |
|----------|-----|----------------|-------------|----------|----------|
| **1** | 53.1% | **54.35%** | 52.1% | **-2.25%** | -1.0% |
| **2** | 51.3% | **51.9%** | 51.3% | -0.6% | 0.0% |
| **3** | **51.75%** | 50.5% | 50.65% | +0.15% | -1.1% |
| **4** | **50.35%** | 49.95% | 50.05% | +0.1% | -0.3% |

**400条数据结论**：
- shot_num=1: 新方案下降 2.25%，低于 v2
- shot_num=2: 新方案下降 0.6%，与 v2 持平
- shot_num=3,4: 新方案略有提升，接近 v2

---

## 总体结论

### 新方案 (hard_plus_soft) vs 旧方案 (legacy/InfoScore)

| 指标 | 新方案表现 |
|------|-----------|
| shot_num=1 | **明显下降** (-2.0% ~ -2.6%) |
| shot_num=2 | **略有下降** (-0.4% ~ -1.2%) |
| shot_num=3 | **略有提升** (+0.15% ~ +2.0%) |
| shot_num=4 | **略有提升** (+0.1% ~ +1.0%) |

### 关键发现

1. **新 reward 方案在低 shot 数时效果更差**
   - shot_num=1,2 时，新方案比旧方案下降 0.4%~2.6%
   - 这可能是因为新方案的 reward 范围 [0, 2] 与旧方案的增益信号不同

2. **新 reward 方案在高 shot 数时略有提升**
   - shot_num=3,4 时，新方案比旧方案提升 0.1%~2.0%
   - 但整体仍低于或接近 v2 baseline

3. **整体不如 v2 baseline**
   - 新 v3 在大多数情况下都低于或等于 v2
   - 旧 v3_1layer 在 shot_num≤2 时超越 v2

### 建议

1. **继续使用旧方案 (legacy reward)**：在 shot_num≤2 的场景下效果更好
2. **调优新方案**：可能需要调整 hard_weight 和 soft_weight 的比例
3. **混合方案**：考虑结合两种 reward 信号

---

## 更新记录

- **2025-12-09**: 完成新 reward 方案 (hard_plus_soft) 的实验，对比旧方案 (legacy)
