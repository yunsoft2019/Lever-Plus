# Lever-Plus 强化学习（RL）重构计划（以数据质量为先）— 2025-12-13

> 目标：**围绕 Qwen2.5-VL（千问）**，把「用于 RL 的数据」做成 **可验证、可诊断、可迭代** 的闭环；先解决“正样本太少/奖励太稀疏/训练无明显提升”的核心问题，再谈更复杂的 GRPO/更大规模实验。

---

## 0. 现状速记（基于你最新实验 & 代码）

### 0.1 你遇到的“矛盾”其实不矛盾

你观察到：

- **整体准确率（例如 50%+）并不低**  
- 但在 RL 数据采样时，**每个 query 采出来的 16 条里只有 2–3 条是正样本**

这两者完全可以同时成立，因为：

- “50% 准确率”通常是指 **你最终选出的 1 条（top1/greedy/beam 最优）** 在 query 维度上的正确率；
- “2–3/16 正样本”是指 **在你当前采样分布（beam + 温度 + 随机）下**，对同一个 query 的候选集合里，**单条候选成为正样本的概率**。

例如：如果单条候选正样本概率 p≈0.15，那么 16 条的期望正样本数就是 16×0.15=2.4（非常符合你观测）。

**结论：** 你现在的问题不是“模型不可能做到 50%”，而是 **“你的采样分布很难采到能让 Qwen 答对的候选”**，导致：

- GRPO 这种 group-relative 方式大量 query 的 reward 全 0 → **无梯度**  
- RCE 方式权重差异小/噪声大 → **学不到明显偏好**

---

## 1. 我认为当前瓶颈在哪（从大到小排序）

### 1.1 最大问题：RL 数据的“有效梯度覆盖率”不足

你现在的 reward（hard + soft）里，**soft 仍然偏离散/稀疏**（很多候选都接近 0），导致：

- 对大量 query：候选 reward 方差非常小甚至全 0  
- group-zscore 后优势≈0  
- 训练看起来“在跑”，但有效更新很少

✅ 解决方向：给每个 query 的候选集合提供 **连续可区分** 的 soft 信号，让“全错”也能有梯度。

---

### 1.2 采样策略问题：探索在“错误的空间/分布”里

你混合了：

- beam（偏 exploitation）
- 温度采样（偏探索）
- random（强探索）

但目前两类风险会让正样本更少：
1) **随机探索空间太大（候选池大）→ 命中概率极低**  
2) 探索分布偏了（例如随机只在很小子集里抽）→ 反而降低命中率

✅ 解决方向：探索要“带先验”，比如：

- **retrieval/topK** 限定候选池（先把空间缩小到 64/128/256）
- 在 topK 内做 beam / top-p / 局部搜索
- 用“便宜的 proxy reward”先筛再用昂贵的生成验证

---

### 1.3 训练方案层面：你现在主要在做 RCE（离线）而不是“真 on-policy RL”

你目前大量实验 `--grpo_epochs 0`，本质是 **离线的 reward-weighted CE**。它能给稳定提升的前提是：

- 数据里高 reward 候选占比足够
- 高 reward 与模型可学习的特征强相关
- 每个 query 的候选 reward 分布有足够区分度

目前这些条件不够满足，所以效果不明显。

✅ 解决方向：先把数据/奖励做“可学”，再把 GRPO 打开做小规模 on-policy 闭环。

---

### 1.4 代码层面的高概率“坑”：会直接降低正样本比例/破坏探索

我在你 zip 的 `lever_lm/models/v3/rl_data_generation.py` 里看到一个 **非常值得立刻修** 的点：

- `generate_pointer_candidates_for_query()` 里 **K 的取值**：
  - 你当前是 `K = cand_emb.shape[1]`
  - 但如果 `cand_emb` 的形状是 `[K, d]`（常见），那么 `shape[1] = d`  
  - 会导致 random sampling 只在 `[0, d-1]` 范围采样（例如 0..511），**探索被严重限制在候选池前 512 个**  
  - 这会显著降低命中正样本概率，也会污染分布

✅ 建议改成：`K = cand_emb.shape[-2]`（同时兼容 `[K,d]` 和 `[B,K,d]`）

---

## 2. 是否要加“response 与 GT 的相关性”作为 reward？

**可以加，但我建议把它放在“第三优先级”，并且只作为辅助 shaping。**

### 2.1 为什么不是首选？

你最终评测指标是 **VQA accuracy（离散匹配）**。  
如果你加一个“语义相似度”奖励，它可能会：

- 把很多“接近但不对”的答案也奖励起来（例如同类词、近义词）
- 让策略更倾向于“语义合理但评测不计分”的输出
- 造成 reward 与最终 metric **不一致**，反而拖慢收敛或带偏

### 2.2 我更推荐的“连续 soft reward”：**GT 概率（expected VQA accuracy）**

你提到“回答正确的概率值”，这是最对路的方向。

做法是：不只看生成出来对不对，而是计算：

- 给定 prompt（含 ICD），Qwen 对“任一 GT 答案字符串”的概率质量（teacher-forcing / cond_prob）
- 并按 VQA 的计分方式给每个 GT 答案一个权重（基于出现次数）

得到一个 **0..1 的连续值**，可以理解成：

> 这个 prompt 下，模型“有多大概率”会产生一个能被评测判对的答案。

这比“语义相似度”更对齐你的最终目标。

---

## 3. 重构总原则：先把 RL 数据做成“可被证明有效”的

下面是一个 **从数据质量→奖励→训练→评测** 的可执行闭环。

---

## 4. Step-by-step：数据质量诊断（你现在就该做）

### 4.1 写一个“RL 数据体检脚本”（建议新建）

新建：`lever_lm/models/v3/analyze_rl_data_v4.py`

输入：RL 数据 JSON（query -> pointer_candidates）

输出（必须打印 + 保存 csv）：

- 全局：
  - query 数量
  - 候选总数
  - 候选去重率（同一 query 内 unique pointer 数）
  - 正样本比例（hard>0）
  - 每个 query 的正样本数分布：min / p25 / median / p75 / max
- 关键诊断：
  - `pct_zero_positive_query`：**多少 query 一个正样本都没有**
  - `pct_flat_reward_query`：多少 query reward 方差 < eps（几乎没梯度）
  - 每个采样来源的贡献：beam / sample(temp=*) / random 的正样本率

伪代码（核心统计）：

```python
for qid, q in rl.items():
    cands = q["pointer_candidates"]
    unique = len(set(tuple(c["pointer"]) for c in cands))
    pos = sum(c["vqa_correct"] == 1 for c in cands)
    acc = [c.get("vqa_acc_score", 0.0) for c in cands]
    # 如果你新增 soft_gt_prob：
    prob = [c.get("vqa_gt_prob", 0.0) for c in cands]
    # reward 方差
    var = np.var(prob)  # 或你真正用来训练的 reward
```

**你会立刻知道：到底是“很多 query 没正样本”，还是“有正样本但分不出来”。**

---

### 4.2 做一个“headroom / upper bound”检查（决定你该不该继续 RL）

对每个 query，你至少要估计两件事：

- `oracle_in_data`：在当前候选集合里，最好的一条（最大 vqa_acc_score 或 vqa_gt_prob）
- `model_pick`：模型真正会选的那条（greedy/beam）

如果两者差距很小，说明：

- 不是 RL 学不会，而是 **候选集合本身就没有更好解**（需要改候选池/检索/提示词）

建议输出：

- mean(oracle) - mean(model_pick)
- query 级别：多少 query 的 oracle 明显更高（例如 +0.2）

---

## 5. Step-by-step：把“soft reward”做成连续、可对齐评测的

### 5.1 新增字段：`vqa_gt_prob`（强烈建议）

在 RL 数据生成阶段，除了生成答案算 `vqa_acc_score`，还计算：

> `vqa_gt_prob` = Σ_{a∈GT_unique} P(a | prompt) * w(a)

其中：

- GT_unique 是去重后的标准答案集合（通常 ≤10）
- w(a) = min(count(a)/3, 1)（对齐 VQA accuracy 的计分方式）

这个值∈[0,1]，是一个**连续**且**与最终 metric 对齐**的 soft 指标。

### 5.2 代码落点（建议你这样改）

改：`lever_lm/models/v3/generate_rl_data.py`

新增函数（伪代码）：

```python
def compute_expected_vqa_acc_prob(interface, prompts_context, gt_answers):
    # 1) 统计 gt answers 频次（已做 normalization）
    uniq = Counter(gt_answers_normalized)
    # 2) 对每个 uniq answer 做 teacher forcing cond prob
    probs = {}
    for ans, cnt in uniq.items():
        x_input, mask_len = build_labeled_input(prompts_context, label=ans)
        p = interface.get_cond_prob(x_input, mask_len)   # 返回概率或可转换为概率
        probs[ans] = p
    # 3) 按 VQA 计分权重加权求和
    score = 0.0
    for ans, cnt in uniq.items():
        w = min(cnt/3.0, 1.0)
        score += probs[ans] * w
    return float(score)
```

⚠️ 关键点：

- 你需要确认 `get_cond_prob` 的返回值到底是 **prob** 还是 **logprob**（你之前 infoscore 用过，通常是 prob）。
- 如果是 logprob，则用 `p = exp(logprob)`（或按 token 平均后再 exp）。

### 5.3 奖励组合建议（简单可控）

我建议你把 reward 改成：

- `hard = 1(vqa_acc_score > 0)`  （你现在的 vqa_correct 也是这个）
- `soft = vqa_gt_prob`           （连续）

然后：

- `reward = hard_weight * hard + soft_weight * soft`

如果你想保持“强区分正负”，可以做：

- `reward = 2.0 * hard + 1.0 * soft`  
  - 保证任何正样本 reward 都显著高于负样本
  - 同时负样本之间也有 soft 差异（不会全 0）

这比“语义相似度”更稳、更对齐。

---

## 6. Step-by-step：正样本挖掘（让每个 query 至少有梯度）

### 6.1 基本原则

对每个 query：

- 至少保留 **1–2 个高 soft 候选（高 vqa_gt_prob）**
- 至少保留 **若干低 soft 候选** 作为对比（否则优势不稳定）
- 尽量保证：`pos_count >= 1`，否则这个 query 在 GRPO 里几乎没用

### 6.2 推荐的“二阶段挖掘”（成本可控、命中率高）

**阶段 A（便宜）：** 批量搜索最大化 `vqa_gt_prob`  

- 用 pointer selector proposal（beam/top-p）产生很多候选（比如 64–256）
- **只算 vqa_gt_prob**（不生成答案）  
- 选出 topM（比如 8–16）进入阶段 B

**阶段 B（昂贵但少）：** 对 topM 做生成 + official accuracy  

- 生成答案 → vqa_acc_score / vqa_correct  
- 最终 RL 数据里保留：
  - topM（高软分）
  - + 少量随机/低分候选（保持多样性）

这样你会得到：

- 更高的正样本比例
- 更连续的 reward 分布
- 更稳定的训练

---

## 7. Step-by-step：训练流程重构（从“可控小闭环”开始）

### 7.1 先做最小闭环（不要一上来全量）

建议你先用 50–100 个 query 做一个可控闭环：

1) 生成 RL 数据 v4（含 vqa_gt_prob）  
2) RCE 训练 1–3 epoch  
3) 用同一批 query 做离线评估：

   - model_pick vs oracle_in_data 的差距是否缩小
     4) 如果有效，再扩大到 800 query

### 7.2 RCE 阶段的两个关键改动

1) **跳过 flat reward query**

- 如果某个 query 的候选 `reward` 方差 < eps，权重接近均匀，RCE 学不到偏好，还可能引入噪声
- 直接跳过更好

2) **权重不要过尖**

- 温度 τ 可以从 2.0 → 0.5，但如果 reward 很稀疏，低温会导致过拟合少数样本
- 建议：先固定 τ=1.0 做对比，再做 schedule

---

## 8. Step-by-step：再把 GRPO 打开（小规模 on-policy）

当你做到：

- `pct_zero_positive_query` 明显下降（比如 <10%）
- `vqa_gt_prob` 与 `vqa_acc_score` 有明显正相关（可在体检脚本里算 Spearman）

再做：

- `--grpo_epochs 1~2` 小规模验证  
- 每轮 on-policy 采样仍然用 **二阶段挖掘**（soft 先筛）

---

## 9. 具体代码修改清单（按优先级）

### P0（今天就改，改完立刻能验证）

1) **修复 random sampling 的 K 维度**

- 文件：`lever_lm/models/v3/rl_data_generation.py`

- 位置：`generate_pointer_candidates_for_query()`

- 修改：

  ```python
  # before
  K = cand_emb.shape[1]
  # after
  K = cand_emb.shape[-2]
  ```

2) **RL 数据生成时，强制 deterministic generation（减少标签噪声）**

- 文件：`lever_lm/models/v3/generate_rl_data.py`

- 统一设置：

  ```python
  generation_kwargs = dict(
      do_sample=False,
      num_beams=1,
      max_new_tokens=10,
  )
  ```

3) **错误不要默默变成负样本**

- 当前：异常 → vqa_correct=0, vqa_acc_score=0
- 建议：加 `eval_failed=True`，训练时跳过这些候选

---

### P1（1–2 天内完成，会显著改善“稀疏 reward”）

4) **新增 `vqa_gt_prob` 计算**

- 文件：`lever_lm/models/v3/generate_rl_data.py`
- 输出到 RL JSON 的每条 candidate

5) **reward_utils 支持新模式**

- 文件：`lever_lm/utils/reward_utils.py`
- 新增 reward_mode，例如：
  - `hard01_plus_gtprob`
  - `hard01_plus_gtprob_separated`

6) **dataset_v3 读取并使用 vqa_gt_prob**

- 文件：`lever_lm/models/v3/dataset_v3.py`
- 让 reward 计算用 `vqa_gt_prob` 作为 soft，而不是仅 vqa_acc_score

---

### P2（中期，提升上限）

7) **做“二阶段挖掘”的 RL 数据生成**

- 先算很多候选的 vqa_gt_prob
- 只对 topM 做生成评测（节省大量时间）

8) **推理阶段加入 beam search 或 rerank（提升上线表现）**

- 目前 eval 多为 greedy
- 你可以：
  - 用 pointer model beam 产生 5–10 条
  - 用 `vqa_gt_prob` 或生成正确率 rerank 选最优

---

## 10. 你问的“到底是数据不够好还是训练方案有问题”怎么判定？

用下面这个判定顺序（非常实用）：

1) **Oracle 还有多少提升空间？**

- 如果 `oracle_in_data` 也就 50%，那 RL 很难超过，说明候选池/任务本身限制
- 如果 oracle 能到 60% 但 model_pick 只有 50%，说明模型/训练还有空间

2) **zero-positive query 比例是多少？**

- 如果很多 query 没有正样本：先解决数据采样/挖掘
- 如果大多数 query 有正样本，但模型还是学不到：检查 reward 设计/优化/推理方式（greedy vs beam）

3) **soft_gt_prob 与 hard_acc 是否相关？**

- 如果相关性很弱：soft reward 设计不对/cond_prob 用法不对/后处理不对
- 如果相关性强：soft reward 是好 proxy，可用于大规模挖掘与训练

---

## 11. 最推荐的“下一步执行清单”（最短路径）

### 第 0 步（今天）

- [ ] 修复 `K = cand_emb.shape[-2]`
- [ ] 把 VQA 生成设为 deterministic
- [ ] 写 RL 数据体检脚本（统计 zero-positive、flat reward）
- [ ] 在 50 个 query 上生成 RL 数据并跑体检

### 第 1 步（明天）

- [ ] 在 RL 数据里新增 `vqa_gt_prob`
- [ ] reward 改成 `reward = 2*hard01 + 1*gt_prob`
- [ ] 重新生成 100–200 query 的 RL 数据（先小规模）
- [ ] RCE 训练 1–3 epoch，验证 model_pick 是否更接近 oracle

### 第 2 步（后天）

- [ ] 把 RL 数据生成改成二阶段（soft 先筛）
- [ ] 扩大到 800 query
- [ ] 再做一次 RCE 对比
- [ ] 如果体检指标健康（zero-positive 低），开启 1 epoch GRPO

---

## 12. 关于“语义相似度 reward”的建议落地方式（可选）

如果你仍然想试“response 与 GT 的相关性”，我建议 **只做 very light shaping**：

- `sim = max_{a∈GT} cosine(emb(pred), emb(a))`
- reward = `2*hard01 + 1*gt_prob + 0.1*sim`

并且必须做两件事：
1) 先在体检脚本里算：sim 是否真的与 hard_acc 正相关  
2) 只在 hard=0 的候选里起作用（避免奖励把你拉偏）

否则风险很大。

---

如果你愿意，我也建议你把你“生成 beam_data 的脚本输出格式”升级：把每个 query 的候选池（candidate_pool_ids）也保存下来。这样后续可以把 action space 固定为 64/128，正样本率会更好，训练也更稳定。