# V3 数据集使用指南

## 数据集类说明

### 1. `BeamDataset`（旧格式）
用于加载旧的 beam 数据格式：
```json
{
  "query_id": {
    "id_list": [[shot1, shot2, query_id], ...],
    "score_list": [0.046, 0.045, ...]
  }
}
```

### 2. `RLBeamDatasetWithEmbedding`（新格式）⭐
用于加载新的 RL 数据格式（包含 correctness）：
```json
{
  "query_id": {
    "pointer_candidates": [
      {
        "pointer": [7, 22],
        "gen_method": "beam",
        "beam_score": 2.13,
        "logprob_score": -1.56,
        "vqa_correct": 1,
        "vqa_acc_score": 1.0
      },
      ...
    ]
  }
}
```

## 使用示例

### 加载新格式的 RL 数据

```python
from lever_lm.models.v3 import (
    RLBeamDatasetWithEmbedding,
    collate_fn_rl_v3,
    load_beam_data
)
from torch.utils.data import DataLoader
import torch

# 1. 加载 RL 数据
rl_data = load_beam_data("results/okvqa/generated_data/rl_data_RandSampler.json")

# 2. 加载 embeddings
query_embeddings = torch.load("results/okvqa/cache/query_embeddings.pt")
candidate_embeddings = torch.load("results/okvqa/cache/candidate_embeddings.pt")
candidate_indices = list(range(len(candidate_embeddings)))

# 3. 创建数据集
dataset = RLBeamDatasetWithEmbedding(
    rl_data=rl_data,
    query_embeddings=query_embeddings,
    candidate_embeddings=candidate_embeddings,
    candidate_indices=candidate_indices,
    shot_num=2,
    normalize_rewards=True,
    reward_alpha=0.2,  # quality权重
    reward_beta=1.0,   # correctness权重
    reward_correctness_mode="pm1",  # "+1/-1" 模式
    use_logprob=False,  # 使用 beam_score
    filter_gen_methods=None  # 不过滤，使用所有生成方法
)

# 4. 创建 DataLoader（batch_size=1，每个batch是一个query-group）
dataloader = DataLoader(
    dataset,
    batch_size=1,  # 重要：batch_size=1
    shuffle=True,
    collate_fn=collate_fn_rl_v3
)

# 5. 使用
for batch in dataloader:
    query_emb = batch["query_emb"]  # [1, d]
    cand_emb = batch["cand_emb"]    # [1, K, d]
    beam_labels = batch["beam_labels"]  # [1, num_candidates, shot_num]
    beam_rewards = batch["beam_rewards"]  # [1, num_candidates]（已归一化）
    beam_rewards_raw = batch["beam_rewards_raw"]  # [1, num_candidates]（原始reward）
    
    # 如果数据中包含logprobs
    if "beam_logprobs" in batch:
        beam_logprobs = batch["beam_logprobs"]  # [1, num_candidates]
```

## 参数说明

### `RLBeamDatasetWithEmbedding` 参数

- `rl_data`: RL数据字典
- `query_embeddings`: Query embeddings [num_queries, d]
- `candidate_embeddings`: Candidate embeddings [num_candidates, d]
- `candidate_indices`: Candidate索引列表
- `shot_num`: Shot数量（默认2）
- `normalize_rewards`: 是否归一化奖励（默认True）
- `reward_alpha`: Quality权重（默认0.2）
- `reward_beta`: Correctness权重（默认1.0）
- `reward_correctness_mode`: Correctness模式，"01"或"pm1"（默认"pm1"）
- `use_logprob`: 是否使用logprob_score而非beam_score（默认False）
- `filter_gen_methods`: 过滤的生成方法列表（如["beam", "sample"]），None表示不过滤

### Reward 计算

Reward 计算公式（按照强化学习.md §3.4）：
```
reward = α * quality + β * correctness_val
```

其中：
- `quality`: `beam_score` 或 `-logprob_score`（取决于 `use_logprob`）
- `correctness_val`: 
  - "01"模式：`vqa_correct` (0/1) 或 `vqa_acc_score` [0,1]
  - "pm1"模式：`2 * vqa_correct - 1` (+1/-1) 或 `2 * vqa_acc_score - 1` [-1,1]

## 过滤生成方法

如果只想使用特定的生成方法（例如只使用beam和sample，不使用random）：

```python
dataset = RLBeamDatasetWithEmbedding(
    rl_data=rl_data,
    query_embeddings=query_embeddings,
    candidate_embeddings=candidate_embeddings,
    candidate_indices=candidate_indices,
    filter_gen_methods=["beam", "sample"]  # 只使用beam和sample
)
```

## 在训练脚本中使用

在 `grpo_post_train.py` 中使用新数据集：

```python
from lever_lm.models.v3 import (
    RLBeamDatasetWithEmbedding,
    collate_fn_rl_v3
)

# 加载数据
rl_data = load_beam_data(args.rl_data_path)
train_data, val_data = split_beam_data(rl_data, train_ratio=0.8)

# 创建数据集
train_dataset = RLBeamDatasetWithEmbedding(
    rl_data=train_data,
    query_embeddings=query_embeddings,
    candidate_embeddings=candidate_embeddings,
    candidate_indices=candidate_indices,
    reward_alpha=args.reward_alpha,
    reward_beta=args.reward_beta
)

# 创建 DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=1,  # batch_size=1
    shuffle=True,
    collate_fn=collate_fn_rl_v3
)
```

## 注意事项

1. **batch_size=1**: 每个batch对应一个query-group，这样GRPO的组内归一化才能正确工作
2. **Reward归一化**: 默认开启组内Z-score归一化，确保不同query的reward分布一致
3. **Logprobs**: 如果数据中包含`logprob_score`，会自动保存为`beam_logprobs`，用于old_log_probs计算
4. **向后兼容**: 旧的`BeamDataset`仍然可用，用于加载旧格式的数据
