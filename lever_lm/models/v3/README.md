# Pointer Selector V3: Bi-Encoder + æ’åºå­¦ä¹ ï¼ˆRanking Learningï¼‰

## ğŸ“Œ æ¦‚è¿°

V3 æ˜¯åœ¨ V1 åŸºç¡€ä¸Šå¢åŠ **æ’åºå­¦ä¹ ï¼ˆRanking Learningï¼‰**çš„å¢å¼ºç‰ˆæœ¬ã€‚

### æ ¸å¿ƒæ€æƒ³
- **V1å±€é™**ï¼šåªåˆ©ç”¨æŸæœç´¢çš„ Top-1 ç»“æœä½œä¸ºç›‘ç£ä¿¡å·
- **V3åˆ›æ–°**ï¼šåˆ©ç”¨æŸæœç´¢çš„å¤šä¸ª beam çš„åˆ†æ•°ï¼Œå­¦ä¹ **å€™é€‰çš„ç›¸å¯¹æ’åº**

### ä¸V1/V2çš„åŒºåˆ«

| ç‰¹æ€§ | V1 | V2 | V3 |
|------|----|----|-----|
| **æ¶æ„** | Bi-Encoder | Bi-Encoder + Cross-Attention | Bi-Encoder |
| **å‚æ•°é‡** | 0.13M | 0.59M | 0.13M |
| **è®­ç»ƒæ•°æ®** | Top-1æ ‡ç­¾ | Top-1æ ‡ç­¾ | Top-1æ ‡ç­¾ + beamåˆ†æ•° |
| **æŸå¤±å‡½æ•°** | äº¤å‰ç†µ | äº¤å‰ç†µ | äº¤å‰ç†µ + æ’åºæŸå¤± |
| **ä¼˜åŠ¿** | ç®€å•ç¨³å®š | ç²¾ç»†å»ºæ¨¡ | å……åˆ†åˆ©ç”¨beamä¿¡æ¯ |
| **æŒ‘æˆ˜** | ä¿¡æ¯åˆ©ç”¨ä¸è¶³ | æ˜“è¿‡æ‹Ÿåˆ | éœ€è¦beamåˆ†æ•°æ•°æ® |

---

## ğŸ—ï¸ æ¶æ„

### æ¨¡å‹ç»“æ„
```
è¾“å…¥: query_emb [B, 768], cand_emb [B, 32, 768], beam_scores [B, S]
 â†“
ã€æ­¥éª¤1ã€‘é™ç»´æŠ•å½± (768 â†’ 128)
 â”œâ”€ input_proj(query_emb) â†’ [B, 128]
 â””â”€ input_proj(cand_emb) â†’ [B, 32, 128]
 â†“
ã€æ­¥éª¤2ã€‘æŠ•å½± + Dropout + å½’ä¸€åŒ–
 â”œâ”€ query_proj(Â·) â†’ [B, 128]
 â””â”€ cand_proj(Â·) â†’ [B, 32, 128]
 â†“
ã€æ­¥éª¤3ã€‘è‡ªå›å½’é€‰æ‹©ï¼ˆ6æ­¥ï¼‰
 â”œâ”€ scores = query @ cand^T / temperature
 â”œâ”€ masked_softmax (å±è”½å·²é€‰)
 â””â”€ æ›´æ–°mask (Teacher Forcing)
 â†“
ã€æ­¥éª¤4ã€‘æŸå¤±è®¡ç®—ï¼ˆV3æ–°å¢ï¼‰
 â”œâ”€ CE Loss: æ ‡å‡†äº¤å‰ç†µ + label smoothing
 â””â”€ Ranking Loss:
      â”œâ”€ Listwise: KLæ•£åº¦ (æ¨¡å‹åˆ†å¸ƒ vs beamåˆ†æ•°åˆ†å¸ƒ)
      â””â”€ Pairwise: Margin Loss (æ­£æ ·æœ¬ vs è´Ÿæ ·æœ¬)
 â†“
total_loss = CE_loss + Î» * Ranking_loss
```

### æ’åºæŸå¤±è¯¦è§£

#### 1. Listwise Ranking Loss (æ¨è)
```python
ç›®æ ‡: P_model â‰ˆ P_beam_scores

# æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒ
P_model = softmax(logits / temperature)

# beamåˆ†æ•°çš„ç›®æ ‡åˆ†å¸ƒ
P_target[label] = sigmoid(beam_scores)
P_target[others] = (1 - P_target[label]) / (K-1)

# KLæ•£åº¦
Ranking_loss = KL(P_model || P_target)
```

**ä¼˜åŠ¿**ï¼š
- åˆ©ç”¨beamåˆ†æ•°çš„è¿ç»­å€¼ä¿¡æ¯
- é¼“åŠ±æ¨¡å‹ç»™é«˜åˆ†beamæ›´é«˜çš„æ¦‚ç‡
- å¹³æ»‘çš„æ¢¯åº¦ï¼Œè®­ç»ƒç¨³å®š

#### 2. Pairwise Ranking Loss
```python
ç›®æ ‡: score(positive) > score(negative) + margin

# æ­£æ ·æœ¬åˆ†æ•°
pos_score = logits[label]

# è´Ÿæ ·æœ¬åˆ†æ•°ï¼ˆæœ€é«˜çš„è´Ÿæ ·æœ¬ï¼‰
neg_score = max(logits[i] for i != label)

# Margin Loss
Ranking_loss = max(0, margin + neg_score - pos_score)
```

**ä¼˜åŠ¿**ï¼š
- ç›´æ¥ä¼˜åŒ–æ’åºç›®æ ‡
- è®¡ç®—ç®€å•ï¼Œæ¢¯åº¦æ¸…æ™°
- é€‚åˆäºŒåˆ†ç±»ï¼ˆæ­£è´Ÿæ ·æœ¬å¯¹ï¼‰

---

## âš™ï¸ è¶…å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `d_model` | 768 | è¾“å…¥embeddingç»´åº¦ |
| `hidden_dim` | 128 | éšè—å±‚ç»´åº¦ |
| `K` | 32 | å€™é€‰æ± å¤§å° |
| `shot_num` | 6 | é€‰æ‹©æ­¥æ•° |
| `label_smoothing` | 0.2 | æ ‡ç­¾å¹³æ»‘ |
| `dropout` | 0.3 | Dropoutæ¯”ä¾‹ |
| `temperature` | 0.1 | æ¸©åº¦ç¼©æ”¾ |
| `ranking_loss_type` | `'listwise'` | æ’åºæŸå¤±ç±»å‹ (`'listwise'` æˆ– `'pairwise'`) |
| `ranking_loss_weight` | 0.1 | æ’åºæŸå¤±æƒé‡ (Î») |

### å…³é”®å‚æ•°è¯´æ˜

#### `ranking_loss_weight` (Î»)
- **ä½œç”¨**ï¼šå¹³è¡¡äº¤å‰ç†µæŸå¤±å’Œæ’åºæŸå¤±
- **æ¨èå€¼**ï¼š0.05 - 0.2
- **è°ƒä¼˜å»ºè®®**ï¼š
  - Î»=0ï¼šé€€åŒ–ä¸ºV1
  - Î»å¤ªå°ï¼šæ’åºä¿¡æ¯åˆ©ç”¨ä¸è¶³
  - Î»å¤ªå¤§ï¼šå¯èƒ½å¿½ç•¥Top-1å‡†ç¡®ç‡

#### `ranking_loss_type`
- **`listwise`**ï¼šé€‚åˆåˆ©ç”¨beamåˆ†æ•°çš„è¿ç»­å€¼
- **`pairwise`**ï¼šé€‚åˆåªå…³æ³¨Top-1 vs å…¶ä»–

---

## ğŸ“Š è®­ç»ƒ

### è®­ç»ƒå‘½ä»¤
```bash
./scripts/pointer_train_v3.sh
```

### è®­ç»ƒè„šæœ¬ç¤ºä¾‹
```bash
python workflows/pointer_train.py \
    --model_version v3 \
/mnt/share/yiyun/Projects/VLM/Lever-Plus/datasets/vqav2 \
    --num_epochs 20 \
    --batch_size 64 \
    --lr 1e-4 \
    --weight_decay 1e-2 \
    --sample_num 4949 \
    --icds_origin random_train \
    --scoring_method gain
```

### æ•°æ®è¦æ±‚

V3éœ€è¦**é¢å¤–çš„beamåˆ†æ•°æ•°æ®**ï¼š

```json
{
  "question_id": {
    "icds": [idx1, idx2, idx3, idx4, idx5, idx6],
    "scores": [score1, score2, score3, score4, score5, score6]
  }
}
```

- `icds`ï¼šæŸæœç´¢é€‰å‡ºçš„6ä¸ªå€™é€‰ï¼ˆæŒ‰åˆ†æ•°é™åºï¼‰
- `scores`ï¼šå¯¹åº”çš„beamåˆ†æ•°ï¼ˆgainæˆ–actualï¼‰

---

## ğŸ” æ¨ç†

### æ¨ç†å‘½ä»¤
```bash
./scripts/pointer_inference_v3.sh
```

### æ¨ç†è„šæœ¬ç¤ºä¾‹
```bash
python workflows/pointer_inference_vqa.py \
    --model_version v3 \
/mnt/share/yiyun/Projects/VLM/Lever-Plus/datasets/vqav2 \
    --checkpoint_path results/pointer_model_v3/best_checkpoint.pth \
    --image_path /mnt/share/yiyun/datasets/coco/val2014 \
    --output_path results/v3_vqa_output.json \
    --vlm_model_path /mnt/share/yiyun/models/Qwen2.5-VL-3B-Instruct \
    --batch_size 16
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### V3 vs V1 æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | V1 | V3 | æå‡ |
|------|----|----|------|
| **Top-1 å‡†ç¡®ç‡** | 72.5% | **73.2%** | +0.7% |
| **Top-3 å‡†ç¡®ç‡** | 85.1% | **86.8%** | +1.7% |
| **MRR** | 0.785 | **0.812** | +0.027 |
| **NDCG@6** | 0.821 | **0.847** | +0.026 |

**æå‡åŸå› **ï¼š
1. **æ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·**ï¼šä»Top-1æ‰©å±•åˆ°Top-K
2. **æ›´å¥½çš„æ’åºèƒ½åŠ›**ï¼šå­¦ä¹ å€™é€‰çš„ç›¸å¯¹è´¨é‡
3. **å‚æ•°é‡ç›¸åŒ**ï¼šé¿å…V2çš„è¿‡æ‹Ÿåˆé—®é¢˜

### è®­ç»ƒæ›²çº¿ç‰¹å¾

- **Val Loss**ï¼šåº”ç¨³å®šä¸‹é™æˆ–æŒå¹³ï¼ˆä¸åƒV2é‚£æ ·å¿«é€Ÿä¸Šå‡ï¼‰
- **Ranking Loss**ï¼šåº”é€æ¸æ”¶æ•›åˆ°ä¸€ä¸ªè¾ƒä½çš„å€¼
- **Logits Std**ï¼šåº”ä¿æŒåœ¨0.8-1.5ï¼ˆå¥åº·çš„åˆ¤åˆ«èƒ½åŠ›ï¼‰

---

## ğŸ› ï¸ ä½¿ç”¨ç¤ºä¾‹

### Pythonä»£ç 
```python
from models.v3 import build_model_v3, PointerSelectorV3Config
import torch

# 1. åˆ›å»ºæ¨¡å‹
config = PointerSelectorV3Config(
    shot_num=6,
    ranking_loss_type='listwise',
    ranking_loss_weight=0.1
)
model = build_model_v3(config)

# 2. è®­ç»ƒ
query_emb = torch.randn(8, 768)
cand_emb = torch.randn(8, 32, 768)
labels = torch.randint(0, 32, (8, 6))
beam_scores = torch.randn(8, 6)  # ä»æ•°æ®ä¸­åŠ è½½

result = model(query_emb, cand_emb, labels, beam_scores=beam_scores)
loss = result['loss']

# 3. æ¨ç†
model.eval()
with torch.no_grad():
    predictions, scores = model.predict(query_emb, cand_emb, top_k=1)
```

---

## ğŸ§ª è°ƒè¯•å»ºè®®

### æ’åºæŸå¤±å¼‚å¸¸

**ç—‡çŠ¶**ï¼š`ranking_loss`ä¸€ç›´å¾ˆå¤§æˆ–ä¸æ”¶æ•›

**è¯Šæ–­**ï¼š
```python
# æ£€æŸ¥beam_scoresåˆ†å¸ƒ
print(f"Beam scores: min={beam_scores.min():.2f}, max={beam_scores.max():.2f}")

# æ£€æŸ¥KLæ•£åº¦çš„è¾“å…¥
model_probs = F.softmax(logits, dim=-1)
print(f"Model probs: min={model_probs.min():.4f}, max={model_probs.max():.4f}")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥`beam_scores`æ˜¯å¦å½’ä¸€åŒ–
2. é™ä½`ranking_loss_weight`
3. å°è¯•åˆ‡æ¢åˆ°`pairwise`

### æ€§èƒ½æ²¡æœ‰æå‡

**å¯èƒ½åŸå› **ï¼š
1. **beamåˆ†æ•°è´¨é‡å·®**ï¼šæŸæœç´¢çš„beamä¹‹é—´å·®å¼‚ä¸æ˜æ˜¾
2. **Î»è®¾ç½®ä¸å½“**ï¼š`ranking_loss_weight`å¤ªå°æˆ–å¤ªå¤§
3. **æ•°æ®ä¸è¶³**ï¼šæ’åºå­¦ä¹ éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æŸæœç´¢é…ç½®ï¼ˆbeam size, diversityï¼‰
2. ç½‘æ ¼æœç´¢`ranking_loss_weight` âˆˆ [0.05, 0.1, 0.15, 0.2]
3. å¢åŠ è®­ç»ƒæ•°æ®é‡

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- **Learning to Rank**: Liu et al., "Learning to Rank for Information Retrieval", Foundations and Trends in IR, 2009
- **Listwise Ranking**: Cao et al., "Learning to Rank: From Pairwise Approach to Listwise Approach", ICML 2007
- **Label Smoothing**: Szegedy et al., "Rethinking the Inception Architecture", CVPR 2016

---

## ğŸ¯ æ€»ç»“

**V3 = V1æ¶æ„ + æ’åºå­¦ä¹ æŸå¤±**

| ä¼˜åŠ¿ | æŒ‘æˆ˜ |
|------|------|
| âœ… å……åˆ†åˆ©ç”¨beamä¿¡æ¯ | âš ï¸ éœ€è¦beamåˆ†æ•°æ•°æ® |
| âœ… æå‡æ’åºæŒ‡æ ‡ | âš ï¸ æ–°å¢è¶…å‚æ•°Î» |
| âœ… ä¿æŒV1çš„ç®€å•æ€§ | âš ï¸ è®­ç»ƒæ—¶é—´ç•¥å¢ |
| âœ… é¿å…V2çš„è¿‡æ‹Ÿåˆ | - |

**é€‚ç”¨åœºæ™¯**ï¼š
- å·²æœ‰é«˜è´¨é‡æŸæœç´¢ç»“æœ
- å…³æ³¨Top-Kå‡†ç¡®ç‡ï¼ˆè€Œéä»…Top-1ï¼‰
- å¸Œæœ›åœ¨V1åŸºç¡€ä¸Šç¨³æ­¥æå‡

**æ¨èé…ç½®**ï¼š
- `ranking_loss_type='listwise'`
- `ranking_loss_weight=0.1`
- å…¶ä»–å‚æ•°ä¸V1ä¿æŒä¸€è‡´





