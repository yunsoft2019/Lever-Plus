# grpo_post_train.py 更新完成总结

## ✅ 已完成的更新

### 1. 自动数据格式检测
- ✅ 自动检测数据是**旧格式**（`id_list`/`score_list`）还是**新格式**（`pointer_candidates`）
- ✅ 根据格式自动选择合适的数据集类和 collate 函数

### 2. 支持新格式数据（RL数据）
- ✅ 使用 `RLBeamDatasetWithEmbedding` 加载新格式数据
- ✅ 使用 `collate_fn_rl_v3` 处理新格式的batch
- ✅ 自动将 batch_size 调整为 1（新格式要求）

### 3. 向后兼容旧格式
- ✅ 继续支持旧格式数据（`BeamDatasetWithEmbedding`）
- ✅ 保持原有功能不变

### 4. Reward 参数支持
- ✅ 添加 `--reward_alpha` 参数（默认0.2）
- ✅ 添加 `--reward_beta` 参数（默认1.0）
- ✅ 添加 `--reward_correctness_mode` 参数（默认"pm1"）
- ✅ 添加 `--use_logprob` 参数（默认False）

### 5. 修复兼容性问题
- ✅ 修复 `compute_old_log_probs` 方法，兼容新格式（batch_size=1）
- ✅ 修复 `train_grpo_epoch` 方法，兼容新格式的 query_id 格式

## 使用示例

### 使用旧格式数据（向后兼容）
```bash
python -m lever_lm.workflows.grpo_post_train \
    --beam_data results/okvqa/generated_data/beam_RandSampler.json \
    --img_emb results/okvqa/cache/img_embeddings.pt \
    --batch_size 32 \
    --rce_epochs 1 \
    --grpo_epochs 3
```

### 使用新格式数据（包含correctness）
```bash
python -m lever_lm.workflows.grpo_post_train \
    --beam_data results/okvqa/generated_data/rl_data_RandSampler.json \
    --img_emb results/okvqa/cache/img_embeddings.pt \
    --batch_size 1 \
    --reward_alpha 0.2 \
    --reward_beta 1.0 \
    --reward_correctness_mode pm1 \
    --rce_epochs 1 \
    --grpo_epochs 3
```

## 关键改进

1. **自动检测**：无需手动指定数据格式，脚本会自动检测
2. **向后兼容**：旧格式数据仍然可以正常使用
3. **组合Reward**：新格式数据使用 `reward = α * quality + β * correctness`
4. **数据多样性**：支持beam + sample + random等多种生成方法

## 注意事项

1. **Batch Size**：
   - 旧格式：可以使用任意 batch_size（如32）
   - 新格式：**必须使用 batch_size=1**（脚本会自动调整）

2. **Reward参数**：
   - 旧格式：忽略 reward 相关参数
   - 新格式：使用 reward 参数计算组合 reward

3. **数据格式**：
   - 旧格式：`{"id_list": [...], "score_list": [...]}`
   - 新格式：`{"pointer_candidates": [...]}`

## 下一步

现在可以：
1. ✅ 使用新格式的RL数据训练v3模型
2. ✅ 利用correctness信号优化策略
3. ✅ 使用多样化的数据生成方法（beam + sample + random）

建议先生成少量RL数据测试流程，确认无误后再进行完整训练。
