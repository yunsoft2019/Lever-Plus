import json
import os
import sys
import uuid

import hydra
import torch
from dotenv import load_dotenv
from loguru import logger
from omegaconf import DictConfig
from tqdm import tqdm

from lever_lm.utils import init_interface
from open_mmicl.metrics.vqa_metrics import compute_vqa_accuracy
from open_mmicl.retriever import RandRetriever
from utils import load_ds, vqa_postprocess


def inference_vqa_direct(
    interface,
    train_ds,
    test_ds,
    icd_idx_list,
    val_ques_path,
    val_ann_path,
    model_name,
    generation_kwargs,
):
    """
    ç›´æ¥æ¨ç†VQAä»»åŠ¡ï¼ŒæŒ‰ç…§æ­¥éª¤ï¼š
    1. éå†æµ‹è¯•é›†
    2. æ ¹æ®èŒƒä¾‹idæ‰¾åˆ°èŒƒä¾‹æ•°æ®
    3. åŒ…è£…messagesï¼ˆåŒºåˆ†Flamingoå’ŒQwen2.5-VLï¼‰
    4. è¾“å…¥æ¨¡å‹å¾—åˆ°ç­”æ¡ˆ
    5. è®¡ç®—å‡†ç¡®ç‡
    """
    preds = []
    
    # éå†æµ‹è¯•é›†
    for idx, sample in enumerate(tqdm(test_ds, desc="æ¨ç†ä¸­", ncols=100)):
        if icd_idx_list is not None and idx < len(icd_idx_list):
            example_indices = icd_idx_list[idx]
            
            # æ­¥éª¤4ï¼šæ ¹æ®èŒƒä¾‹idï¼Œæ‰¾åˆ°èŒƒä¾‹çš„å›¾ç‰‡ï¼Œé—®é¢˜ï¼Œç­”æ¡ˆ
            ice_sample_list = []
            for ex_idx in example_indices:
                if ex_idx < len(train_ds):
                    ice_sample_list.append(train_ds[ex_idx])
                else:
                    logger.warning(f"è­¦å‘Šï¼šèŒƒä¾‹ç´¢å¼• {ex_idx} è¶…å‡ºè®­ç»ƒé›†èŒƒå›´ï¼ˆè®­ç»ƒé›†å¤§å°: {len(train_ds)}ï¼‰")
            
            # å°†èŒƒä¾‹å’Œæµ‹è¯•æ ·æœ¬ç»„åˆ
            data_sample_list = ice_sample_list + [sample]
            
            # æ­¥éª¤5ï¼šåŒ…è£…messagesï¼ˆåŒºåˆ†Flamingoå’ŒQwen2.5-VLï¼‰
            # ä½¿ç”¨transfer_promptsè½¬æ¢ä¸ºpromptæ ¼å¼
            prompts = interface.transfer_prompts(
                [data_sample_list], is_last_for_generation=True
            )
            
            # ä½¿ç”¨prepare_inputè½¬æ¢ä¸ºmessagesæ ¼å¼ï¼ˆtensorï¼‰
            input_dict = interface.prepare_input(
                prompts, is_last_for_generation=True
            )
            
            # å¤„ç† BatchFeature å¯¹è±¡ï¼Œè½¬æ¢ä¸º dict
            if hasattr(input_dict, 'data'):
                input_dict = dict(input_dict.data)
            elif not isinstance(input_dict, dict):
                input_dict = dict(input_dict)
            
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            data = {k: v.to(interface.device) if isinstance(v, torch.Tensor) else v 
                   for k, v in input_dict.items()}
            
            # å¤„ç† Qwen2.5-VL çš„ç‰¹æ®Šæƒ…å†µï¼ˆimage_grid_thwï¼‰
            if 'image_grid_thw' in data:
                if data['image_grid_thw'].dim() == 3 and data['image_grid_thw'].shape[0] == 1:
                    # ç§»é™¤batchç»´åº¦
                    data['image_grid_thw'] = data['image_grid_thw'].squeeze(0)
                    if 'image_nums' in data:
                        if isinstance(data['image_nums'], torch.Tensor) and data['image_nums'].numel() > 1:
                            data['image_nums'] = data['image_nums'][0:1]
                        elif isinstance(data['image_nums'], list) and len(data['image_nums']) > 0:
                            data['image_nums'] = torch.tensor([data['image_nums'][0]], dtype=torch.long)
                elif data['image_grid_thw'].dim() == 2:
                    if 'image_nums' not in data:
                        num_images = data['image_grid_thw'].shape[0]
                        data['image_nums'] = torch.tensor([num_images], dtype=torch.long)
            
            # æ­¥éª¤6ï¼šæŠŠmessagesè¾“å…¥æ¨ç†æ¨¡å‹ï¼Œå¾—åˆ°æ¨ç†ç­”æ¡ˆ
            prompt_len = int(data["attention_mask"].shape[1])
            
            with torch.inference_mode():
                outputs = interface.generate(
                    **data,
                    eos_token_id=interface.tokenizer.eos_token_id,
                    pad_token_id=interface.tokenizer.pad_token_id,
                    **generation_kwargs,
                )
            
            # è§£ç ç”Ÿæˆç»“æœ
            if isinstance(outputs, torch.Tensor):
                outputs = outputs.tolist()
            
            # ç¡®ä¿outputsæ˜¯åˆ—è¡¨æ ¼å¼
            if not isinstance(outputs, list):
                outputs = [outputs]
            if len(outputs) > 0 and not isinstance(outputs[0], list):
                outputs = [outputs]
            
            # è§£ç ï¼šåªå–promptä¹‹åçš„éƒ¨åˆ†
            generated = interface.tokenizer.batch_decode(
                [output[prompt_len:] for output in outputs],
                skip_special_tokens=True,
            )
            
            # åå¤„ç†å¾—åˆ°answer
            prediction = generated[0] if generated else ""
            answer = vqa_postprocess(prediction, model_name=model_name)
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            question_id = sample.get('question_id', None)
            if question_id is not None:
                preds.append({
                    "answer": answer,
                    "question_id": question_id,
                })
            else:
                logger.warning(f"æ ·æœ¬ {idx}: ç¼ºå°‘ question_idï¼Œæ— æ³•ç”¨äºè®¡ç®—å‡†ç¡®ç‡")
        else:
            logger.warning(f"æ ·æœ¬ {idx}: æ— æ³•è·å– ICDs åˆ—è¡¨ï¼ˆicd_idx_list ä¸ºç©ºæˆ–ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼‰")
    
    # æ­¥éª¤7ï¼šæ ¹æ®æ¨ç†ç­”æ¡ˆè®¡ç®—å‡†ç¡®ç‡
    if len(preds) > 0:
        random_uuid = str(uuid.uuid4())
        temp_result_file = f"{random_uuid}.json"
        
        with open(temp_result_file, "w") as f:
            json.dump(preds, f, indent=4)
        
        try:
            accuracy = compute_vqa_accuracy(temp_result_file, val_ques_path, val_ann_path)
            # å¤„ç†å‡†ç¡®ç‡æ ¼å¼
            if accuracy > 1:
                accuracy_percent = accuracy
                accuracy_decimal = accuracy / 100
            else:
                accuracy_decimal = accuracy
                accuracy_percent = accuracy * 100
            return accuracy_decimal
        finally:
            if os.path.exists(temp_result_file):
                os.remove(temp_result_file)
    else:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœï¼Œæ— æ³•è®¡ç®—å‡†ç¡®ç‡")
        return 0.0


@hydra.main(version_base=None, config_path="./configs", config_name="inference.yaml")
def main(cfg: DictConfig):
    # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º INFOï¼Œè¿‡æ»¤æ‰ DEBUG æ—¥å¿—ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡æ˜¾ç¤º
    logger.remove()  # ç§»é™¤é»˜è®¤çš„ handler
    logger.add(sys.stderr, level="INFO", format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level} | {name}:{function}:{line} - {message}")  # åªæ˜¾ç¤º INFO åŠä»¥ä¸Šçº§åˆ«
    
    # æ‰“å°å…³é”®é…ç½®ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("éšæœºèŒƒä¾‹æ¨ç†é…ç½®ä¿¡æ¯:")
    logger.info(f"  ä»»åŠ¡ç±»å‹ (cfg.task.task_name): {cfg.task.task_name}")
    logger.info(f"  æ•°æ®é›†åç§° (cfg.dataset.name): {cfg.dataset.name}")
    logger.info(f"  æ•°æ®é›†ç‰ˆæœ¬ (cfg.dataset.version): {cfg.dataset.get('version', 'N/A')}")
    logger.info(f"  éªŒè¯é›†è·¯å¾„ (cfg.dataset.val_path): {cfg.dataset.get('val_path', 'N/A')}")
    logger.info("=" * 60)
    
    # åŠ è½½æ•°æ®é›†
    logger.info("å¼€å§‹åŠ è½½æ•°æ®é›†...")
    logger.info(f"load_ds å°†æ ¹æ® cfg.task.task_name='{cfg.task.task_name}' æ¥é€‰æ‹©åŠ è½½å‡½æ•°")
    
    ds = load_ds(cfg)
    logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œæ•°æ®é›†é”®: {list(ds.keys())}")
    
    # è·å–æµ‹è¯•é›†å’Œè®­ç»ƒé›†
    test_ds = ds["validation"]
    train_ds = ds["train"]
    logger.info(f"æµ‹è¯•é›†å­—æ®µ: {test_ds.column_names}")
    logger.info(f"æµ‹è¯•é›†æ€»æ ·æœ¬æ•°: {len(test_ds)}")
    logger.info(f"è®­ç»ƒé›†æ€»æ ·æœ¬æ•°: {len(train_ds)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åº”è¯¥å­˜åœ¨çš„å­—æ®µ
    if "captions" in test_ds.column_names or "single_caption" in test_ds.column_names:
        logger.warning("âš ï¸  è­¦å‘Šï¼šæµ‹è¯•é›†ä¸­åŒ…å« caption ç›¸å…³å­—æ®µï¼")
        logger.warning(f"   è¿™ä¸åº”è¯¥å‡ºç°åœ¨ VQA æ•°æ®é›†ä¸­ã€‚")
        logger.warning(f"   å½“å‰ task_name: {cfg.task.task_name}")
        logger.warning(f"   å¯èƒ½åŠ è½½äº†é”™è¯¯çš„æ•°æ®é›†ç±»å‹ï¼")
    
    # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹å’Œshotæ•°é‡
    # æ³¨æ„ï¼šåªæµ‹è¯•å½“å‰æŒ‡å®šçš„å•ä¸ªæ¨¡å‹
    
    # ä»å‘½ä»¤è¡Œå‚æ•°ä¸­è·å– infer_model çš„å€¼ï¼ˆé…ç½®æ–‡ä»¶åï¼‰
    # sys å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
    infer_model_config = None
    for arg in sys.argv:
        if arg.startswith("infer_model="):
            infer_model_config = arg.split("=", 1)[1]
            break
    
    if not infer_model_config:
        logger.error("é”™è¯¯: å¿…é¡»æŒ‡å®š infer_model å‚æ•°")
        logger.error("ç”¨æ³•: python random_predict.py task=vqa dataset=okvqa_local infer_model=flamingo_3B")
        return
    
    # åªæµ‹è¯•æŒ‡å®šçš„å•ä¸ªæ¨¡å‹
    models_to_test = [infer_model_config]
    logger.info(f"æµ‹è¯•æ¨¡å‹: {infer_model_config}")
    
    shot_num_list = [1, 2, 3, 4, 6, 8]
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
    val_ques_path = cfg.dataset.get('val_ques_path', None)
    val_ann_path = cfg.dataset.get('val_ann_path', None)
    
    if not val_ques_path or not val_ann_path:
        logger.error("âš ï¸  ç¼ºå°‘ val_ques_path æˆ– val_ann_pathï¼Œæ— æ³•è®¡ç®—å‡†ç¡®ç‡")
        logger.error(f"   val_ques_path: {val_ques_path}")
        logger.error(f"   val_ann_path: {val_ann_path}")
        return
    
    # è·å–ç”Ÿæˆå‚æ•°
    generation_kwargs = cfg.task.gen_args if hasattr(cfg.task, 'gen_args') else {}
    
    # åˆå§‹åŒ–éšæœºæ£€ç´¢å™¨
    logger.info("=" * 60)
    logger.info("åˆå§‹åŒ–éšæœºæ£€ç´¢å™¨ï¼ˆRandomRetrieverï¼‰...")
    retriever = RandRetriever(
        train_ds,
        test_ds,
        seed=cfg.get('seed', 42),
        fixed=cfg.get('random_retrieval_fixed', True),
    )
    logger.info("éšæœºæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info("=" * 60)
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    results = {}
    
    # éå†æ¯ä¸ªæ¨¡å‹
    for model_config_name in models_to_test:
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹æµ‹è¯•æ¨¡å‹é…ç½®: {model_config_name}")
        logger.info("=" * 60)
        
        # ä¿å­˜åŸå§‹é…ç½®
        from omegaconf import OmegaConf
        import os
        original_infer_model = OmegaConf.create(OmegaConf.to_container(cfg.infer_model))
        
        # åŠ è½½æ–°çš„æ¨¡å‹é…ç½®æ–‡ä»¶
        config_file = os.path.join("configs", "infer_model", f"{model_config_name}.yaml")
        if not os.path.exists(config_file):
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            continue
        
        try:
            # åŠ è½½æ–°çš„æ¨¡å‹é…ç½®
            new_model_config = OmegaConf.load(config_file)
            # æ›´æ–°å½“å‰cfgçš„infer_modeléƒ¨åˆ†
            cfg.infer_model = new_model_config
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
            logger.error(f"é…ç½®æ–‡ä»¶: {config_file}")
            continue
        
        # è·å–å®é™…çš„æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—å’Œç»“æœä¿å­˜ï¼‰
        actual_model_name = cfg.infer_model.name
        
        # åŠ è½½æ¨ç†æ¨¡å‹
        logger.info(f"åŠ è½½æ¨ç†æ¨¡å‹: {actual_model_name} (é…ç½®: {model_config_name})")
        logger.info(f"  è®¾å¤‡: {cfg.device}")
        logger.info(f"  ç²¾åº¦: {cfg.precision}")
        interface = init_interface(cfg, device=cfg.device)
        logger.info("æ¨ç†æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # å­˜å‚¨è¯¥æ¨¡å‹çš„ç»“æœ
        model_results = {}
        
        # æå‰å‡†å¤‡ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå¢é‡ä¿å­˜ï¼‰
        dataset_name = cfg.dataset.name.replace('_local', '')
        result_dir = os.path.join(
            cfg.result_dir,
            dataset_name,
            "icl_inference",
        )
        os.makedirs(result_dir, exist_ok=True)
        model_name_safe = actual_model_name.replace('.', '_').replace('-', '_').replace('/', '_')
        result_filename = f"{model_name_safe}_RandomRetriever_baseline_metrics.json"
        result_json_path = os.path.join(result_dir, result_filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°è¯•åŠ è½½å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
        if os.path.exists(result_json_path):
            try:
                with open(result_json_path, "r") as f:
                    existing_results = json.load(f)
                    logger.info(f"å‘ç°å·²æœ‰ç»“æœæ–‡ä»¶ï¼ŒåŠ è½½å·²æœ‰ç»“æœ: {result_json_path}")
                    model_results = existing_results
            except Exception as e:
                logger.warning(f"åŠ è½½å·²æœ‰ç»“æœå¤±è´¥ï¼Œå°†é‡æ–°å¼€å§‹: {e}")
                model_results = {}
        
        # éå†æ¯ä¸ªshotæ•°é‡
        for shot_num in shot_num_list:
            # æ£€æŸ¥è¯¥shot_numæ˜¯å¦å·²å®Œæˆï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
            shot_key = f"shot_num_{shot_num}"
            if shot_key in model_results:
                logger.info("=" * 60)
                logger.info(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {actual_model_name} with shot_num={shot_num}")
                logger.info(f"   å·²æœ‰ç»“æœ: {model_results[shot_key]:.4f} ({model_results[shot_key]*100:.2f}%)")
                logger.info("=" * 60)
                continue
            
            logger.info("=" * 60)
            logger.info(f"å¼€å§‹æµ‹è¯•: {actual_model_name} with shot_num={shot_num}")
            logger.info("=" * 60)
            
            # ä½¿ç”¨éšæœºæ£€ç´¢å™¨è·å–èŒƒä¾‹åˆ—è¡¨
            logger.info(f"ä½¿ç”¨éšæœºæ£€ç´¢å™¨æ£€ç´¢èŒƒä¾‹ï¼ˆshot_num={shot_num}ï¼‰...")
            icd_idx_list = retriever.retrieve(shot_num)
            logger.info(f"èŒƒä¾‹æ£€ç´¢å®Œæˆï¼Œå…± {len(icd_idx_list)} ä¸ªæµ‹è¯•æ ·æœ¬çš„èŒƒä¾‹åˆ—è¡¨")
            
            # è¿›è¡Œæ¨ç†å¹¶è®¡ç®—å‡†ç¡®ç‡
            logger.info("å¼€å§‹æ¨ç†...")
            accuracy = inference_vqa_direct(
                interface=interface,
                train_ds=train_ds,
                test_ds=test_ds,
                icd_idx_list=icd_idx_list,
                val_ques_path=val_ques_path,
                val_ann_path=val_ann_path,
                model_name=actual_model_name,
                generation_kwargs=generation_kwargs,
            )
            
            # ä¿å­˜ç»“æœ
            model_results[f"shot_num_{shot_num}"] = accuracy
            accuracy_percent = accuracy * 100 if accuracy <= 1 else accuracy
            
            # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¢é‡ä¿å­˜ï¼Œé˜²æ­¢ä¸­é€”å´©æºƒä¸¢å¤±ç»“æœï¼‰
            logger.info(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°æ–‡ä»¶: {result_json_path}")
            with open(result_json_path, "w") as f:
                json.dump(model_results, f, indent=4)
            logger.info("âœ… ç»“æœå·²ä¿å­˜")
            
            logger.info("=" * 60)
            logger.info(f"âœ… {actual_model_name} - shot_num={shot_num}: {accuracy:.4f} ({accuracy_percent:.2f}%)")
            logger.info("=" * 60)
        
        # æ¢å¤åŸå§‹é…ç½®
        cfg.infer_model = original_infer_model
        
        # ä¿å­˜è¯¥æ¨¡å‹çš„æ‰€æœ‰ç»“æœï¼ˆä½¿ç”¨å®é™…æ¨¡å‹åç§°ä½œä¸ºkeyï¼Œæ›´æ˜“è¯»ï¼‰
        results[actual_model_name] = model_results
    
    # æ”¶é›†æ‰€æœ‰ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæœ€ç»ˆæ±‡æ€»æ˜¾ç¤ºï¼‰
    result_files = {}
    dataset_name = cfg.dataset.name.replace('_local', '')
    result_dir = os.path.join(
        cfg.result_dir,
        dataset_name,
        "icl_inference",
    )
    for model_name, model_results in results.items():
        model_name_safe = model_name.replace('.', '_').replace('-', '_').replace('/', '_')
        result_filename = f"{model_name_safe}_RandomRetriever_baseline_metrics.json"
        result_json_path = os.path.join(result_dir, result_filename)
        result_files[model_name] = result_json_path
    
    logger.info("=" * 60)
    logger.info("ç»“æœæ–‡ä»¶ä¿å­˜ä½ç½®:")
    for model_name, file_path in result_files.items():
        logger.info(f"  {model_name}: {file_path}")
    logger.info("=" * 60)
    
    # æ‰“å°æœ€ç»ˆç»“æœæ±‡æ€»
    logger.info("=" * 60)
    logger.info("æœ€ç»ˆç»“æœæ±‡æ€»:")
    logger.info("=" * 60)
    for model_name, model_results in results.items():
        logger.info(f"\n{model_name}:")
        for shot_key, accuracy in sorted(model_results.items()):
            accuracy_percent = accuracy * 100 if accuracy <= 1 else accuracy
            logger.info(f"  {shot_key}: {accuracy:.4f} ({accuracy_percent:.2f}%)")
    logger.info("=" * 60)
    logger.info("âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° icl_inference ç›®å½•")
    logger.info("=" * 60)


if __name__ == "__main__":
    load_dotenv()
    main()

