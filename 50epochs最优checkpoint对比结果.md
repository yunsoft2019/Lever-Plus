# 50 Epochs 训练最优 Checkpoint 对比结果

> 日期：2025-12-23  
> 对比：50 epochs 训练后的最优 checkpoint (grpo_epoch1) vs 之前的结果

---

## 一、测试配置

- **数据集**: OKVQA
- **测试数据量**: 100 samples
- **推理模型**: Qwen2.5-VL-3B-Instruct
- **采样器**: RandSampler
- **Checkpoint**: `grpo_epoch1.pt`（50 epochs 训练后，Val Loss 最小）

---

## 二、结果对比（100 samples）

### 2.1 完整对比表

| Shot | Baseline (12-12) | 方案五之前 (grpo_epoch3) | 50 epochs 最优 (grpo_epoch1) | vs Baseline | vs 方案五之前 |
|------|------------------|--------------------------|------------------------------|-------------|---------------|
| **1** | 64.2% | 63.8% | **64.8%** | **+0.6%** ⬆️ | **+1.0%** ⬆️ |
| **2** | 64.8% | 65.8% | **63.8%** | **-1.0%** ⬇️ | **-2.0%** ⬇️ |
| **3** | 62.8% | 62.2% | **63.2%** | **+0.4%** ⬆️ | **+1.0%** ⬆️ |
| **4** | 61.4% | 61.8% | **61.8%** | **+0.4%** ⬆️ | **0.0%** ➡️ |

### 2.2 统计汇总

| 对比项 | vs Baseline | vs 方案五之前 |
|--------|-------------|---------------|
| **平均差异** | **+0.1%** | **0.0%** |
| **最大提升** | +0.6% (Shot 1) | +1.0% (Shot 1, 3) |
| **最大下降** | -1.0% (Shot 2) | -2.0% (Shot 2) |
| **提升的配置数** | 3/4 (75%) | 2/4 (50%) |
| **下降的配置数** | 1/4 (25%) | 1/4 (25%) |
| **持平的配置数** | 0/4 (0%) | 1/4 (25%) |

---

## 三、详细分析

### 3.1 vs Baseline（方案五之前）

| Shot | Baseline | 50 epochs 最优 | 差异 | 评价 |
|------|----------|----------------|------|------|
| Shot 1 | 64.2% | **64.8%** | **+0.6%** | ⬆️ 明显提升 |
| Shot 2 | 64.8% | 63.8% | **-1.0%** | ⬇️ 明显下降 |
| Shot 3 | 62.8% | **63.2%** | **+0.4%** | ⬆️ 略有提升 |
| Shot 4 | 61.4% | **61.8%** | **+0.4%** | ⬆️ 略有提升 |

**结论**：
- ✅ Shot 1 提升明显（+0.6%）
- ⚠️ Shot 2 下降明显（-1.0%），这是主要问题
- ✅ Shot 3 和 Shot 4 略有提升（+0.4%）
- 📊 平均提升 +0.1%，基本持平

### 3.2 vs 方案五之前（grpo_epoch3）

| Shot | 方案五之前 | 50 epochs 最优 | 差异 | 评价 |
|------|-----------|----------------|------|------|
| Shot 1 | 63.8% | **64.8%** | **+1.0%** | ⬆️ **明显提升** |
| Shot 2 | **65.8%** | 63.8% | **-2.0%** | ⬇️ **明显下降** |
| Shot 3 | 62.2% | **63.2%** | **+1.0%** | ⬆️ **明显提升** |
| Shot 4 | 61.8% | 61.8% | **0.0%** | ➡️ 持平 |

**结论**：
- ✅ Shot 1 和 Shot 3 提升明显（+1.0%）
- ⚠️ Shot 2 下降明显（-2.0%），这是主要问题
- ➡️ Shot 4 持平
- 📊 平均差异 0.0%，但波动较大

---

## 四、关键发现

### 4.1 Shot 2 下降的原因分析

**问题**：Shot 2 是主要应用场景，但表现下降：
- vs Baseline: -1.0%
- vs 方案五之前: -2.0%

**可能的原因**：
1. **过拟合**：50 epochs 训练可能导致过拟合，特别是在 Shot 2 场景
2. **Checkpoint 选择**：Epoch 1 虽然 Val Loss 最小，但可能不是 Shot 2 场景的最优 checkpoint
3. **训练不稳定**：Adv Std 仍然很小（0.0063），说明 advantage 信号弱，训练可能不稳定

### 4.2 Shot 1 和 Shot 3 提升的原因

**优势**：
- Shot 1: +0.6% (vs Baseline), +1.0% (vs 方案五之前)
- Shot 3: +0.4% (vs Baseline), +1.0% (vs 方案五之前)

**可能的原因**：
- 50 epochs 训练让模型在单 shot 和多 shot 场景上都有提升
- 但 Shot 2 场景可能因为过拟合而下降

---

## 五、建议

### 5.1 尝试其他 Checkpoint

由于 Shot 2 下降明显，建议尝试其他 epoch 的 checkpoint：

```bash
# 尝试 Epoch 2（Val Loss 第二小）
python scripts/convert_v3_to_v2_format.py \
    --v3_ckpt ./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/grpo_epoch2.pt

# 尝试 Epoch 6（Val Loss 第三小）
python scripts/convert_v3_to_v2_format.py \
    --v3_ckpt ./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/grpo_epoch6.pt
```

### 5.2 检查训练过程

从训练日志看，Adv Std 仍然很小（0.0062-0.0063），说明：
- 方案五（关闭 Rank Normalization）可能没有完全生效
- 或者 reward 差异本身很小，导致 advantage 信号弱

**建议检查**：
1. 确认 `compute_advantage` 方法是否使用了 Z-score 归一化（而非 rank normalization）
2. 检查训练数据中的 reward 分布
3. 考虑增大学习率或调整其他参数

### 5.3 最佳策略

**当前最佳方案**：
- 对于 Shot 1 和 Shot 3：使用 `grpo_epoch1`（50 epochs 训练后）
- 对于 Shot 2：可能需要使用 `grpo_epoch3`（方案五之前）或其他 epoch

---

## 六、总结

### 6.1 50 Epochs 训练的效果

1. **整体表现**：
   - 平均正确率 vs Baseline: +0.1%（基本持平）
   - 平均正确率 vs 方案五之前: 0.0%（持平）

2. **优势**：
   - ✅ Shot 1 提升明显（+0.6% vs Baseline, +1.0% vs 方案五之前）
   - ✅ Shot 3 提升明显（+0.4% vs Baseline, +1.0% vs 方案五之前）

3. **劣势**：
   - ⚠️ Shot 2 下降明显（-1.0% vs Baseline, -2.0% vs 方案五之前）
   - ⚠️ 训练可能过拟合，特别是在 Shot 2 场景

### 6.2 结论

**50 epochs 训练的效果有限**：
- 虽然训练了 50 个 epochs，但整体提升很小（+0.1%）
- Shot 2 场景反而下降，这是主要问题
- Adv Std 仍然很小（0.0063），说明 advantage 信号弱，训练可能不稳定

**建议**：
1. 尝试其他 epoch 的 checkpoint（特别是 Epoch 2、6、4）
2. 检查方案五的代码修改是否完全生效
3. 考虑调整训练参数（学习率、KL beta 等）

---

*报告生成时间：2025-12-23*


