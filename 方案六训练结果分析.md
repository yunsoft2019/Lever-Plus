# 方案六训练结果分析（方案 5 + 方案 6）

> 训练日期：2025-12-23  
> 训练配置：USE_RANK_ADVANTAGE=false, GRPO_LR=5e-5, GRPO_EPOCHS=50, KL_BETA=0.1

---

## 一、训练完成情况

✅ **训练已完成**：50 个 GRPO epochs 全部完成

**训练配置**：
- USE_RANK_ADVANTAGE: false（方案五：关闭 Rank Normalization）
- GRPO_LR: 5e-5（方案六：从 5e-6 提升 10 倍）
- GRPO_EPOCHS: 50
- KL_BETA: 0.1

---

## 二、最优 Checkpoint 分析

### 2.1 按 Val Loss 排序（Top 10）

| Epoch | Val Loss | PPO Loss | KL | Adv Std | 评价 |
|-------|----------|----------|-----|---------|------|
| **2** | **5.15152** | 0.00005 | 0.03565 | 0.0062 | ⭐ **最优** |
| 1 | 5.15333 | -0.00042 | 0.03542 | 0.0063 | 第二 |
| 3 | 5.15363 | -0.00019 | 0.03664 | 0.0062 | 第三 |
| 4 | 5.15676 | 0.00009 | 0.03280 | 0.0062 | 第四 |
| 5 | 5.15958 | -0.00003 | 0.03797 | 0.0062 | 第五 |
| 22 | 5.21149 | -0.00050 | 0.03581 | 0.0062 | ... |
| 49 | 5.22740 | -0.00095 | 0.03266 | 0.0062 | ... |
| 50 | 5.23428 | -0.00087 | 0.03167 | 0.0062 | 最后 |

### 2.2 最优 Checkpoint：**Epoch 2**

**指标详情**：
- **Val Loss**: 5.15152（最小）
- **Train Loss**: 0.00009
- **PPO Loss**: 0.00005（仍然很小，但比方案五略大）
- **KL**: 0.03565（合理范围）
- **Adv Std**: 0.0062（仍然很小，与方案五相同）
- **Adv Max**: 0.0090
- **Beta**: 0.0010

**Checkpoint 文件**：
- v3 格式：`grpo_epoch2.pt`
- v2 格式：需要转换（如果不存在）

---

## 三、训练指标分析

### 3.1 与方案五对比

| 指标 | 方案五（Epoch 2） | 方案六（Epoch 2） | 变化 |
|------|------------------|------------------|------|
| **Val Loss** | 5.14132 | **5.15152** | ⬆️ +0.01020（略差） |
| **PPO Loss** | -0.00007 | **0.00005** | ⬆️ 提升（从负变正） |
| **KL** | 0.04286 | **0.03565** | ⬇️ 降低（更好） |
| **Adv Std** | 0.0062 | **0.0062** | ➡️ 相同 |
| **GRPO_LR** | 5e-6 | **5e-5** | ⬆️ 提升 10 倍 |

### 3.2 关键发现

1. **Val Loss 略差**：
   - 方案六：5.15152
   - 方案五：5.14132
   - **差异：+0.01020**（方案六略差）

2. **PPO Loss 提升**：
   - 方案五：-0.00007（负值，可能有问题）
   - 方案六：0.00005（正值，更合理）
   - **说明学习率增大后，参数更新更有效**

3. **Adv Std 仍然很小**：
   - 方案五和方案六都是 0.0062
   - **说明 advantage 信号仍然很弱，学习率增大并没有改变这一点**

4. **KL 降低**：
   - 方案六的 KL 更小（0.03565 vs 0.04286）
   - **说明模型更接近参考策略**

---

## 四、训练趋势分析

### 4.1 Val Loss 趋势

- **Epoch 1-5**：Val Loss 在 5.15-5.16 之间（最佳范围）
- **Epoch 6-20**：Val Loss 逐渐上升（5.17-5.22）
- **Epoch 21-50**：Val Loss 在 5.23-5.27 之间（持续上升）

**结论**：**早期 epoch（1-5）表现最好，后期出现过拟合**

### 4.2 PPO Loss 趋势

- **Epoch 1**：-0.00042（负值）
- **Epoch 2-7**：正值，但很小（0.00005-0.00068）
- **Epoch 8-50**：负值（-0.00012 到 -0.00098）

**结论**：**PPO Loss 在早期为正，后期变为负，说明训练不稳定**

### 4.3 Adv Std 趋势

- **所有 epoch**：Adv Std = 0.0062（恒定）
- **Adv Max**：0.0090（恒定）

**结论**：**Advantage 信号在整个训练过程中都很弱，没有改善**

---

## 五、与方案五对比总结

### 5.1 方案六的优势

1. ✅ **PPO Loss 更合理**：
   - 方案五：-0.00007（负值，可能有问题）
   - 方案六：0.00005（正值，更合理）

2. ✅ **KL 更小**：
   - 方案六的 KL 更小，说明模型更接近参考策略

3. ✅ **学习率增大后参数更新更有效**：
   - 虽然 Adv Std 仍然很小，但学习率增大后能产生更大的参数更新

### 5.2 方案六的劣势

1. ⚠️ **Val Loss 略差**：
   - 方案六：5.15152
   - 方案五：5.14132
   - **差异：+0.01020**

2. ⚠️ **Adv Std 仍然很小**：
   - 方案五和方案六都是 0.0062
   - **学习率增大并没有改善 advantage 信号**

3. ⚠️ **训练不稳定**：
   - PPO Loss 在后期变为负值
   - Val Loss 在后期持续上升

---

## 六、后续建议

### 6.1 立即行动：测试 Epoch 2 的推理效果

**最优 Checkpoint**：`grpo_epoch2.pt`

**推理命令**：
```bash
# 1. 转换 checkpoint 格式（如果不存在）
python scripts/convert_v3_to_v2_format.py \
    --v3_ckpt ./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/grpo_epoch2.pt

# 2. 运行推理（800 samples）
export LEVER_LM_CHECKPOINT_PATH=./results/okvqa/model_cpk/v3_RandSampler_Qwen2_5-VL-3B-Instruct/grpo_epoch2_v2format.ckpt
bash scripts/inference.sh vqa okvqa_local 0 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B v3 800
```

### 6.2 如果方案六效果不够好

**可能的原因**：
1. ⚠️ Val Loss 略差（5.15152 vs 5.14132）
2. ⚠️ Adv Std 仍然很小（0.0062）
3. ⚠️ 学习率增大后训练不稳定

**建议**：
1. 🔄 **尝试方案 5 + 6 + 8**（Reward Shaping）：增大 reward 差异，从而增大 advantage
2. 🔄 **调整学习率**：尝试 2e-5 或 1e-5（介于 5e-6 和 5e-5 之间）
3. 🔄 **尝试方案 5 + 6 + 9**（Curriculum Learning）：逐步增加训练难度

### 6.3 如果方案六效果良好

✅ **继续优化**：
- 尝试方案 5 + 6 + 8（Reward Shaping）
- 预期进一步提升到 2-5%

---

## 七、总结

### 7.1 方案六的关键发现

1. ✅ **训练完成**：50 epochs 全部完成
2. ✅ **最优 Checkpoint**：Epoch 2（Val Loss: 5.15152）
3. ⚠️ **Val Loss 略差**：比方案五高 0.01020
4. ⚠️ **Adv Std 仍然很小**：0.0062（与方案五相同）
5. ✅ **PPO Loss 更合理**：从负值变为正值

### 7.2 下一步行动

1. **立即测试**：使用 Epoch 2 checkpoint 进行推理（800 samples）
2. **对比结果**：与方案五（Epoch 2）和 Baseline 对比
3. **决定方向**：
   - 如果效果好 → 继续优化（方案 5+6+8）
   - 如果效果不够 → 尝试其他方案

---

*分析时间：2025-12-23*


