# Lever-Plus Pointer Selector 强化学习后训练详细方案  

（RCE + GRPO / REINFORCE++，含数据生成与采样策略）

> **核心问题**：  
>
> - Pointer Selector 只输出 2 个 token（两个样本索引），传统 SFT 已经训练到一个还不错的策略。  
> - 当前 RL 数据主要来自 **beam search**，担心“强化学习只是把 beam 结果再学一遍”，提升有限。  
> - 希望：  
>   - 强化学习的数据不仅来自 beam，还能通过 **温度采样增加随机性**；  
>   - 在构建数据集时，**“能否答对”必须被记录**（即 correctness 必须收集），以后 reward 用不用可以再决定；  
>   - 得到一套可以直接落地、脚本化的 **完整计划书（Markdown + 伪代码）**。

本文是一个可执行的工程计划，适合直接放到仓库 `docs/` 目录中作为 RL 设计文档。

---

## 0. 背景与问题重述

### 0.1 当前系统

- 模块：**Pointer Selector v2 / v2_lora**  
  - 输入：  
    - `query_emb`: 图像 + 问题编码得到的向量 `[d]`；  
    - `cand_emb`: 候选范例池的向量 `[K, d]`。  
  - 输出：  
    - 一个长度固定为 `S=2` 的索引序列（2 个“token”），表示从 `K` 个候选中选哪两个样本做 in-context 示例。  

- 现有训练：  
  - 已经完成 v2 / v2_lora 的 **监督微调（SFT）**。  
  - 利用 beam search 在训练集上生成了多个候选 pointer 序列（beam），每条 beam 对应一个 `score`。  

- 下游任务：  
  - 使用 Pointer Selector 选出的两个样本构建 prompt（ICD），送到 VQA 模型（如 Flamingo/Qwen-VL），对图像问题进行作答。  
  - 通过比较 VQA 输出和 GT answer，可以得到一条 pointer 组合的 **正确性 correctness（0/1）**。

### 0.2 本文目标

本计划解决三个关键点：

1. **强化学习数据是否必须用 beam？**  
   - 回答：不必须。我们推荐：**beam + 温度采样 + 少量随机组合** 的混合策略，以兼顾高质量与多样性。

2. **如何通过温度采样增加随机性，同时又不把分布搞得太乱？**  
   - 使用 pointer 模型自身的分布，在不同温度下采样多个 pointer 序列：  
     - 低温（如 `τ=0.7`）接近 greedy；  
     - 中温（`τ=1.0`）接近原始 SFT 分布；  
     - 稍高温（`τ=1.3`）增加探索。  

3. **构建数据集时，如何保证 correctness 信息“必备”？**  
   - 设计统一的数据生成脚本：对 **每一条 pointer 序列** 必须调用下游 VQA 模型，计算答案是否正确，记录为 `correct` 或 `acc_score`。  
   - 即使当前 reward 不用 correctness，数据格式也要预留该字段，方便以后切换 reward 设计。

---

## 1. 强化学习数据：Beam vs 采样 vs 随机

### 1.1 一定要用 beam 吗？

不一定，但强烈建议 **保留 beam**，原因：

- beam 提供了 **高质量、稳定的“好策略”样本**：  
  - 对每个 query，beam search 找到 SFT 模型下概率最高的一小撮 pointer 序列；  
  - 这些序列往往在 VQA 上表现比较好，是“正样本密集区域”。

- 强化学习的一个主要作用是：  
  - **在已有轨迹空间内重新分配概率**，把更多质量集中到高 reward 的动作上，  
  - 同时适度探索更多未充分利用的动作（如果有采样数据）。  

如果完全不用 beam，而只靠随机采样或高温采样：

- 风险是：  
  - 数据质量可能骤降，大部分 pointer 序列都会给 VQA 带来错误答案，  
  - 强化学习会见到大量 reward 很差的数据，训练信号很 noisy。

结论：

- Beam **不是必须的唯一来源**，但应该作为 RL 数据的 **“高质量支撑部分”** 保留下来。

---

### 1.2 加温度采样的好处

为了避免“RL 只是把 beam 再学一遍”，我们需要引入 **额外的多样性**，而温度采样是最自然的方式：

- Pointer Selector 是一个自回归模型：  

  - 第一步：在 K 个候选中选第一个 index。  
  - 第二步：在剩余 K-1 个候选中选第二个 index。  

- 对每一步，我们有 logits 向量 `logits ∈ ℝ^K`（mask 掉已选位置），SFT 时做的是 `softmax(logits / 1.0)` + greedy / beam。  

- 温度采样做的是：

  ```python
  probs = softmax(logits / tau)
  idx = categorical_sample(probs)
  ```

- 温度 `τ` 的含义：  

  - `τ < 1`：分布更“尖锐”，更接近 argmax；  
  - `τ = 1`：原始分布；  
  - `τ > 1`：分布更均匀，增加随机性。  

#### 1.2.1 温度区间建议

对 pointer 这样的小离散动作空间：

- 推荐温度：  
  - `τ_low = 0.7`（轻微贪心）；  
  - `τ_mid = 1.0`（基线分布）；  
  - `τ_high = 1.3`（适度探索）。  

- 每个温度采样几条（比如 2~3 条）pointer 序列即可，避免数据爆炸。

---

### 1.3 随机组合是否有必要？

随机组合 = 完全脱离模型分布，在 K 个候选中 **均匀随机选择 2 个不同 index** 的 pointer 序列：

- 好处：  
  - 提供了模型很少会“自发”选择的组合，增加数据支持域（support）；  
  - 让 RL 看见一些“非常差但也可能偶尔很好的”组合，有助于估计极端 reward。  

- 坏处：  
  - 大多数随机组合的 VQA 正确率会非常低，导致 reward 大量为负；  
  - 如果比例过高，会把训练变成“学会避免所有东西”，而缺乏正向 signal。  

建议：

- **可选项**，按小比例加入，例如：
  - 每个 query 随机采样 1~2 条 pointer 序列；  
  - 这些样本的存在更多是作为负样本和支持多样性。  

---

### 1.4 推荐的数据来源配比

对每个 query（图 + 问题），推荐的候选 pointer 序列来源：

| 来源             | 数量示例 | 说明                                   |
| ---------------- | -------- | -------------------------------------- |
| beam search      | 3~5 条   | 稳定高质量样本，高 reward 区域支撑     |
| τ=1.0 采样       | 2~3 条   | 接近 SFT 分布的中间样本                |
| τ=1.3 采样       | 2~3 条   | 稍高温度，提升探索                     |
| 随机组合（可选） | 1~2 条   | 低概率区域样本，提供负样本与拓展支持域 |

可根据实际算力与数据量调整配比，比如控制在每个 query 总共 8~12 条 pointer 序列。

---

### 1.5 correctness 信息：必须采集

无论是 beam、温度采样还是随机组合，**必须统一执行以下步骤**：

1. 给定 pointer 序列（两个索引） → 选出对应的两个 candidates 示例。  
2. 把这两个示例 + 原始图像 + 问题拼成 VQA prompt（in-context 文本/图像序列）。  
3. 调 VQA 模型生成答案 `pred_answer`。  
4. 通过标准评估脚本与 GT 答案对比，得到：  
   - `correct ∈ {0,1}` 或 soft score，例如 VQAv2 风格的 `acc ∈ [0,1]`。  

并将 correctness 信息 **写入数据集** 中的每一条样本记录里，哪怕当前 reward 只使用 beam score，也不能省略 correctness 的记录。

---

## 2. 数据与文件格式设计

### 2.1 原始生成数据（raw RL data）

建议原始生成数据采用 JSONL 或 JSON 格式，按 query 存储。示例 schema：

```jsonc
{
  "query_id": "12345",       // 图像 + 问题的唯一 ID
  "meta": {
    "image_id": "COCO_000001",
    "question": "What is the man holding?",
    "split": "train"
  },
  "candidate_pool": {
    "candidate_ids": [101, 102, 103, ...],     // 候选样本的 ID
    "size": 32
  },
  "pointer_candidates": [
    {
      "pointer": [7, 22],                      // 两个 index（相对 candidate_pool）
      "gen_method": "beam",
      "beam_rank": 0,
      "beam_score": 2.13,
      "logprob_score": -1.56,                  // pointer 序列在 SFT 模型下的 log π_old（可选）
      "temperature": null,                     // beam 时为 null
      "vqa_pred_answer": "a book",             // VQA 输出答案
      "vqa_correct": 1,                        // correctness（必须有）
      "vqa_acc_score": 1.0                     // 如 VQAv2 风格 [0,1]（可选）
    },
    {
      "pointer": [5, 18],
      "gen_method": "sample",
      "beam_rank": null,
      "beam_score": null,                      // 如果用不到可以置 null
      "logprob_score": -2.03,
      "temperature": 1.0,
      "vqa_pred_answer": "a notebook",
      "vqa_correct": 1,
      "vqa_acc_score": 0.9
    },
    {
      "pointer": [3, 9],
      "gen_method": "random",
      "beam_rank": null,
      "beam_score": null,
      "logprob_score": null,
      "temperature": null,
      "vqa_pred_answer": "a phone",
      "vqa_correct": 0,
      "vqa_acc_score": 0.0
    }
  ]
}
```

特点：

- 对每个 query，我们统一管理一个 `pointer_candidates` 列表，包含多个不同来源的 pointer 序列。  
- 每条 pointer 序列都存：  
  - 生成方式 `gen_method`；  
  - 可能的 `beam_score` 或 `logprob_score`；  
  - **必须**存 `vqa_correct` 或 `vqa_acc_score`；  
  - 为 RL 预留信息，如 `logprob_score` 用来构造 `old_log_prob`。

这种格式比较冗余，但易于扩展、容易下游处理。

---

### 2.2 RL 训练数据（processed RL data）

从原始生成数据中，我们构造 RL 训练使用的“扁平化”数据结构，一般是一个列表，每条是一个 pointer 样本：

```python
{
  "query_id": "12345",
  "pointer": [7, 22],               # labels
  "gen_method": "beam",
  "beam_score": 2.13,
  "logprob_score": -1.56,           # π_old 的 log_prob（如果当时存了的话）
  "vqa_correct": 1,
  "vqa_acc_score": 1.0,
  # 一般会额外加：
  "reward": 0.85                     # 预先计算好的最终 reward（可选，或训练时再算）
}
```

最终送给 `BeamTrajectoryDataset` 的结构是按 query 分组的：

```python
{
  "query_emb": Tensor[B, d],
  "cand_emb": Tensor[B, K, d],
  "labels":   Tensor[B, 2],         # pointer 序列
  "rewards":  Tensor[B],            # reward
  "group_ids": Tensor[B],           # 都是同一个 query 的 id
  "raw_scores": Tensor[B],          # 可选：原始 beam_score / logprob
  "query_id": "12345"
}
```

---

### 2.3 embedding_store.pt

为了加速 RL 训练，我们将 query 与 candidate embedding 统一导出为一个 `.pt` 文件：

```python
{
  "12345": {
    "query_emb": Tensor[d],
    "cand_emb": Tensor[K, d]
  },
  "12346": {
    "query_emb": Tensor[d],
    "cand_emb": Tensor[K, d]
  },
  ...
}
```

这个文件通过一个独立脚本从 SFT 模型导出（见后面 §3.2）。

---

## 3. 数据生成与采样流程（详细说明 + 伪代码）

### 3.1 总体流程

对每个训练样本（图像 + 问题）：

1. 生成候选池 `candidate_pool`（由你之前的 pipeline 决定）。  
2. 用 pointer SFT 模型 **执行 beam search** 得到几条高质量 pointer 序列。  
3. 用相同模型 **执行温度采样**（多个温度）得到多条 pointer 序列。  
4. （可选）采样少量完全随机 pointer 序列。  
5. 对上述所有 pointer 序列：  
   - 构造 ICD + prompt；  
   - 调 VQA 模型，获得答案与 correctness；  
   - 记录到 JSON。  

### 3.2 Pointer 采样伪代码（包含 beam + 温度 + 随机）

```python
def generate_pointer_candidates_for_query(
    model,              # Pointer Selector (SFT 版本)
    query_emb,          # [d]
    cand_emb,           # [K, d]
    num_beams=5,
    temps=(1.0, 1.3),
    num_samples_per_temp=2,
    num_random=1,
):
    """
    返回：一个 list，每个元素是 dict:
    {
      "pointer": [i, j],
      "gen_method": "beam" / "sample" / "random",
      "beam_rank": ...,
      "beam_score": ...,
      "logprob_score": ...,
      "temperature": ...
    }
    """

    candidates = []
    K = cand_emb.shape[0]

    # 1) Beam search（已存在逻辑，可直接调用你当前的 beam 函数）
    beam_results = model.beam_search(
        query_emb=query_emb,
        cand_emb=cand_emb,
        num_beams=num_beams,
    )
    # beam_results: List[{"pointer": [i, j], "score": float, "logprob": float}]
    for rank, br in enumerate(beam_results):
        candidates.append({
            "pointer": br["pointer"],
            "gen_method": "beam",
            "beam_rank": rank,
            "beam_score": br["score"],
            "logprob_score": br["logprob"],   # 可选，没有就设 None
            "temperature": None,
        })

    # 2) 温度采样
    for tau in temps:
        for _ in range(num_samples_per_temp):
            pointer, logprob = sample_pointer_with_temperature(
                model=model,
                query_emb=query_emb,
                cand_emb=cand_emb,
                tau=tau,
            )
            candidates.append({
                "pointer": pointer,
                "gen_method": "sample",
                "beam_rank": None,
                "beam_score": None,
                "logprob_score": logprob,
                "temperature": tau,
            })

    # 3) 随机组合（可选）
    import random
    for _ in range(num_random):
        i, j = random.sample(range(K), 2)
        pointer = [i, j]
        candidates.append({
            "pointer": pointer,
            "gen_method": "random",
            "beam_rank": None,
            "beam_score": None,
            "logprob_score": None,
            "temperature": None,
        })

    # 4) 去重（同一个 pointer 只保留一个，优先 beam > sample > random）
    uniq = {}
    priority = {"beam": 3, "sample": 2, "random": 1}

    for c in candidates:
        key = tuple(sorted(c["pointer"]))
        if key not in uniq:
            uniq[key] = c
        else:
            # 保留优先级高的
            if priority[c["gen_method"]] > priority[uniq[key]["gen_method"]]:
                uniq[key] = c

    final_candidates = list(uniq.values())
    return final_candidates


def sample_pointer_with_temperature(model, query_emb, cand_emb, tau: float):
    """
    对 pointer 进行两步采样：
    step1: 从 K 个 candidates 中选第一个 index
    step2: 在剩余 K-1 个 candidates 中选第二个 index
    同时返回整个 pointer 序列的 logprob。
    """
    import torch
    import torch.nn.functional as F

    device = query_emb.device
    query_emb = query_emb.unsqueeze(0)           # [1, d]
    cand_emb = cand_emb.unsqueeze(0)             # [1, K, d]
    K = cand_emb.shape[1]

    selected = []
    total_logprob = 0.0

    # step 1
    logits_step1 = model.compute_step_logits(
        query_emb=query_emb,
        cand_emb=cand_emb,
        selected_indices=None,
    )  # [1, K]
    logits_step1 = logits_step1[0] / tau         # [K]
    probs_step1 = F.softmax(logits_step1, dim=-1)  # [K]
    idx1 = torch.multinomial(probs_step1, num_samples=1).item()
    total_logprob += torch.log(probs_step1[idx1] + 1e-8).item()
    selected.append(idx1)

    # step 2: mask 已选 index
    mask = torch.ones(K, dtype=torch.bool, device=device)
    mask[idx1] = False
    logits_step2 = model.compute_step_logits(
        query_emb=query_emb,
        cand_emb=cand_emb,
        selected_indices=[idx1],
    )[0]  # [K]
    logits_step2 = logits_step2 / tau
    logits_step2_masked = logits_step2.masked_fill(~mask, -1e9)
    probs_step2 = F.softmax(logits_step2_masked, dim=-1)
    idx2 = torch.multinomial(probs_step2, num_samples=1).item()
    total_logprob += torch.log(probs_step2[idx2] + 1e-8).item()
    selected.append(idx2)

    return selected, total_logprob
```

> 注：上面用到了类似 `model.compute_step_logits` 的接口，你可以根据自己的 Pointer Selector 实现进行适配。实质是同一件事：给定已选索引和候选 embedding，输出下一步的 logits。

---

### 3.3 correctness 计算伪代码

对每个 pointer 序列，都要跑一次 VQA：

```python
def evaluate_pointer_candidate(
    vqa_model,
    image,
    question,
    candidate_pool,
    pointer,
    ground_truth_answers,
):
    """
    返回:
    - pred_answer: VQA 输出
    - correct: 0/1
    - acc_score: float (可选)
    """

    # 1) 根据 pointer 从 candidate_pool 里取出两个示例
    ex1 = candidate_pool[pointer[0]]
    ex2 = candidate_pool[pointer[1]]

    # 2) 构造 in-context prompt（文字+图像）
    prompt = build_vqa_prompt(image, question, ex1, ex2)

    # 3) 调 VQA 模型推理
    pred_answer = vqa_model.generate(prompt)

    # 4) 用标准评测脚本比较答案
    correct, acc_score = compute_vqa_accuracy(pred_answer, ground_truth_answers)

    return pred_answer, correct, acc_score
```

在数据生成主循环里：

```python
for each query in dataset:
    query_id = ...
    image = ...
    question = ...
    gt_answers = ...

    # 先获取 candidate_pool（可以是 embedding + meta 信息）
    candidate_pool = get_candidate_pool(query_id)

    # 用 pointer 模型生成 pointer_candidates（beam + sample + random）
    pointer_candidates = generate_pointer_candidates_for_query(
        model=pointer_model_sft,
        query_emb=query_emb,
        cand_emb=cand_emb,
        num_beams=5,
        temps=(1.0, 1.3),
        num_samples_per_temp=2,
        num_random=1,
    )

    # 对每条 pointer，计算 VQA correctness
    for c in pointer_candidates:
        pointer = c["pointer"]
        pred_answer, correct, acc_score = evaluate_pointer_candidate(
            vqa_model=vqa_model,
            image=image,
            question=question,
            candidate_pool=candidate_pool,
            pointer=pointer,
            ground_truth_answers=gt_answers,
        )

        c["vqa_pred_answer"] = pred_answer
        c["vqa_correct"] = int(correct)          # 必须有
        c["vqa_acc_score"] = float(acc_score)    # 建议保留

    # 最后保存为 JSON 结构（见 §2.1）
```

---

### 3.4 Reward 设计与计算

在得到 `beam_score` / `logprob_score` + correctness 后，可以定义多种 reward：

```python
def compute_reward_for_candidate(
    beam_score: float,
    logprob_score: float,
    vqa_correct: int,
    vqa_acc_score: float,
    alpha=0.2,
    beta=1.0,
    correctness_mode="pm1",  # "01" or "pm1"
    use_logprob=False,
):
    """
    返回一个标量 reward，数值范围建议控制在 [-5, 5] 内。
    """
    # correctness 部分
    if correctness_mode == "01":
        correctness_val = float(vqa_correct)     # 0 or 1
    else:
        # pm1 模式：正确=+1，错误=-1
        correctness_val = 2.0 * float(vqa_correct) - 1.0

    # 质量部分：可以选 beam_score 或 logprob_score 或不限
    if use_logprob and logprob_score is not None:
        quality = -logprob_score    # logprob 越大越好，因此取负号
    elif beam_score is not None:
        quality = float(beam_score)
    else:
        quality = 0.0

    # 具体归一化的 z-score 在 group 内处理（见 dataset_v3）
    # 这里只是一个线性组合模板
    reward = alpha * quality + beta * correctness_val

    # clip 一下数值
    reward = max(min(reward, 5.0), -5.0)
    return reward
```

> 最终在 `BeamTrajectoryDataset` 中，我们会对同一 query 的 reward 做二次归一化（z-score 或 minmax）以得到 group 内相对优势。

---

### 3.5 是否预先存储 old_log_prob？

- 如果在生成数据时就已经存了 `logprob_score`，并且这个 `logprob_score` 是 SFT 模型下 pointer 序列的 log π_old，那么在 RL 阶段：  
  - 可以省去再次用 SFT 模型计算 old_log_probs。  
  - 直接读这个值即可。  

- 如果当时没有存：  
  - RL 阶段可以再加载 SFT checkpoint，用 `compute_log_probs` 跑一遍，得到 old_log_probs；  
  - 性能上会慢一些，但逻辑清晰。

**建议**：  
数据生成时就多做一步，把 pointer 在 SFT 下的 logprob 一并存进 JSON（哪怕暂时用不到）。这能增强数据的通用性与可复用性。

---

## 4. RL Pipeline：RCE + GRPO / REINFORCE++

这一部分与前一版计划类似，只是在数据设计和采样策略上更丰富。

### 4.1 导出 embedding_store（可选但推荐）

使用独立脚本，将 SFT 训练数据中的 query 与 candidate embedding 导出为 `.pt` 文件：

- 优点：  
  - RL 阶段不再需要重复跑大模型/视觉 encoder。  
  - Pointer Selector 只负责从 embedding 上算 logits，训练速度快很多。

伪代码（见上版本中的 `export_pointer_embeddings.py`，此处不再重复）。

---

### 4.2 BeamTrajectoryDataset：按 query 分组的 RL 数据集

详见前一版本的 `dataset_v3.py`，这里补充几点与新数据格式相关的说明：

- 输入：  
  - `beam_json_path`: 新的原始 RL 数据（包含 pointer_candidates + correctness）；  
  - `embedding_store_path`: embedding_store.pt；  
  - `RewardConfig`: 控制 `alpha, beta, score_norm, reward_clip` 等。  

- 对每个 query：  
  - 从 `pointer_candidates` 中过滤/采样你想用的部分（例如 beam + sample + random 都要）；  
  - 对这些候选的 `beam_score` / `logprob_score` / `vqa_correct` 计算 group 内归一化后的 reward；  
  - 拆成 B 条样本，生成一个 `BeamGroup`。  

- `__getitem__` 返回：

```python
{
  "query_emb": [B, d],
  "cand_emb": [B, K, d],
  "labels": [B, 2],
  "rewards": [B],
  "group_ids": [B],
  "raw_scores": [B],      # 可选
  "query_id": str
}
```

> 注意：每个 group 对应一个 query。当前简化方案中，DataLoader 的 `batch_size=1`，每个 batch 恰好是一个 query-group，GRPO 的组内归一化天然成立。

---

### 4.3 RCE 预热阶段

RCE = Reward-weighted Cross-Entropy：

- 直观理解：  
  - 在原本的监督 CE loss 上，加上 reward 的权重；  
  - reward 高的样本在 loss 中权重大，reward 低的样本影响被压低甚至变成负贡献。  

- 好处：  
  - 比起直接上 GRPO / PPO，RCE 是一个“更像有监督”的平滑过渡；  
  - 可以先把策略从“贴近人工标签”拉到“贴近高 reward 的 beam / sample”。  

训练流程：

```python
out = model.forward_with_mode(
    query_emb=query_emb,
    cand_emb=cand_emb,
    labels=labels,
    rewards=rewards,            # group 内已归一化的 reward
    mode="rce",
)
loss = out["loss"]
loss.backward()
optimizer.step()
```

推荐设置：

- `rce_epochs = 1~2`；  
- `rce_temperature` 从 2.0 线性降到 0.5；  
- 学习率比 SFT 小 3~10 倍（如 1e-5）。

---

### 4.4 GRPO / REINFORCE++ 阶段

#### 4.4.1 组内优势计算（group-relative advantage）

无论是 GRPO 还是 REINFORCE++，对于 pointer 这样的“每个 query 有多条候选”的场景，**组内 advantage** 是自然的选择：

- 对同一 query 的 B 条候选 reward 做归一化（z-score 或 minmax）；  
- baseline 取为组内均值；  
- `advantage_i = (normalized_reward_i - baseline)`；  
- 再对 advantage 做一个 clip（如 [-5,5]）。  

这样可以保证：

- 如果一个 query 的所有 pointer 都很烂（reward 全部为负），优势仍然是相对的；  
- 如果有一两条 pointer 特别好，它们得到明显的正优势；  
- 对 RL 来说，“比较一组内部的相对好坏”比“绝对分数”往往更稳定。

#### 4.4.2 GRPO（带 PPO ratio）

- 使用 `old_log_probs`（来自 SFT 或 ref 模型）：  

  ```python
  log_ratio = current_log_probs - old_log_probs
  ratio = exp(log_ratio)
  
  surr1 = ratio * advantage
  surr2 = clip(ratio, 1-eps, 1+eps) * advantage
  
  policy_loss = -mean(min(surr1, surr2))
  ```

- 可选次优项：  

  - KL penalty：`kl_beta * (ratio - 1 - log_ratio)` 的均值。  

实现上已经在 `PointerSelectorV3.compute_grpo_loss` 里有雏形，可以基于 group_ids 做些微扩展。

#### 4.4.3 REINFORCE++ 风格

- 若不想上 PPO ratio，可以直接：

  ```python
  policy_loss = -mean(advantage * current_log_probs)
  ```

- 如需要一点稳定性，也可以加上 ratio 与 clip，变成“轻量版 PPO”：  

  - 逻辑与 GRPO 极其相似，只是 advantage 的归一化方式不同。

在 `PointerSelectorV3` 中我们加了 `compute_reinforcepp_loss`，调用方式：

```python
out = model.forward_with_mode(
    query_emb=query_emb,
    cand_emb=cand_emb,
    labels=labels,
    rewards=rewards,
    old_log_probs=old_log_probs,
    mode="reinforcepp",
)
loss = out["loss"]
```

---

### 4.5 避免“和 SFT 一样”的几个关键点

1. **数据多样性**  
   - Beam 只覆盖模型当前分布里最靠前的很小一部分 pointer 序列；  
   - 温度采样 + 随机组合让数据支持域扩展，RL 可以从中发现“原来 SFT 概率不高但 reward 很高”的组合。  

2. **reward 主导而非人工标签**  
   - SFT 的 loss 只关心“模仿标签”；  
   - RL 的 loss 关注的是 **reward**，标签只是 trajectory 的记录；  
   - 比如某些 beam 虽然 SFT 概率高，但 correctness 并不比中温采样的组合好，那么 RL 会重新分配概率，更偏向后者。

3. **policy 与 old_policy 的区分**  
   - RL 中你可以固定 ref 模型（带来 old_log_probs），只更新 policy 模型；  
   - 这样可以避免 policy 过度漂移，同时允许在支持域内做较大调整。  

---

## 5. 实施路线图（工程角度）

### 阶段 1：数据生成与清洗

1. 实现 pointer 采样脚本：  
   - 支持 beam + 多温度采样 + 随机组合。  
2. 接上 VQA 模型，计算每条 pointer 的 correctness：  
   - 确保 `vqa_correct` 字段永远存在。  
3. 统一写成 JSONL / JSON 格式，方便后续处理。  
4. 做一轮简单统计：  
   - 每种 `gen_method` 下的平均 correctness；  
   - pointer 中是否有明显偏置（例如总是选某几个 index）。

### 阶段 2：embedding 导出

1. 写一个工具脚本，从 SFT 训练数据中导出 `query_emb` 和 `cand_emb`：  
   - 可从已有 Dataset 中直接复用 encode 逻辑。  
2. 保存为 `embedding_store.pt`：  
   - key 为 `query_id`；  
   - value 为 dict：`{"query_emb": Tensor[d], "cand_emb": Tensor[K, d]}`。  

### 阶段 3：RL Dataset + Reward 处理

1. 实现 `reward_utils.py`：  
   - group 内 z-score / minmax 归一化；  
   - 线性组合 beam_score / logprob_score / correctness；  
   - 最终 reward clip 在 [-5,5]。  

2. 实现 `BeamTrajectoryDataset`（`dataset_v3.py`）：  
   - 从 raw JSON 中读取 pointer_candidates；  
   - 对每个 query 构建一个 `BeamGroup`；  
   - `__getitem__` 返回某个 query-group 的所有 pointer 样本。  

3. 实现 `rl_collate_fn`：  
   - 简化版用 `batch_size=1`，一个 batch 一个 query-group。  
   - 后续可扩展到 `batch_size>1`，通过 `group_ids` 做组内 advantage。

### 阶段 4：RCE + GRPO / REINFORCE++ 训练

1. 在 `pointer_selector_v3.py` 中补全：  
   - `compute_reinforcepp_loss`；  
   - `forward_with_mode(..., mode="reinforcepp")` 分支。  

2. 编写 `grpo_post_train.py`：  
   - 解析 CLI 参数（路径、lr、epochs 等）；  
   - 构建 `BeamTrajectoryDataset` + DataLoader；  
   - 加载 `PointerSelectorV3`（从 v2_lora checkpoint 初始化）；  
   - Phase 1: RCE；  
   - Phase 2: GRPO 或 REINFORCE++。  

3. 编写 `scripts/grpo_post_train.sh`：  
   - 统一运行命令，便于复现。  

### 阶段 5：评估与迭代

1. 用 RL 后的 pointer 模型替换 SFT pointer：  
   - 在验证集上跑一次完整的“选例 + VQA 推理 + 评估”。  

2. 对比指标：  
   - Pointer 选例后的 VQA overall accuracy；  
   - 每个 query 的平均 reward；  
   - Beam vs RL pointer 的分布差异（可统计 pointer 中各 index 被选中的频率变化）。  

3. 根据现象迭代：  
   - 如果 RL 太保守：适当减小 KL penalty，增加温度采样数据占比；  
   - 如果 RL 不稳定：减小学习率、减小 advantage_clip，或增加 RCE epoch 数；  
   - 如果 RL 明显偏向某些“投机组合”：审查 reward 设计，考虑加入多目标约束（例如 diversity penalty）。

---

## 6. 小结

这份计划书扩展了原来的方案，重点解决了三个你关心的问题：

1. **RL 数据是否必须用 beam？**  
   - 回答：不必，但保留 beam 很重要；  
   - 建议采用“beam + 多温度采样 + 少量随机”的混合生成策略。  

2. **如何通过温度采样增加随机性，同时保持可控？**  
   - 使用 Pointer SFT 模型自身的 logits，在 `τ ∈ {0.7, 1.0, 1.3}` 下采样 pointer 序列；  
   - 每种温度采样少量样本即可，避免数据爆炸。  

3. **构建数据集时 correctness 是否必须记录？**  
   - 是的，必须；  
   - 本文所有数据 schema 都默认每条 pointer 样本具有 `vqa_correct` / `vqa_acc_score` 字段，即使当前 reward 不用也要存。  

在此基础上，配合 RCE + GRPO / REINFORCE++ 的 RL pipeline，你可以在保持工程可控的前提下，系统地提升 Pointer Selector 的策略质量，而不是简单重复 SFT 时的行为模式。  