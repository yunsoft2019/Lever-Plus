# GRPO Post-Training 创新方案设计

## 一、项目背景总结

### 1.1 项目概述

**Lever-Plus** 是一个用于视觉问答（VQA）任务的**范例选择系统**，核心思想是：

- 使用**束搜索（Beam Search）**生成多个候选范例序列
- 训练一个**Pointer Selector**模型从候选池中选择最优范例序列
- 通过选择高质量的范例来提升下游VQA任务的准确率

### 1.2 技术演进路径

- **v0**: 基于GPT2的自回归语言模型，通过生成范例索引序列
- **v1**: Bi-Encoder指针网络架构，独立编码query和candidates
- **v2**: v1 + 多层Cross-Attention（3层），增强query与candidates的交互
- **v2_lora**: v2 + LoRA，参数高效微调
- **v3**: v2 + 离线强化学习（RCE预热 + GRPO后训练）

### 1.3 当前进度

- ✅ **束搜索数据生成**：已完成，支持多种采样器（RandSampler, TextSimSampler, ImgSimSampler, MixSampler）
- ✅ **SFT训练**：v0/v1/v2/v2_lora版本已完成训练和推理
- ✅ **v3代码框架**：`pointer_selector_v3.py`已实现RCE和GRPO核心算法
- ⚠️ **v3训练脚本**：缺少完整的post-training训练流程
- ⚠️ **数据加载**：需要从束搜索JSON文件中提取beam和score作为reward

### 1.4 束搜索数据结构

每个样本包含：

- `id_list`: 5个beam，每个beam是一个shot序列（如`[7232, 2229, 8211]`）
- `score_list`: 5个beam对应的分数（如`[0.046, 0.045, 0.037, ...]`）

## 二、创新性GRPO Post-Training方案

### 2.1 核心创新点

#### 创新点1：**多层级奖励设计**

- **Beam级奖励**：直接使用束搜索分数（info score）
- **序列级奖励**：考虑整个序列的连贯性和多样性
- **任务级奖励**：端到端VQA准确率（可选，需要额外评估）

#### 创新点2：**自适应温度调度**

- RCE阶段：温度从高到低（τ: 2.0 → 0.5），逐步聚焦高分beam
- GRPO阶段：根据KL散度动态调整温度，平衡探索与利用

#### 创新点3：**组内相对优势（Group-Relative Advantage）**

- 在每个query的5个beam内计算相对优势
- 避免跨query的奖励分布差异影响训练
- 更稳定的梯度信号

#### 创新点4：**课程学习策略**

- **阶段1（RCE预热）**：使用所有beam，softmax加权
- **阶段2（GRPO早期）**：只使用top-3 beam，减少噪声
- **阶段3（GRPO后期）**：使用所有beam，精细优化

#### 创新点5：**KL散度自适应调整**

- 监控KL散度，如果偏离过大（>0.1），增加kl_beta
- 如果KL过小（<0.01），减少kl_beta，允许更大更新

### 2.2 详细训练流程

#### 阶段1：数据准备

1. **加载束搜索数据**：从JSON文件读取`id_list`和`score_list`
2. **构建训练样本**：

   - Query: 原始query的embedding
   - Candidates: 候选池的embedding
   - Labels: beam中的shot序列
   - Rewards: beam的分数（归一化）
   - Old_log_probs: 从SFT模型计算（冻结参数）

#### 阶段2：RCE预热（1-2 epochs）

- **目标**：稳定地从监督学习过渡到强化学习
- **损失**：`L_RCE = Σ w_i * CE(π_new, labels_i)`，其中`w_i = softmax(score_i / τ)`
- **温度调度**：τ从2.0线性降到0.5
- **学习率**：使用较小的学习率（如SFT的1/10）

#### 阶段3：GRPO训练（2-5 epochs）

- **目标**：最大化高分beam的概率，同时保持策略稳定性
- **损失**：`L_GRPO = L_PPO + β * L_KL`
  - `L_PPO = -E[min(r * A, clip(r, 1-ε, 1+ε) * A)]`
  - `L_KL = E[r - 1 - log(r)]`（近似KL散度）
- **优势计算**：组内相对优势（每个query的5个beam内归一化）
- **KL自适应**：根据KL散度动态调整β

### 2.3 关键技术细节

#### 奖励归一化策略

- **组内Z-score**：在每个query的5个beam内计算均值和标准差
- **优势裁剪**：限制在[-5, 5]范围内，防止极端梯度

#### 稳定性保障

- **梯度裁剪**：max_norm=1.0
- **学习率调度**：余弦退火，warmup=10%
- **检查点保存**：每个epoch保存，保留最佳KL散度的模型

#### 监控指标

- **训练指标**：PPO loss, KL loss, mean ratio, mean advantage
- **验证指标**：Val loss, KL散度, 优势分布
- **下游指标**：VQA准确率（可选，需要额外推理）

### 2.4 实现文件结构

```
lever_lm/
├── models/v3/
│   ├── pointer_selector_v3.py  # ✅ 已实现
│   └── dataset_v3.py           # ⚠️ 需要创建：加载beam数据
├── workflows/
│   └── grpo_post_train.py      # ⚠️ 需要创建：GRPO训练脚本
└── utils/
    └── reward_utils.py         # ⚠️ 需要创建：奖励处理工具

configs/
└── train/lever_lm/v3/
    └── grpo_post_train.yaml    # ⚠️ 需要创建：GRPO训练配置

scripts/
└── grpo_post_train.sh          # ⚠️ 需要创建：训练启动脚本
```

### 2.5 使用流程

```bash
# 1. 完成SFT训练（v2版本）
bash scripts/train_lever_lm.sh vqa okvqa_local 0 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B v2

# 2. 进行GRPO post-training
bash scripts/grpo_post_train.sh \
    --sft_ckpt results/okvqa/model_cpk/v2/xxx.ckpt \
    --beam_data results/okvqa/generated_data/xxx.json \
    --rce_epochs 1 \
    --grpo_epochs 3 \
    --output_dir results/okvqa/model_cpk/v3/
```

## 三、创新性亮点总结

1. **多层级奖励**：结合beam级、序列级、任务级奖励
2. **自适应调度**：温度、KL权重、课程学习三管齐下
3. **组内相对优势**：更稳定的梯度信号
4. **稳定性保障**：多重机制防止训练崩溃
5. **端到端优化**：可选的任务级奖励，直接优化VQA准确率

