# V3 改进方案

## 问题诊断

根据实验结果，v3 效果不好的核心原因：

| 问题 | 旧方案 (legacy) | 新方案 (hard_plus_soft) |
|------|----------------|------------------------|
| Reward 类型 | InfoScore（连续、有区分度） | correctness（稀疏、边界模糊） |
| 正负样本区分 | 自然区分（增益可正可负） | 边界模糊（正样本≥1，负样本<1） |
| shot_num=1,2 | **效果好** | 效果差 |
| shot_num=3,4 | 效果一般 | **略有提升** |

**核心问题**：新方案的 reward 设计导致正负样本区分度不够。

---

## 改进方案 1：回归 Legacy + 微调（推荐，最快见效）

直接使用旧方案，因为它在 shot_num≤2 时效果更好。

```bash
# 训练命令
export REWARD_MODE=legacy
export REWARD_ALPHA=0.0  # 不使用 beam_score
export REWARD_BETA=1.0   # 只使用 correctness（但通过 InfoScore 计算）
bash scripts/train_v3.sh vqa okvqa_local 0 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B
```

**预期效果**：恢复到 v3_1layer 的水平，shot_num=1 时比 v2 高 1-3%。

---

## 改进方案 2：增大正负样本差距

修改 reward 公式，让正负样本的差距更大。

### 方案 2.1：乘法加成

```python
# 原方案：reward = hard + soft，范围 [0, 2]
# 新方案：reward = (1 + 2*hard) * (0.5 + soft)
# 正样本：(1+2) * (0.5+soft) = 3 * (0.5~1.5) = 1.5~4.5
# 负样本：(1+0) * (0.5+soft) = 1 * (0.5~1.5) = 0.5~1.5
```

### 方案 2.2：指数加成

```python
# reward = soft + exp(hard) - 1
# 正样本：soft + e - 1 ≈ soft + 1.718，范围 [1.718, 2.718]
# 负样本：soft + 1 - 1 = soft，范围 [0, 1]
```

### 方案 2.3：阈值分离（推荐）

```python
# 正样本：reward = 2.0 + soft，范围 [2.0, 3.0]
# 负样本：reward = soft，范围 [0.0, 1.0]
# 这样正负样本之间有明确的 gap（至少 1.0）
def compute_reward_separated(vqa_correct, vqa_acc_score):
    soft = vqa_acc_score if vqa_acc_score is not None else 0.0
    if vqa_correct == 1:
        return 2.0 + soft  # [2.0, 3.0]
    else:
        return soft  # [0.0, 1.0]
```

---

## 改进方案 3：混合 InfoScore 和 Correctness

结合两种信号的优点。

```python
def compute_reward_hybrid(beam_score, vqa_correct, vqa_acc_score, alpha=0.3):
    """
    混合 reward：
    - InfoScore 提供连续的、有区分度的信号
    - Correctness 提供直接的目标信号
    
    Args:
        beam_score: InfoScore（增益）
        vqa_correct: 0/1
        vqa_acc_score: [0, 1]
        alpha: InfoScore 权重（0.3 表示 30% InfoScore + 70% Correctness）
    """
    # InfoScore 部分（归一化到 [0, 1]）
    # 假设 InfoScore 范围是 [-0.1, 0.1]，映射到 [0, 1]
    info_normalized = (beam_score + 0.1) / 0.2
    info_normalized = max(0.0, min(1.0, info_normalized))
    
    # Correctness 部分（使用方案 2.3 的分离设计）
    if vqa_correct == 1:
        correct_reward = 2.0 + vqa_acc_score
    else:
        correct_reward = vqa_acc_score
    
    # 混合
    reward = alpha * info_normalized + (1 - alpha) * correct_reward
    
    return reward
```

---

## 改进方案 4：改进 Advantage 计算

当前问题：排名归一化丢失了 reward 的绝对差异信息。

### 修改 `pointer_selector_v3.py`

```python
def compute_advantage(self, rewards, normalize=True, use_rank=False, min_std=0.5):
    """
    改进的 advantage 计算：
    1. 默认使用 Z-score 归一化（保留绝对差异）
    2. 设置较大的 min_std（0.5），避免 advantage 过大
    3. 只在 reward 差异极小时才使用排名归一化
    """
    batch_size, num_beams = rewards.shape
    
    # 检查 reward 差异是否足够大
    std = rewards.std(dim=-1, keepdim=True)
    use_rank_for_batch = (std < 0.01).squeeze(-1)  # 差异太小时使用排名
    
    # Z-score 归一化
    mean = rewards.mean(dim=-1, keepdim=True)
    std_clamped = torch.clamp(std, min=min_std)
    advantages_zscore = (rewards - mean) / std_clamped
    
    # 排名归一化（备选）
    ranks = rewards.argsort(dim=-1, descending=True).argsort(dim=-1).float()
    advantages_rank = 1.0 - 2.0 * ranks / (num_beams - 1) if num_beams > 1 else torch.zeros_like(ranks)
    
    # 根据 reward 差异选择归一化方式
    advantages = torch.where(
        use_rank_for_batch.unsqueeze(-1),
        advantages_rank,
        advantages_zscore
    )
    
    # 裁剪
    advantages = torch.clamp(advantages, -self.advantage_clip, self.advantage_clip)
    
    return advantages
```

---

## 改进方案 5：条件性训练

只在有正样本的 query 上进行 GRPO 训练。

```python
def compute_grpo_loss_conditional(self, ..., min_positive_ratio=0.1):
    """
    条件性 GRPO：只在有足够正样本的 query 上训练
    """
    # 检查每个 query 的正样本比例
    # 假设 reward > 1.5 为正样本（使用方案 2.3 的设计）
    positive_mask = (beam_rewards > 1.5).float()
    positive_ratio = positive_mask.mean(dim=-1)
    
    # 只在有足够正样本的 query 上训练
    valid_mask = positive_ratio >= min_positive_ratio
    
    if valid_mask.sum() == 0:
        # 没有有效的 query，返回零损失
        return {"loss": torch.tensor(0.0, device=beam_rewards.device)}
    
    # 只计算有效 query 的损失
    # ... 后续计算
```

---

## 实施建议

### 短期（立即可行）

1. **方案 1**：切换回 legacy 模式，验证是否恢复到 v3_1layer 的效果
2. **方案 2.3**：实现阈值分离的 reward 设计

### 中期（需要代码修改）

3. **方案 3**：实现混合 reward
4. **方案 4**：改进 advantage 计算

### 长期（需要实验验证）

5. **方案 5**：条件性训练
6. 针对不同 shot_num 使用不同策略

---

## 代码修改清单

### 1. `lever_lm/utils/reward_utils.py`

添加新的 reward 模式：

```python
elif reward_mode == "separated":
    # 方案 2.3：阈值分离
    hard = float(vqa_correct) if vqa_correct is not None else 0.0
    soft = float(vqa_acc_score) if vqa_acc_score is not None else 0.0
    if hard == 1.0:
        reward = 2.0 + soft  # 正样本 [2.0, 3.0]
    else:
        reward = soft  # 负样本 [0.0, 1.0]

elif reward_mode == "hybrid_v2":
    # 方案 3：混合 InfoScore 和 Correctness
    hard = float(vqa_correct) if vqa_correct is not None else 0.0
    soft = float(vqa_acc_score) if vqa_acc_score is not None else 0.0
    info_score = float(beam_score) if beam_score is not None else 0.0
    
    # InfoScore 归一化
    info_normalized = (info_score + 0.1) / 0.2
    info_normalized = max(0.0, min(1.0, info_normalized))
    
    # Correctness 部分（分离设计）
    if hard == 1.0:
        correct_reward = 2.0 + soft
    else:
        correct_reward = soft
    
    # 混合（alpha 使用 hard_weight）
    alpha = min(1.0, max(0.0, hard_weight))
    reward = alpha * info_normalized + (1.0 - alpha) * correct_reward
```

### 2. `lever_lm/models/v3/pointer_selector_v3.py`

改进 `compute_advantage` 方法（见方案 4）。

### 3. `scripts/train_v3.sh`

添加新的 reward 模式支持：

```bash
# REWARD_MODE 可选值：
# - legacy: 旧方案（InfoScore）
# - hard_plus_soft: 当前新方案
# - separated: 阈值分离（推荐）
# - hybrid_v2: 混合方案
export REWARD_MODE=separated
```

---

## 实验计划

| 实验 | Reward Mode | 预期效果 |
|------|-------------|----------|
| 1 | legacy | 恢复 v3_1layer 效果 |
| 2 | separated | 正负样本区分更清晰 |
| 3 | hybrid_v2 (alpha=0.3) | 结合两种信号优点 |
| 4 | hybrid_v2 (alpha=0.5) | 更多 InfoScore 权重 |

每个实验在 100/200/400 条数据上测试，shot_num=1,2,3,4。
