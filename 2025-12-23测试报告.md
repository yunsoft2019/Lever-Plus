# GRPO 训练问题诊断与新优化方案

> 日期：2025-12-23  
> 项目：Lever-Plus v3 GRPO 训练优化

---

## 一、已尝试方案回顾

### 方案 1：测试 GRPO Epoch 2/3
- **结果**：Epoch 1、2、3 的推理结果几乎相同
- **提升**：无明显提升，部分配置甚至下降

### 方案 2：降低 KL Beta（0.15 → 0.12/0.05）
- **结果**：KL Beta 从 0.15 降到 0.12，推理结果完全相同
- **提升**：0%，范例索引选择 100% 相同

### 方案 3：不冻结 Backbone（全参数训练）
- **结果**：训练参数从 50.1% 提升到 100%
- **提升**：无明显提升

### 方案 4：使用 Separated Reward Mode
- **结果**：正样本 [2,3]，负样本 [0,1]，差距更明显
- **提升**：无明显提升，部分场景下降

---

## 二、核心问题诊断

### 2.1 关键发现：模型策略几乎没有变化

从训练日志可以看到：

| 指标 | Epoch 1 | Epoch 2 | Epoch 3 | 问题 |
|------|---------|---------|---------|------|
| KL | 0.094 | 0.081 | 0.074 | ⚠️ 变化太小（仅 21%） |
| PPO Loss | 0.00034 | 0.00083 | 0.00092 | ⚠️ 太小（< 0.001） |
| Adv Std | 0.0063 | 0.0062 | 0.0062 | ⚠️ 几乎不变 |
| Adv Max | 0.0072 | 0.0090 | 0.0090 | ⚠️ 范围太小 |

**结论**：GRPO 训练几乎没有在学习，模型只是在原地踏步。

### 2.2 根因分析

#### 原因 1：Advantage 范围被压缩

当前使用 **Rank Normalization** 计算 advantage：
```python
ranks = rewards.argsort(dim=-1, descending=True).argsort(dim=-1).float()
advantages = 1.0 - 2.0 * ranks / (num_beams - 1)  # 范围 [-1, 1]
```

**问题**：
- 所有 query 的 advantage 分布都被压缩到 [-1, 1]
- 丢失了原始 reward 的绝对差异信息
- 当正负样本 reward 差距很小时，advantage 也很小
- **实际 Adv Std 只有 0.006**，梯度信号极弱

#### 原因 2：学习率太小

| 参数 | 当前值 | 问题 |
|------|--------|------|
| GRPO LR | 5e-6 | 太小，梯度更新几乎没有效果 |
| RCE LR | 1e-4 | 正常 |

GRPO LR 比 RCE LR 小 20 倍，导致 GRPO 阶段的参数更新量极小。

#### 原因 3：KL 惩罚过强

当前 KL Beta = 0.15，KL 惩罚项在 loss 中占比过大：
```
Total Loss = PPO Loss + 0.15 * KL Loss
           ≈ 0.0009 + 0.15 * 0.08
           ≈ 0.0009 + 0.012
           ≈ 0.013
```

**KL 惩罚占 loss 的 92%**，PPO 信号被淹没。

#### 原因 4：正负样本 Reward 差距不够

| Reward Mode | 正样本范围 | 负样本范围 | Gap |
|-------------|-----------|-----------|-----|
| hard_plus_soft | [1, 2] | [0, 1) | ~1.0 |
| separated | [2, 3] | [0, 1] | ~1.0 |

虽然 separated 模式有明确的 gap，但经过 rank normalization 后，这个 gap 被压缩了。

#### 原因 5：负样本没有梯度信号

当前 reward 设计中：
- 正样本：reward = 1 + acc_score（有梯度）
- 负样本：reward = 0 + acc_score ≈ 0（几乎没有梯度）

负样本的 reward 都接近 0，导致模型无法从负样本中学习。

### 2.3 为什么修改参数后结果相同？

**关键发现**：KL_BETA=0.12 和 0.15 的模型，**范例索引选择 100% 相同**。

原因：
1. 权重差异太小（平均差异 0.00047），不足以改变 argmax 结果
2. 模型已收敛到相似状态，不同参数只是在同一个局部最优附近震荡
3. GRPO 的梯度信号太弱，无法推动模型跳出当前状态

---

## 三、新优化方案

### 方案 5：关闭 Rank Normalization（最推荐）⭐

**原理**：使用 Z-score 归一化代替 rank 归一化，保留原始 reward 的绝对差异。

**修改**：`lever_lm/models/v3/pointer_selector_v3.py`

```python
def compute_advantage(self, rewards, normalize=True, use_rank=False):  # 默认关闭 rank
    if use_rank:
        # 排名归一化（当前方式，会压缩 advantage）
        ranks = rewards.argsort(dim=-1, descending=True).argsort(dim=-1).float()
        advantages = 1.0 - 2.0 * ranks / (num_beams - 1)
    else:
        # Z-score 归一化（推荐，保留原始差异）
        mean = rewards.mean(dim=-1, keepdim=True)
        std = rewards.std(dim=-1, keepdim=True)
        std = torch.clamp(std, min=0.1)  # 避免除零，同时保证最小方差
        advantages = (rewards - mean) / std
    
    advantages = torch.clamp(advantages, -self.advantage_clip, self.advantage_clip)
    return advantages
```

**训练命令**：
```bash
export USE_RANK_ADVANTAGE=false
export GRPO_EPOCHS=3
export KL_BETA=0.1
bash scripts/train_v3.sh vqa okvqa_local 0 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B
```

**预期效果**：Adv Std 从 0.006 提升到 0.5-1.0，梯度信号增强 100 倍。

---

### 方案 6：增大 GRPO 学习率

**原理**：当前 GRPO LR = 5e-6 太小，需要增大以产生有效的参数更新。

**训练命令**：
```bash
export GRPO_LR=5e-5   # 从 5e-6 提升 10 倍
export KL_BETA=0.15   # 保持 KL 约束
export GRPO_EPOCHS=3
bash scripts/train_v3.sh ...
```

**风险**：学习率过大可能导致训练不稳定，建议配合 KL 约束使用。

---

### 方案 7：使用 PPO Clip 代替 KL 惩罚

**原理**：PPO 的 clip 机制比 KL 惩罚更适合这种场景，允许更大的策略更新。

**修改**：`lever_lm/models/v3/pointer_selector_v3.py`

```python
def compute_grpo_loss(self, ..., clip_ratio=0.2, use_kl_penalty=False):
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    # PPO clip（推荐）
    clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)
    ppo_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()
    
    if use_kl_penalty:
        # 可选：保留小的 KL 惩罚
        kl_loss = 0.01 * kl  # 大幅减小 KL 权重
        total_loss = ppo_loss + kl_loss
    else:
        total_loss = ppo_loss
    
    return total_loss
```

**训练命令**：
```bash
export USE_PPO_CLIP=true
export PPO_CLIP_RATIO=0.2
export KL_BETA=0.01  # 大幅减小或设为 0
bash scripts/train_v3.sh ...
```

---

### 方案 8：Reward Shaping（负样本也有梯度）

**原理**：使用 relevance 给负样本 shaping，让负样本也有梯度信号。

**训练命令**：
```bash
export REWARD_MODE=hard_plus_gtprob_plus_rel
export HARD_WEIGHT=1.0
export SOFT_WEIGHT=0.5
export REL_WEIGHT=0.3  # 负样本也有 0-0.3 的 reward
bash scripts/train_v3.sh ...
```

**Reward 计算**：
```python
# 正样本：reward = 1.0 + gt_prob + 0（rel 只在负样本上使用）
# 负样本：reward = 0.0 + gt_prob + 0.3 * rel_score
# 这样负样本也有 0-0.3 的 reward，可以产生梯度
```

---

### 方案 9：Curriculum Learning（从简单到难）

**原理**：先在高质量数据上训练，再逐步加入难样本。

**训练命令**：
```bash
# 第一阶段：只用有正样本的 query（约 55% 的数据）
export REQUIRE_POSITIVE_QUERY=true
export GRPO_EPOCHS=2
bash scripts/train_v3.sh ...

# 第二阶段：用全部数据继续训练
export REQUIRE_POSITIVE_QUERY=false
export GRPO_EPOCHS=2
# 从第一阶段的 checkpoint 继续
```

---

### 方案 10：专注优化 RCE（放弃 GRPO）

**原理**：如果 GRPO 确实不适合这个任务，可以专注优化 RCE。

**训练命令**：
```bash
export RCE_EPOCHS=10
export GRPO_EPOCHS=0  # 不做 GRPO
export RCE_TEMP_START=1.0  # 从较低温度开始
export RCE_TEMP_END=0.1    # 降到更低温度
export RCE_LR=5e-5         # 稍微降低学习率，训练更久
bash scripts/train_v3.sh ...
```

---

## 四、推荐实验顺序

### 第一优先级：方案 5（关闭 Rank Normalization）

这是最简单且最可能有效的改动，只需修改一行代码。

```bash
# 快速验证
export USE_RANK_ADVANTAGE=false
export GRPO_EPOCHS=3
bash scripts/train_v3.sh vqa okvqa_local 0 query_img_text_icd_img_text rand_sampler qwen2.5_vl_3B
```

### 第二优先级：方案 5 + 方案 6（组合）

如果方案 5 有效但提升不够，增大学习率。

```bash
export USE_RANK_ADVANTAGE=false
export GRPO_LR=5e-5
export KL_BETA=0.1
export GRPO_EPOCHS=5
bash scripts/train_v3.sh ...
```

### 第三优先级：方案 5 + 方案 8（组合）

如果负样本梯度仍然不足，加入 reward shaping。

```bash
export USE_RANK_ADVANTAGE=false
export GRPO_LR=5e-5
export REWARD_MODE=hard_plus_gtprob_plus_rel
export REL_WEIGHT=0.2
export GRPO_EPOCHS=5
bash scripts/train_v3.sh ...
```

### 备选：方案 10（放弃 GRPO）

如果以上方案都无效，说明 GRPO 可能不适合 pointer selector 任务，专注优化 RCE。

---

## 五、预期效果

| 方案 | 预期 Adv Std | 预期 PPO Loss | 预期提升 |
|------|-------------|---------------|---------|
| 当前 | 0.006 | 0.0009 | 0% |
| 方案 5 | 0.5-1.0 | 0.01-0.1 | 1-3% |
| 方案 5+6 | 0.5-1.0 | 0.05-0.2 | 2-5% |
| 方案 5+6+8 | 0.5-1.0 | 0.05-0.2 | 3-5% |

---

## 六、总结

### 为什么之前的方案都没有效果？

1. **Advantage 被压缩**：Rank normalization 将所有 advantage 压缩到极小范围
2. **学习率太小**：GRPO LR = 5e-6 导致参数更新量极小
3. **KL 惩罚过强**：KL 惩罚占 loss 的 92%，PPO 信号被淹没
4. **负样本无梯度**：负样本 reward ≈ 0，无法学习

### 新方案的核心思路

1. **增强梯度信号**：关闭 rank normalization，保留原始 reward 差异
2. **增大更新幅度**：提高学习率，减小 KL 惩罚
3. **负样本 shaping**：让负样本也有梯度信号
4. **渐进式训练**：从简单数据开始，逐步增加难度

---

*报告生成时间：2025-12-23*
