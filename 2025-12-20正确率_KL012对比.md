# Lever-Plus v3 GRPO 正确率对比报告（KL_BETA=0.12 vs 0.15）

## 测试配置

- **数据集**: OKVQA
- **推理模型**: Qwen2.5-VL-3B-Instruct
- **采样器**: RandSampler
- **测试日期**: 2025-12-21
- **对比模型**:
  - **KL_BETA=0.12**: GRPO epoch 1, 2, 3（本次训练，⚠️ 使用旧数据）
  - **KL_BETA=0.15**: GRPO epoch 1, 2（参考：2025-12-20正确率.md，使用统一数据）

---

## ⚠️ 重要说明：数据文件不一致

**当前对比结果使用的是不同的数据文件**，可能影响对比的公平性：

| 配置 | 数据文件 | 数据质量 |
|------|---------|---------|
| **KL_BETA=0.12** | `rl_data_RandSampler_Qwen2_5-VL-3B-Instruct.json` | ⚠️ 较低（0.6% query 有正样本） |
| **KL_BETA=0.15** | `rl_data_k64_v3.json` | ✅ 较高（83% query 有正样本） |

**数据质量差异**：
- 旧数据：平均 reward=0.0020，只有 5/800 query 有正样本
- 统一数据：平均 reward=1.0264，有 664/800 query 有正样本

**建议**：为了公平对比，应使用统一数据（`rl_data_k64_v3.json`）重新训练 KL_BETA=0.12 和 0.18 的模型。所有训练脚本已更新为使用统一数据。

---

## 训练配置对比

### KL_BETA=0.12 训练配置

| 参数 | 值 |
|------|-----|
| 基础 checkpoint | RCE epoch 5 |
| GRPO epochs | 3 |
| GRPO LR | 5e-6 |
| **KL beta** | **0.12** |
| 冻结 backbone | 是（只训练 50.1% 参数）|
| reward_mode | hard_plus_soft |
| RL 数据 | `rl_data_RandSampler_Qwen2_5-VL-3B-Instruct.json` ⚠️（旧数据，质量较低） |

### KL_BETA=0.15 训练配置（参考）

| 参数 | 值 |
|------|-----|
| 基础 checkpoint | RCE epoch 2 (val_loss=7.8863) |
| GRPO epochs | 3 |
| GRPO LR | 5e-6 |
| **KL beta** | **0.15** |
| 冻结 backbone | 是（只训练 50.1% 参数）|
| reward_mode | hard_plus_soft |
| RL 数据 | rl_data_k64_v3.json (800 queries, 53.3% 正样本) |

---

## 训练过程对比

### KL_BETA=0.12 训练指标

| Epoch | Train Loss | Val Loss | PPO Loss | KL | Adv Std | Adv Max | Beta |
|-------|------------|----------|----------|-----|---------|---------|------|
| **1** | 0.01162 | 5.11661 | 0.00034 | **0.09400** | 0.0063 | 0.0072 | 0.12 |
| **2** | 0.01059 | 5.11663 | 0.00083 | **0.08131** | 0.0062 | 0.0090 | 0.12 |
| **3** | 0.00982 | 5.11607 | 0.00092 | **0.07414** | 0.0062 | 0.0090 | 0.12 |

**关键观察**：
- KL 值从 0.094 逐渐降低到 0.074（KL_BETA=0.12 允许更大的策略更新）
- PPO Loss 逐渐增加（0.00034 → 0.00092），说明模型在学习
- Advantage 范围稳定（0.006-0.009）

### KL_BETA=0.15 训练指标（参考文档未提供详细指标）

参考文档中未提供详细的训练过程指标，只有最终推理结果。

---

## GRPO Epoch 1 推理结果对比

### KL_BETA=0.12 推理结果（使用旧数据训练）

**注意**：此结果使用的是旧数据文件 `rl_data_RandSampler_Qwen2_5-VL-3B-Instruct.json`（数据质量较低，只有 0.6% query 有正样本）

| 样本数 | Shot 1 | Shot 2 | Shot 3 | Shot 4 |
|--------|--------|--------|--------|--------|
| **100** | ? | ? | ? | ? |
| **200** | **57.0%** | **56.1%** | **55.1%** | **55.2%** |
| **400** | ? | ? | ? | ? |
| **800** | ? | ? | ? | ? |

### KL_BETA=0.15 推理结果（参考）

| 样本数 | Shot 1 | Shot 2 | Shot 3 | Shot 4 |
|--------|--------|--------|--------|--------|
| **100** | 63.2% | 65.8% | 60.8% | 61.8% |
| **200** | 58.8% | 56.6% | 53.9% | 55.2% |
| **400** | 53.0% | 51.0% | 50.55% | 50.95% |
| **800** | 49.75% | 47.8% | 47.15% | 47.83% |

### Epoch 1 差异对比

| 样本数 | Shot | KL_BETA=0.15 | KL_BETA=0.12 | 差异 | 评价 |
|--------|------|--------------|--------------|------|------|
| **100** | 1 | 63.2% | ? | ? | ? |
| **100** | 2 | 65.8% | ? | ? | ? |
| **100** | 3 | 60.8% | ? | ? | ? |
| **100** | 4 | 61.8% | ? | ? | ? |
| **200** | 1 | 58.8% | **57.0%** | **-1.8%** | ⬇️ |
| **200** | 2 | 56.6% | **56.1%** | **-0.5%** | ⬇️ |
| **200** | 3 | 53.9% | **55.1%** | **+1.2%** | ⬆️ |
| **200** | 4 | 55.2% | **55.2%** | **0.0%** | ➡️ |

### Epoch 2 差异对比

| 样本数 | Shot | KL_BETA=0.15 | KL_BETA=0.12 | 差异 | 评价 |
|--------|------|--------------|--------------|------|------|
| **200** | 1 | ? | **57.0%** | ? | ? |
| **200** | 2 | ? | **56.1%** | ? | ? |
| **200** | 3 | ? | **55.1%** | ? | ? |
| **200** | 4 | ? | **55.2%** | ? | ? |

**⚠️ 重要说明**：
1. KL_BETA=0.12 的结果使用的是旧数据文件（数据质量较低），与 KL_BETA=0.15 使用的数据文件不同，对比可能不够公平。建议使用统一数据重新训练后再对比。
2. **Epoch 1 和 Epoch 2 的结果完全相同**（都是 57.0%, 56.1%, 55.1%, 55.2%），说明训练 2 个 epochs 后，KL_BETA=0.12 和 0.18 的模型策略分布几乎相同（KL 值差异仅 2.45%）。
| **400** | 1 | 53.0% | ? | ? | ? |
| **400** | 2 | 51.0% | ? | ? | ? |
| **400** | 3 | 50.55% | ? | ? | ? |
| **400** | 4 | 50.95% | ? | ? | ? |
| **800** | 1 | 49.75% | ? | ? | ? |
| **800** | 2 | 47.8% | ? | ? | ? |
| **800** | 3 | 47.15% | ? | ? | ? |
| **800** | 4 | 47.83% | ? | ? | ? |

---

## GRPO Epoch 2 推理结果对比

### KL_BETA=0.12 推理结果

| 样本数 | Shot 1 | Shot 2 | Shot 3 | Shot 4 |
|--------|--------|--------|--------|--------|
| **100** | ? | ? | ? | ? |
| **200** | ? | ? | ? | ? |
| **400** | ? | ? | ? | ? |
| **800** | ? | ? | ? | ? |

### KL_BETA=0.15 推理结果（参考）

| 样本数 | Shot 1 | Shot 2 | Shot 3 | Shot 4 |
|--------|--------|--------|--------|--------|
| **100** | 64.2% | 63.8% | 60.8% | 61.8% |
| **200** | 58.8% | 56.6% | 53.9% | 55.2% |
| **400** | 54.1% | 51.6% | 50.55% | 50.95% |
| **800** | 49.62% | 48.33% | 47.15% | 47.83% |

### Epoch 2 差异对比

| 样本数 | Shot | KL_BETA=0.15 | KL_BETA=0.12 | 差异 | 评价 |
|--------|------|--------------|--------------|------|------|
| **100** | 1 | 64.2% | ? | ? | ? |
| **100** | 2 | 63.8% | ? | ? | ? |
| **100** | 3 | 60.8% | ? | ? | ? |
| **100** | 4 | 61.8% | ? | ? | ? |
| **200** | 1 | 58.8% | ? | ? | ? |
| **200** | 2 | 56.6% | ? | ? | ? |
| **200** | 3 | 53.9% | ? | ? | ? |
| **200** | 4 | 55.2% | ? | ? | ? |
| **400** | 1 | 54.1% | ? | ? | ? |
| **400** | 2 | 51.6% | ? | ? | ? |
| **400** | 3 | 50.55% | ? | ? | ? |
| **400** | 4 | 50.95% | ? | ? | ? |
| **800** | 1 | 49.62% | ? | ? | ? |
| **800** | 2 | 48.33% | ? | ? | ? |
| **800** | 3 | 47.15% | ? | ? | ? |
| **800** | 4 | 47.83% | ? | ? | ? |

---

## GRPO Epoch 3 推理结果（KL_BETA=0.12 独有）

### KL_BETA=0.12 Epoch 3 推理结果

| 样本数 | Shot 1 | Shot 2 | Shot 3 | Shot 4 |
|--------|--------|--------|--------|--------|
| **100** | ? | ? | ? | ? |
| **200** | ? | ? | ? | ? |
| **400** | ? | ? | ? | ? |
| **800** | ? | ? | ? | ? |

**注意**：参考配置（KL_BETA=0.15）只训练到 Epoch 2，因此 Epoch 3 是 KL_BETA=0.12 的额外实验。

---

## 训练过程分析

### KL 值变化趋势

**KL_BETA=0.12**：
- Epoch 1: KL = 0.09400
- Epoch 2: KL = 0.08131（↓ 13.5%）
- Epoch 3: KL = 0.07414（↓ 8.8%）

**分析**：
- KL 值持续下降，说明模型在逐渐偏离初始策略
- KL_BETA=0.12 比 0.15 更小，允许更大的策略更新
- KL 值在合理范围内（< 0.1），训练稳定

### PPO Loss 变化趋势

**KL_BETA=0.12**：
- Epoch 1: PPO Loss = 0.00034
- Epoch 2: PPO Loss = 0.00083（↑ 144%）
- Epoch 3: PPO Loss = 0.00092（↑ 11%）

**分析**：
- PPO Loss 逐渐增加，说明模型在学习新的策略
- Loss 值很小（< 0.001），训练稳定
- 与 KL 值下降趋势一致，说明模型在优化策略

---

## 统计汇总（待填充）

### KL_BETA=0.12 vs 0.15 整体对比

| 指标 | Epoch 1 | Epoch 2 |
|------|---------|---------|
| 平均差异 | ? | ? |
| 最大提升 | ? | ? |
| 最大下降 | ? | ? |
| 提升的配置数 | ?/16 | ?/16 |
| 下降的配置数 | ?/16 | ?/16 |

### 按 Shot 分析

| Shot | Epoch 1 平均差异 | Epoch 2 平均差异 | 趋势 |
|------|-----------------|-----------------|------|
| Shot 1 | ? | ? | ? |
| Shot 2 | ? | ? | ? |
| Shot 3 | ? | ? | ? |
| Shot 4 | ? | ? | ? |

### 按样本量分析

| 样本量 | Epoch 1 平均差异 | Epoch 2 平均差异 | 趋势 |
|--------|-----------------|-----------------|------|
| 100 | ? | ? | ? |
| 200 | ? | ? | ? |
| 400 | ? | ? | ? |
| 800 | ? | ? | ? |

---

## 关键发现（待填充）

### ✅ KL_BETA=0.12 的优势

（待填充）

### ⚠️ KL_BETA=0.12 的劣势

（待填充）

---

## 结论（待填充）

### KL_BETA 选择建议

（待填充）

### 最佳模型推荐

（待填充）

---

## 使用说明

### 1. 运行评估

```bash
# 评估 GRPO epoch 1
bash scripts/eval_grpo_kl012.sh 1 0 100  # GPU 0, 100 样本
bash scripts/eval_grpo_kl012.sh 1 0 200  # GPU 0, 200 样本
bash scripts/eval_grpo_kl012.sh 1 1 400  # GPU 1, 400 样本
bash scripts/eval_grpo_kl012.sh 1 1 800  # GPU 1, 800 样本

# 评估 GRPO epoch 2
bash scripts/eval_grpo_kl012.sh 2 0 100
bash scripts/eval_grpo_kl012.sh 2 0 200
bash scripts/eval_grpo_kl012.sh 2 1 400
bash scripts/eval_grpo_kl012.sh 2 1 800

# 评估 GRPO epoch 3（KL_BETA=0.12 独有）
bash scripts/eval_grpo_kl012.sh 3 0 100
bash scripts/eval_grpo_kl012.sh 3 0 200
bash scripts/eval_grpo_kl012.sh 3 1 400
bash scripts/eval_grpo_kl012.sh 3 1 800

# 或使用完整评估脚本（自动评估所有 epoch 和样本数）
bash scripts/eval_grpo_kl012_all.sh
```

### 2. 填写对比结果

评估完成后，将推理结果填入上述表格，并更新统计汇总和结论部分。
