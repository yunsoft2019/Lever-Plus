# 改进 3.4 和 3.5.2 实现总结

> 实现时间：2025-12-10  
> 基于：`LeverPlus_v3_RL_next_steps_2025_12_10.md`  
> 方案：方案A（不需要重新生成 RL 数据）

---

## ✅ 3.4 增加 `rce_use_raw_reward` 开关

### 修改内容

#### 1. 添加 CLI 参数（`grpo_post_train.py` 第486-489行）

**新增参数：**
```python
# 3.4: RCE 使用 raw reward 开关
parser.add_argument("--rce_use_raw_reward", action="store_true",
                    help="RCE 训练时使用原始 reward（默认True，即使用beam_rewards_raw；不传此参数则默认使用raw reward）")
parser.add_argument("--rce_use_normalized_reward", action="store_true",
                    help="RCE 训练时使用归一化后的 reward（与--rce_use_raw_reward互斥，优先级更高）")
```

**设计说明：**
- 默认行为：使用 `beam_rewards_raw`（保持当前行为）
- `--rce_use_raw_reward`：显式指定使用 raw reward（可选，默认就是 raw）
- `--rce_use_normalized_reward`：切换到归一化后的 reward（用于对比实验）

#### 2. 在 GRPOTrainer 中添加属性（`grpo_post_train.py` 第83行）

**新增：**
```python
self.rce_use_raw_reward = True  # 默认使用 raw reward，将在 main() 中设置
```

#### 3. 在 `train_rce_epoch` 中实现开关逻辑（`grpo_post_train.py` 第171-178行）

**修改前：**
```python
beam_rewards_raw = batch["beam_rewards_raw"].to(self.device)

loss = self.model.compute_rce_loss(
    query_emb, cand_emb, beam_labels, beam_rewards_raw,
    temperature=temperature,
    use_top1_only=self.use_top1_only
)
```

**修改后：**
```python
beam_rewards_raw = batch["beam_rewards_raw"].to(self.device)
beam_rewards = batch.get("beam_rewards", beam_rewards_raw).to(self.device)

# 3.4: 根据开关选择使用 raw reward 还是归一化后的 reward
if self.rce_use_raw_reward:
    reward_for_rce = beam_rewards_raw
else:
    reward_for_rce = beam_rewards

loss = self.model.compute_rce_loss(
    query_emb, cand_emb, beam_labels, reward_for_rce,
    temperature=temperature,
    use_top1_only=self.use_top1_only
)
```

#### 4. 在 main() 中设置参数（`grpo_post_train.py` 第810-820行）

**新增：**
```python
# 3.4: 设置 RCE 使用 raw reward 开关
# 默认使用 raw reward（当前行为），除非明确指定使用归一化后的 reward
if args.rce_use_normalized_reward:
    trainer.rce_use_raw_reward = False
    print("✓ RCE 训练将使用归一化后的 reward (beam_rewards)")
else:
    trainer.rce_use_raw_reward = True  # 默认使用 raw reward
    if args.rce_use_raw_reward:
        print("✓ RCE 训练将使用原始 reward (beam_rewards_raw) [显式指定]")
    else:
        print("✓ RCE 训练将使用原始 reward (beam_rewards_raw) [默认]")
```

### 效果

- ✅ **保持默认行为**：默认使用 `beam_rewards_raw`，不影响现有实验
- ✅ **支持对比实验**：可以通过 `--rce_use_normalized_reward` 切换到归一化后的 reward
- ✅ **便于调试**：明确显示使用的是哪种 reward

---

## ✅ 3.5.2 支持冻结 backbone

### 修改内容

#### 1. 添加 CLI 参数（`grpo_post_train.py` 第490-492行）

**新增参数：**
```python
# 3.5.2: GRPO 时冻结 backbone
parser.add_argument("--freeze_backbone_in_grpo", action="store_true",
                    help="GRPO 时只训练 pointer head（cross_attn_layers + attn_norms），冻结投影层（input_proj, query_proj, cand_proj）")
```

#### 2. 在 GRPOTrainer 中添加属性（`grpo_post_train.py` 第88行）

**新增：**
```python
self.freeze_backbone_in_grpo = False  # 默认不冻结，将在 main() 中设置
```

#### 3. 在 GRPO 训练前实现冻结逻辑（`grpo_post_train.py` 第435-460行）

**新增代码：**
```python
# 3.5.2: 如果开启冻结 backbone，只训练 pointer head
if self.freeze_backbone_in_grpo:
    print("⚠️  冻结 backbone，GRPO 时只训练 pointer head（cross_attn_layers + attn_norms）")
    print("   冻结的层：input_proj, query_proj, cand_proj")
    
    # 冻结投影层（backbone）
    for name, param in self.model.named_parameters():
        if any(x in name for x in ["input_proj", "query_proj", "cand_proj"]):
            param.requires_grad = False
        else:
            param.requires_grad = True
    
    # 只优化需要训练的参数
    trainable_params = [p for p in self.model.parameters() if p.requires_grad]
    trainable_count = sum(p.numel() for p in trainable_params)
    total_count = sum(p.numel() for p in self.model.parameters())
    print(f"   可训练参数: {trainable_count:,} / {total_count:,} ({100*trainable_count/total_count:.1f}%)")
else:
    # 确保所有参数都可训练
    for param in self.model.parameters():
        param.requires_grad = True

# 只优化需要训练的参数
if self.freeze_backbone_in_grpo:
    trainable_params = [p for p in self.model.parameters() if p.requires_grad]
    grpo_optimizer = AdamW(trainable_params, lr=self.grpo_lr)
else:
    grpo_optimizer = AdamW(self.model.parameters(), lr=self.grpo_lr)
```

#### 4. 在 main() 中设置参数（`grpo_post_train.py` 第822行）

**新增：**
```python
# 3.5.2: 设置 GRPO 冻结 backbone 开关
trainer.freeze_backbone_in_grpo = args.freeze_backbone_in_grpo
```

### 效果

- ✅ **降低 GRPO 的"破坏力"**：只训练 pointer head，保护 RCE-only 阶段学到的表示
- ✅ **减少可训练参数**：只训练 cross-attention 层和 normalization 层
- ✅ **保护 RCE 成果**：避免 GRPO 阶段破坏 RCE-only 的良好性能

---

## 测试建议

### 1. 测试 3.4（RCE reward 开关）

```bash
# 测试默认行为（使用 raw reward）
python -m lever_lm.workflows.grpo_post_train \
    --beam_data ./results/okvqa/generated_data/rl_data_RandSampler_Qwen2_5-VL-3B-Instruct.json \
    --img_emb ./results/okvqa/cache/query_embeddings.pt \
    --sft_ckpt ./results/okvqa/model_cpk/v2/xxx.ckpt \
    --output_dir ./results/okvqa/model_cpk/v3_test \
    --device cuda:0

# 应该看到：
# ✓ RCE 训练将使用原始 reward (beam_rewards_raw) [默认]

# 测试使用归一化后的 reward
python -m lever_lm.workflows.grpo_post_train \
    --beam_data ./results/okvqa/generated_data/rl_data_RandSampler_Qwen2_5-VL-3B-Instruct.json \
    --img_emb ./results/okvqa/cache/query_embeddings.pt \
    --sft_ckpt ./results/okvqa/model_cpk/v2/xxx.ckpt \
    --output_dir ./results/okvqa/model_cpk/v3_test_normalized \
    --rce_use_normalized_reward \
    --device cuda:0

# 应该看到：
# ✓ RCE 训练将使用归一化后的 reward (beam_rewards)
```

### 2. 测试 3.5.2（冻结 backbone）

```bash
# 测试 GRPO 时冻结 backbone
python -m lever_lm.workflows.grpo_post_train \
    --beam_data ./results/okvqa/generated_data/rl_data_RandSampler_Qwen2_5-VL-3B-Instruct.json \
    --img_emb ./results/okvqa/cache/query_embeddings.pt \
    --sft_ckpt ./results/okvqa/model_cpk/v2/xxx.ckpt \
    --output_dir ./results/okvqa/model_cpk/v3_test_freeze \
    --grpo_epochs 3 \
    --freeze_backbone_in_grpo \
    --device cuda:0

# 应该看到：
# ⚠️  冻结 backbone，GRPO 时只训练 pointer head（cross_attn_layers + attn_norms）
#    冻结的层：input_proj, query_proj, cand_proj
#    可训练参数: XXX / XXX (XX.X%)
```

---

## 修改的文件

1. ✅ `lever_lm/workflows/grpo_post_train.py`
   - 添加 CLI 参数（第486-492行）
   - 在 GRPOTrainer 中添加属性（第83、88行）
   - 在 `train_rce_epoch` 中实现 reward 选择逻辑（第171-178行）
   - 在 `train()` 中实现冻结 backbone 逻辑（第435-460行）
   - 在 main() 中设置参数（第810-822行）

---

## 下一步

- ✅ **已完成**：3.4 和 3.5.2
- ⏭️ **可选**：3.3（Reward 质量标记）- 需要重新生成 RL 数据
- ⏭️ **实验建议**：
  - 对比 `--rce_use_normalized_reward` vs 默认 raw reward 的效果
  - 测试 `--freeze_backbone_in_grpo` 是否能保护 RCE-only 的成果

---

**修改完成时间：** 2025-12-10





