# Lever-Plus：RL 数据重新生成（Reward 与最终评测严格一致）+ 一次生成可复用的“全量保存”方案

> 依据你提供的最新代码（zip：`Lever-Plus-main (5).zip`），我重点检查了：
>
> - RL 数据生成：`lever_lm/models/v3/generate_rl_data.py`
> - Reward 组合：`lever_lm/utils/reward_utils.py`
> - RL 数据集加载：`lever_lm/models/v3/dataset_v3.py`
> - 最终评测 / 推理：`icl_inference.py`、`lever_lm/workflows/evaluate_v3.py`、`configs/task/vqa.yaml`

---

## 0. 你现在要达成的目标（我按“可验收”的标准重新定义）

你说的“**RL 数据生成阶段的正确与否、正确概率等，要和最终评估完全一致**”，我把它拆成 3 个可检验条件：

1. **同一条 (ICDs, query) 输入**，VQA 模型（千问）生成答案时用的：
   - prompt 构造方法
   - `gen_args`（例如 `num_beams`, `max_new_tokens`…）
   - postprocess 规则  
     必须和最终测试/评估时保持一致。

2. **正确率/得分计算**（`vqa_acc_score`）必须和最终评估完全同一套：
   - 同一个 VQA 官方评测实现（VQAEval）
   - 同一套答案归一化（normalization）
   - 同一份 questions/annotations 文件（train 用 train，val 用 val）
   - **禁止“字符串包含/部分匹配”这类 fallback**（会导致 RL reward 与最终评测不一致）

3. “**正确概率**”（你现在想要的软信号）必须定义清楚，并且：
   - 生成时就算出来并保存（以后无需重新跑千问）
   - 计算口径和最终评测的 ground-truth 集合/权重一致  

你现在代码里已经有一个非常合适的定义：`vqa_gt_prob`（Expected VQA Accuracy Probability）  
= `Σ_{unique_gt_answer a} P_model(a | ICDs, query) * w(a)`，其中 `w(a)=min(cnt(a)/3, 1)`。

---

## 1. 现状审计：哪些已经一致？哪些还差最后一公里？

### 1.1 已经做对的地方（✅保持）

**(A) 后处理一致：**

- RL 数据生成：`utils.vqa_postprocess`（内部调用 `open_mmicl.metrics.vqa_metrics.postprocess_vqa_generation`）
- 最终推理：`icl_inference.py` 同样调用 `vqa_postprocess`  
  => 这块基本一致。

**(B) 评测实现一致：**

- 最终评测：通过 `open_mmicl.metrics.vqa_metrics.compute_vqa_accuracy` 跑 VQAEval
- RL 数据生成：`generate_rl_data.py` 里用 `VQAEval` 对单个 question 计算 `acc_score`  
  => 本质同源（同一个 VQA 官方评测逻辑），只要用对 split 文件，就是一致的。

**(C) 你已经加入“正确概率”：**

- `generate_rl_data.py` 已实现 `compute_expected_vqa_acc_prob(...) -> vqa_gt_prob`
- `reward_utils.py` 已支持 `reward_mode="hard_plus_gtprob"` 等  
  => 方向正确。

**(D) 训练阶段做了“fallback reward 过滤”：**

- `dataset_v3.RLBeamDatasetWithEmbedding(skip_fallback_reward=True)`  
  => 能避免“非官方评测”污染训练。

---

### 1.2 仍然会导致“不一致/数据不可复用”的问题（❗必须修）

#### 问题 1：RL 数据里没有保存 **raw response / prompt / gt answers**，导致后续想换 reward 必须重跑千问

目前每个 candidate 只保存了 `vqa_pred_answer`（postprocess 后），但你现在想要：

- 保存 raw generation（postprocess 前）
- 保存 prompt（可选，但强烈建议）
- 保存 ground-truth answers（原始+归一化）
- 保存相关性指标（relevance）  
  否则你后面一改 reward 或想做 error 分析，还得重跑千问。

✅ **修复：在 RL 数据 JSON 里补齐“可复用字段”。**

---

#### 问题 2：候选索引体系（pointer / candidate_indices）容易混淆，当前实现对“非连续 candidate_indices”不鲁棒

在 `generate_rl_data.py` 中：

- pointer_candidates 里的 `pointer` 是 **candidate pool 的 position（0..K-1）**

- 你又做了 `original_pointer = [candidate_indices[p] for p in pointer]`（映射回全局 index）

- 但你取示例时用的是：

  ```python
  ex1 = candidate_pool[original_pointer[0]]
  ex2 = candidate_pool[original_pointer[1]]
  ```

  这只有在 `candidate_indices == [0,1,2,...]` 时才不会错。  
  一旦 `candidate_indices` 是“真实全局 id”（常见情况），这里会 **越界或拿错示例**，直接把 RL reward 搞坏。

✅ **修复：**

- 取示例用 `candidate_pool[position]`（position 才是 list 下标）
- 同时在 JSON 里保存：
  - `pointer_pos`（position）
  - `pointer`（global id，供训练/分析复用）

---

#### 问题 3：仍然存在 accuracy 的“简单匹配 fallback”（即使后面过滤，也不如直接禁止）

你现在的实现虽然在生成循环里优先 train，再 val，再 fallback；而训练又能过滤 fallback。  
但你现在要“严格一致”+“一次生成可复用”，我建议：

- **生成阶段直接禁止 fallback**（没有官方评测文件就报错退出）
- RL 数据里只允许 `vqa_eval_mode == "vqaEval"` 的样本写入

否则你以后复用 RL 数据时，很容易混进不一致样本。

✅ **修复：加一个 hard fail 开关，默认开启。**

---

## 2. 新 RL 数据格式（一次生成，后续不再重跑千问）

建议输出 JSON 结构如下（向后兼容：`pointer_candidates` 仍然存在）：

```json
{
  "_meta": {
    "created_at": "2025-12-15",
    "vqa_model": "Qwen/Qwen2.5-VL-3B-Instruct",
    "task_gen_args": {"max_new_tokens": 5, "num_beams": 3, "length_penalty": 0.0},
    "eval": {
      "train_ques_path": "...",
      "train_ann_path": "...",
      "val_ques_path": "...",
      "val_ann_path": "..."
    },
    "candidate_indices": {"type": "list", "value_path": "results/.../candidate_indices.json"},
    "notes": "RL data v4: save raw response + prompt + gt answers + relevance."
  },

  "123": {
    "query": {
      "query_id": 123,
      "question_id": 123456789,
      "image_id": 987654321,
      "question": "What is ...?",
      "gt_answers_raw": ["...", "...", "..."],
      "gt_answers_norm": ["...", "...", "..."]
    },
    "pointer_candidates": [
      {
        "pointer": [7232, 2229],
        "pointer_pos": [3, 17],
        "gen_method": "beam",
        "beam_score": 0.046,
        "logprob_score": -12.34,

        "vqa_prompt_text": "...(可选)...",
        "vqa_raw_generation": "the answer is ...",
        "vqa_pred_answer": "..."

        "vqa_acc_score": 0.6,
        "vqa_correct": 1,
        "vqa_gt_prob": 0.41,

        "vqa_rel_token_f1": 0.5,
        "vqa_rel_edit_sim": 0.62,
        "vqa_rel_score": 0.56,

        "vqa_eval_mode": "vqaEval",
        "eval_split_used": "train",
        "eval_failed": false
      }
    ]
  }
}
```

> 关键点：  
>
> - `gt_answers_*` 存在 query 级别（不重复）  
> - candidate 级别保存 **raw generation + postprocessed answer + 所有 reward 特征**  
> - 同时保存 `pointer_pos` 与 `pointer(global)`，避免索引混淆  
> - `_meta` 可选，但建议加；为了不影响现有 loader，需要 loader 跳过非数字 key

---

## 3. 你想加的“response 与 ground truth 相关性（relevance）”：怎么做最稳？

你现在的核心困境之一是：**正样本太少**，纯 0/1 或纯 acc_score 的稀疏信号很难学；  
relevance 可以作为**负样本上的 shaping**，让模型从“更错误”往“更接近正确”移动。

### 3.1 我推荐的 relevance 定义（不引入额外模型，稳定可复现）

对 `pred = vqa_pred_answer`、`gt_list = gt_answers_norm`：

- `token_f1(pred, gt)`：按空格/数字/字母分词的 token F1，取 max
- `edit_sim(pred, gt)`：字符级编辑相似度（`difflib.SequenceMatcher().ratio()`），取 max
- `vqa_rel_score = max(token_f1_max, edit_sim_max)`（或两者平均）

> 这样做的好处：
>
> - 不依赖额外 embedding 模型（避免引入新的不一致源）
> - 完全由 **保存下来的 pred+gt** 计算，后续可复算验证

### 3.2 relevance 不要“抢主目标”

推荐只在错误样本上加一点 shaping：

```python
reward = w_hard * hard + w_prob * vqa_gt_prob + w_rel * (1 - hard) * vqa_rel_score
```

- hard 正确的样本：不需要 relevance 再加分（避免 reward hacking）
- hard 错误的样本：给一点“接近程度”信号，缓解稀疏

> 你也可以把 `vqa_acc_score` 加进来，但如果 `vqa_acc_score` 本身是由生成结果得到的（离散值），它和 `hard` 可能冗余；  
> 更推荐主干用：`hard + vqa_gt_prob`，再用 relevance 做 shaping。

---

## 4. 需要改哪些代码？怎么改？（精准到文件/位置/伪代码）

下面是“从你当前代码出发”的最小改动清单。

---

### 4.1 `lever_lm/models/v3/generate_rl_data.py`

#### 改动 A：修复 candidate_pool 的索引使用（必须）

**问题：** 目前用 `candidate_pool[original_pointer[i]]`，当 `candidate_indices` 非连续时会错。  
**影响：** 示例选错 => 千问输出错 => `acc_score` 错 => RL reward 全错。  

**修改方式：**

```diff
- original_pointer = [candidate_indices[p] for p in pointer]
- ex1 = candidate_pool[original_pointer[0]]
- ex2 = candidate_pool[original_pointer[1]]
+ pointer_pos = pointer
+ pointer_global = [candidate_indices[p] for p in pointer_pos]
+ ex1 = candidate_pool[pointer_pos[0]]  # list 下标必须是 pos
+ ex2 = candidate_pool[pointer_pos[1]]

# 保存时同时写入
c["pointer_pos"] = pointer_pos
c["pointer"] = pointer_global
```

---

#### 改动 B：build_vqa_prompt_and_generate 返回 “raw + prompt_text + postprocess”

**目的：** 保存输出，后续不再重跑千问。  
**修改方式（伪代码）：**

```python
def build_vqa_prompt_and_generate(..., return_prompt_text=True):
    ...
    raw_generation = generated[0] if generated else ""
    pred_answer = vqa_postprocess(raw_generation, model_name=...)
    prompt_text = interface.concat_prompt(prompts[0]) if return_prompt_text else None

    return {
        "pred_answer": pred_answer,
        "raw_generation": raw_generation,
        "prompt_text": prompt_text,
        "prompt_len": prompt_len,
    }
```

然后写入 candidate：

```python
out = build_vqa_prompt_and_generate(...)
c["vqa_raw_generation"] = out["raw_generation"]
c["vqa_pred_answer"] = out["pred_answer"]
if save_prompts:
    c["vqa_prompt_text"] = out["prompt_text"]
    c["vqa_prompt_len"] = out["prompt_len"]
```

---

#### 改动 C：把 ground truth answers 保存到 query 级别（必须）

**目的：** 以后算 relevance / 复算 acc_score / 复分析，不再依赖外部 dataset。  

```python
gt_answers_raw = query_item.get("answers", [])
gt_answers_norm = [vqa_postprocess(a, model_name=vqa_model_name) for a in gt_answers_raw]

rl_data[query_id_str] = {
    "query": {
        "query_id": query_id,
        "question_id": question_id_str,
        "image_id": query_item.get("image_id", None),
        "question": question,
        "gt_answers_raw": gt_answers_raw,
        "gt_answers_norm": gt_answers_norm,
    },
    "pointer_candidates": pointer_candidates_with_correctness
}
```

---

#### 改动 D：新增 relevance 计算，并保存（必须，按你需求）

**新增函数（建议放在 generate_rl_data.py 内部或单独 util）：**

```python
import re
from difflib import SequenceMatcher

def _tok(s: str):
    return re.findall(r"[a-z0-9]+", s.lower())

def token_f1(a: str, b: str) -> float:
    A, B = _tok(a), _tok(b)
    if len(A)==0 or len(B)==0:
        return 0.0
    from collections import Counter
    ca, cb = Counter(A), Counter(B)
    common = sum((ca & cb).values())
    prec = common / max(1, len(A))
    rec  = common / max(1, len(B))
    if prec + rec == 0:
        return 0.0
    return 2*prec*rec/(prec+rec)

def edit_sim(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()

def compute_relevance(pred: str, gt_list: list[str]) -> dict:
    if pred is None: pred = ""
    pred = pred.strip().lower()
    gt_list = [(g or "").strip().lower() for g in gt_list]
    if pred == "" or len(gt_list) == 0:
        return {"vqa_rel_token_f1": 0.0, "vqa_rel_edit_sim": 0.0, "vqa_rel_score": 0.0}
    f1 = max(token_f1(pred, g) for g in gt_list)
    ed = max(edit_sim(pred, g) for g in gt_list)
    return {"vqa_rel_token_f1": f1, "vqa_rel_edit_sim": ed, "vqa_rel_score": max(f1, ed)}
```

在每个 candidate 上：

```python
rel = compute_relevance(pred_answer, gt_answers_norm)
c.update(rel)
```

---

#### 改动 E：严格一致性模式（禁用 fallback，缺文件就报错）

你现在想“完全一致”，建议添加 `--strict_eval`（默认 True）。  
实现方式：

- 如果 train/val 评测文件都没提供：直接 raise
- 只要 `eval_failed=True` 或 `vqa_eval_mode!="vqaEval"`：这个 candidate 不写入或标记并最终过滤

伪代码：

```python
if strict_eval and (train_ques_path is None or train_ann_path is None) and (val_ques_path is None or val_ann_path is None):
    raise ValueError("strict_eval enabled but no official VQA eval files provided")

...
if strict_eval and eval_failed:
    continue  # 严格模式下不保存
```

---

#### 改动 F：在输出 JSON 顶层写 `_meta`（建议）

并同步修改 dataset loader 跳过非数字 key（见 4.3）。

---

### 4.2 `lever_lm/utils/reward_utils.py`（把 relevance 纳入 reward）

你现在 reward 已支持 `vqa_gt_prob`，但还不支持 relevance。  
建议扩展：

- 新增参数：`vqa_rel_score: Optional[float]`
- 新增 reward_mode：`"hard_plus_gtprob_plus_rel"`

伪代码（核心逻辑）：

```python
elif reward_mode == "hard_plus_gtprob_plus_rel":
    hard = float(vqa_correct or 0.0)
    gtprob = float(vqa_gt_prob or 0.0)
    rel = float(vqa_rel_score or 0.0)
    reward = hard_weight*hard + gt_prob_weight*gtprob + rel_weight*(1-hard)*rel
```

---

### 4.3 `lever_lm/models/v3/dataset_v3.py`（兼容 `_meta` + 传入 relevance）

#### 改动 A：跳过 `_meta`（必须，如果你加 meta）

当前写法：

```python
for query_id_str, query_data in rl_data.items():
    query_id = int(query_id_str)  # 遇到 "_meta" 会崩
```

改成：

```python
for query_id_str, query_data in rl_data.items():
    try:
        query_id = int(query_id_str)
    except Exception:
        continue
```

#### 改动 B：把 `vqa_rel_score` 传给 reward

在 `compute_reward_for_candidate(...)` 调用里新增：

```python
vqa_rel_score=c.get("vqa_rel_score"),
```

---

## 5. 生成新 RL 数据的“标准流程”（建议你按这个顺序跑，边跑边验）

### Step 1：固定最终评测用的千问 gen_args（必须一致）

确保你最终评测使用 `configs/task/vqa.yaml` 的 `gen_args`，并且 RL 数据生成也用同一份。

> 你现在 `generate_rl_data.py` 已经能加载 `configs/task/vqa.yaml` 的 `gen_args`，只要你别手动覆盖成别的即可。

---

### Step 2：开启 strict_eval（不一致直接失败）

生成时要求提供 train/val 的 `questions/annotations` 文件路径（至少 train）。

---

### Step 3：生成 RL 数据（保存 raw/prompt/gt/relevance）

建议输出文件名带上关键信息，避免混淆：

```
results/okvqa/rl_data/
  rl_v4_qwen_beam5_temp1.0-1.3_rand1_shot2_seed0_strictEval_saveAll.json
```

---

### Step 4：一致性自检（强烈建议你加一个 verify 脚本）

因为你已经保存了 `gt_answers_norm` 与 `vqa_pred_answer`，你可以用纯本地公式复算：

```python
acc_local = min(count_eq(pred_norm, gt_norm_list)/3, 1)
assert abs(acc_local - stored_vqa_acc_score) < 1e-6
```

这一步能快速发现：

- postprocess 不一致
- question_id/ann file 用错 split
- 保存字段错位等

---

### Step 5：开始 RL 训练（reward 组合不要太复杂）

建议先跑最简单可解释组合：

- `reward_mode="hard_plus_gtprob"`  
- 然后再加 `hard_plus_gtprob_plus_rel`，并让 `rel_weight` 很小（例如 0.05~0.2）

---

## 6. 你关心的点：是不是“数据不够好”还是“训练方案有问题”？

在你现在“正样本少、训练提升有限”的现象下，**优先级最高**的是把数据做到“可相信”：

- 一旦出现示例索引错位、评测 split 用错、fallback 混入、gen_args 不一致……  
  训练再好也学不出来（reward 噪声太大）。

你这次“重生成 + save-all + strict_eval + 自检”做完，至少能把问题明确分成两类：

1. **数据质量问题**：同一条 pointer candidate 的 `vqa_acc_score` 都算不稳/不一致（那就是 pipeline bug）
2. **任务可达性问题**：数据完全一致，但大多数 query 在你的 candidate pool 下就是找不到能答对的 ICDs  
   这时候才需要去做更强的正样本挖掘/探索（beam/采样/多轮搜索/扩 candidate pool）

---

## 7. 你接下来我建议你立刻做的“最小闭环”清单（按优先级）

1. ✅ **先把 candidate_pool 索引修正**（pos vs global id）
2. ✅ **保存 gt_answers_raw/norm + vqa_raw_generation +（可选）prompt_text**
3. ✅ **加入 relevance 指标并保存**
4. ✅ **strict_eval：禁用 fallback**
5. ✅ **输出 JSON 增加 `_meta` 并修改 dataset loader 跳过它**
6. ✅ **加本地复算 acc_score 的一致性校验**（生成阶段就 assert/报警）
7. 最后才是：尝试不同 reward 组合、不同探索策略、不同 shot_num 等训练层面的改进

---

## 附：你可以直接照着改的“关键伪代码片段”

### A. 在生成循环里写 candidate

```python
# pointer 是 position
pointer_pos = c["pointer"]
pointer_global = [candidate_indices[p] for p in pointer_pos]

ex1 = candidate_pool[pointer_pos[0]]
ex2 = candidate_pool[pointer_pos[1]]

out = build_vqa_prompt_and_generate(...)

pred = out["pred_answer"]          # postprocessed
raw  = out["raw_generation"]

# official acc
correct, acc_score, used_file_metric = compute_vqa_accuracy(...)
vqa_gt_prob = compute_expected_vqa_acc_prob(...)

# relevance
rel = compute_relevance(pred, gt_answers_norm)

c.update({
  "pointer_pos": pointer_pos,
  "pointer": pointer_global,
  "vqa_raw_generation": raw,
  "vqa_pred_answer": pred,
  "vqa_acc_score": acc_score,
  "vqa_correct": int(acc_score > 0.0),  # 或 acc_score==1.0，阈值你自己定
  "vqa_gt_prob": vqa_gt_prob,
  **rel,
  "vqa_eval_mode": "vqaEval",
  "eval_split_used": split_used,
})
```

### B. reward 组合（训练时）

```python
reward = w_hard*hard + w_prob*vqa_gt_prob + w_rel*(1-hard)*vqa_rel_score
```

---

> 如果你愿意，我也可以把“改动点对应的 patch（diff）”一并整理出来，方便你直接复制到仓库里。