# Lever-Plus v3 改进需求对比报告

> 对比 `LeverPlus_v3_RL_next_steps_2025_12_10.md` 中提出的改进需求与当前代码实现情况

---

## 1. 短期改进需求（3.1 - 3.5）

### ✅ 3.1 把 RCE-only 明确固化成 v3 默认训练方式

**需求：**
- 在 `grpo_post_train.py` 的命令行说明里，明确推荐默认配置：`--rce_epochs 5 --grpo_epochs 0`
- 在 `GRPOTrainer.train()` 里加保护：如果 `grpo_epochs <= 0`，打印提示并直接返回

**当前实现状态：**
- ❌ **未实现**
- `grpo_post_train.py` 第431行：`--grpo_epochs` 默认值是 `3`，不是 `0`
- 第432行：`--rce_epochs` 默认值是 `1`，不是 `5`
- 没有 `grpo_epochs <= 0` 时的保护逻辑

**建议修改：**
```python
# grpo_post_train.py 第430-431行
parser.add_argument("--rce_epochs", type=int, default=5, help="RCE预热epochs（推荐5）")
parser.add_argument("--grpo_epochs", type=int, default=0, help="GRPO训练epochs（推荐0，即RCE-only模式）")

# GRPOTrainer.train() 方法中，在 RCE 训练后添加：
if self.grpo_epochs <= 0:
    print("⚠️ GRPO epochs == 0，仅进行 RCE 预热，不执行 GRPO。")
    print("✓ RCE-only baseline 训练完成，当前模型即为最终模型。")
    return
```

---

### ❌ 3.2 SFT checkpoint 加载时增加一致性检查

**需求：**
- 在 `generate_rl_data.py` 和 `grpo_post_train.py` 中，加载 checkpoint 后检查 `missing` 和 `unexpected` 参数
- 如果缺失参数过多（>1000），直接 raise 错误

**当前实现状态：**
- ❌ **未实现**
- `generate_rl_data.py` 第88行：`model.load_state_dict(state_dict, strict=False)`，没有检查返回值
- `grpo_post_train.py` 第337、341、343行：同样没有检查返回值

**建议修改：**
```python
# generate_rl_data.py 第88行
missing, unexpected = model.load_state_dict(state_dict, strict=False)

if missing:
    print(f"[警告] 加载 checkpoint 时有 {len(missing)} 个参数缺失，例如：")
    for k in list(missing)[:10]:
        print(f"  - missing: {k}")

if unexpected:
    print(f"[警告] 有 {len(unexpected)} 个多余参数，例如：")
    for k in list(unexpected)[:10]:
        print(f"  - unexpected: {k}")

if len(missing) > 1000:
    raise RuntimeError("Checkpoint 与当前模型结构差异过大，请检查。")
```

---

### ❌ 3.3 在 RL JSON 中显式标记 reward 质量

#### 3.3.1 添加 `vqa_eval_mode` 字段

**需求：**
- 在 `generate_rl_data.py` 中，对每个 pointer 计算完 correctness 后，添加 `vqa_eval_mode` 字段
- 值为 `"file"`（官方 VQA metric）或 `"fallback"`（字符串匹配）

**当前实现状态：**
- ❌ **未实现**
- `generate_rl_data.py` 第586-588行：只添加了 `vqa_pred_answer`、`vqa_correct`、`vqa_acc_score`
- 没有添加 `vqa_eval_mode` 字段

**建议修改：**
```python
# generate_rl_data.py 第586-588行
c["vqa_pred_answer"] = pred_answer
c["vqa_correct"] = correct
c["vqa_acc_score"] = acc_score
c["vqa_eval_mode"] = "file" if used_file_metric else "fallback"  # 新增
```

#### 3.3.2 在 Dataset 中支持跳过 fallback 样本

**需求：**
- 在 `RLBeamDatasetWithEmbedding.__init__` 中增加 `skip_fallback_reward` 参数
- 在遍历 `pointer_candidates` 时，如果 `skip_fallback_reward=True` 且 `vqa_eval_mode=="fallback"`，则跳过该样本

**当前实现状态：**
- ❌ **未实现**
- `dataset_v3.py` 第350-382行：`__init__` 方法中没有 `skip_fallback_reward` 参数
- 第411-425行：遍历 `pointer_candidates` 时没有过滤 fallback 样本的逻辑

**建议修改：**
```python
# dataset_v3.py 第350行，在 __init__ 参数中添加：
def __init__(
    ...,
    skip_fallback_reward: bool = False,  # 新增
):
    ...
    self.skip_fallback_reward = skip_fallback_reward  # 新增

# dataset_v3.py 第411行，在遍历时添加过滤：
for c in pointer_candidates:
    # 如果要求跳过 fallback 样本
    if self.skip_fallback_reward and c.get("vqa_eval_mode") == "fallback":
        continue  # 新增
    
    pointer = c["pointer"]
    ...
```

#### 3.3.3 在训练脚本中暴露开关

**需求：**
- 在 `grpo_post_train.py` 的 argparse 中增加 `--skip_fallback_reward` 参数
- 在构造 dataset 时传入该参数

**当前实现状态：**
- ❌ **未实现**
- `grpo_post_train.py` 第449行：argparse 中没有 `--skip_fallback_reward` 参数
- 需要找到 dataset 构造的位置并传入参数

---

### ⚠️ 3.4 RCE 阶段使用原始 reward（部分实现）

**需求：**
- 在 `GRPOTrainer` 中增加 `rce_use_raw_reward` 开关（CLI 参数 `--rce_use_raw_reward`）
- 在 `train_rce_epoch` 中根据开关决定使用 `beam_rewards_raw` 还是 `beam_rewards`

**当前实现状态：**
- ⚠️ **部分实现**
- `grpo_post_train.py` 第171行：`train_rce_epoch` 中已经使用了 `beam_rewards_raw`（原始 reward）
- ❌ 但没有开关控制，无法切换到归一化后的 reward 进行对比实验

**建议修改：**
```python
# grpo_post_train.py argparse 中添加：
parser.add_argument(
    "--rce_use_raw_reward",
    action="store_true",
    default=True,  # 默认使用 raw reward（当前行为）
    help="RCE 训练时使用原始 reward（True）还是归一化后的 reward（False）"
)

# GRPOTrainer.__init__ 中添加：
self.rce_use_raw_reward = args.rce_use_raw_reward

# train_rce_epoch 第171行修改为：
if self.rce_use_raw_reward:
    reward_for_rce = batch["beam_rewards_raw"].to(self.device)
else:
    reward_for_rce = batch["beam_rewards"].to(self.device)

loss = self.model.compute_rce_loss(
    query_emb, cand_emb, beam_labels, reward_for_rce,  # 使用 reward_for_rce
    temperature=temperature,
    use_top1_only=self.use_top1_only
)
```

---

### ❌ 3.5 GRPO 阶段：减小"破坏力"

#### 3.5.1 降低默认学习率和训练轮数

**需求：**
- `grpo_epochs` 默认值改为 1~3（而不是 8 或 25）
- `grpo_lr` 比 RCE 再小一档（例如 `3e-5` 或 `1e-5`）

**当前实现状态：**
- ⚠️ **部分实现**
- `grpo_post_train.py` 第431行：`--grpo_epochs` 默认值是 `3`（✅ 符合要求）
- 第434行：`--grpo_lr` 默认值是 `1e-5`（✅ 符合要求，比 RCE 的 `1e-4` 小一档）

**结论：** ✅ **已实现**

#### 3.5.2 限制更新的参数范围（冻结 backbone）

**需求：**
- 在 `grpo_post_train.py` 中增加 `--freeze_backbone_in_grpo` 参数
- 如果开启，GRPO 时只训练 pointer head，冻结 backbone

**当前实现状态：**
- ❌ **未实现**
- `grpo_post_train.py` 中没有 `--freeze_backbone_in_grpo` 参数
- 没有参数冻结的逻辑

**建议修改：**
```python
# grpo_post_train.py argparse 中添加：
parser.add_argument(
    "--freeze_backbone_in_grpo",
    action="store_true",
    help="GRPO 时只训练 pointer head，冻结 backbone"
)

# GRPOTrainer.train() 方法中，在 GRPO 训练前添加：
if self.grpo_epochs > 0 and args.freeze_backbone_in_grpo:
    print("冻结 backbone，GRPO 时只训练 pointer head...")
    for name, param in self.model.named_parameters():
        # 根据实际模型结构调整，这里假设有 pointer_head 相关的层
        if "pointer_head" in name or "cross_attn" in name:
            param.requires_grad = True
        else:
            param.requires_grad = False
```

---

## 2. 中期改进需求（4.1 - 4.3）

### ❌ 4.1 针对不同 shot_num 单独生成 RL 数据 & 训练

**需求：**
- 在 `generate_rl_data.py` 中增加 `--shot_num` 参数
- 支持生成多份 RL 数据（2-shot / 3-shot / 4-shot）
- 在训练脚本中支持加载多份 RL JSON 并联合训练

**当前实现状态：**
- ❌ **未实现**
- 当前 RL 数据生成和训练都只支持 `shot_num=2`

---

### ❌ 4.2 更系统地探索 reward_mode

**需求：**
- 设计网格实验：`reward_mode × (hard_weight, soft_weight) × rce_use_raw_reward`
- 先在 RCE-only 场景下确定最佳配置

**当前实现状态：**
- ⚠️ **部分实现**
- 代码已经支持多种 `reward_mode`（`hard_plus_soft`, `hard_plus_soft_v2`, `separated`, `hard_only`, `soft_only`, `hybrid`, `legacy`）
- 但没有系统性的网格实验脚本

---

### ❌ 4.3 Hybrid：让 InfoScore 回来当"辅助信号"

**需求：**
- 在 `reward_utils.py` 中完善 `hybrid` 模式的实现
- 让 InfoScore 作为辅助信号，而不是主角

**当前实现状态：**
- ⚠️ **部分实现**
- `reward_utils.py` 中已经有 `hybrid` 模式的基础实现
- 但需要验证和完善

---

## 3. 总结

### 已实现的改进

1. ✅ **RCE 训练使用原始 reward**（3.4，部分实现）
   - 当前代码已经在使用 `beam_rewards_raw`，但没有开关控制

2. ✅ **GRPO 默认学习率和训练轮数**（3.5.1）
   - `grpo_epochs` 默认值已经是 `3`
   - `grpo_lr` 默认值已经是 `1e-5`（比 RCE 的 `1e-4` 小一档）

3. ✅ **严格指针映射检查**（已在代码中实现）
   - `dataset_v3.py` 第418-424行：已经实现了严格的指针索引检查

### 未实现的改进（优先级排序）

#### 高优先级（建议立即实现）

1. ❌ **3.1 把 RCE-only 明确固化成默认训练方式**
   - 修改默认参数：`--rce_epochs 5 --grpo_epochs 0`
   - 添加 `grpo_epochs <= 0` 时的保护逻辑

2. ❌ **3.2 SFT checkpoint 加载时增加一致性检查**
   - 检查 `missing` 和 `unexpected` 参数
   - 如果缺失过多，直接 raise 错误

3. ❌ **3.3.1 添加 `vqa_eval_mode` 字段**
   - 在 RL JSON 中标记 reward 质量来源

4. ❌ **3.3.2 支持跳过 fallback 样本**
   - 在 Dataset 中添加 `skip_fallback_reward` 参数
   - 在训练脚本中暴露开关

#### 中优先级（建议近期实现）

5. ⚠️ **3.4 增加 `rce_use_raw_reward` 开关**
   - 虽然当前已经使用 raw reward，但需要开关以便对比实验

6. ❌ **3.5.2 支持冻结 backbone**
   - 添加 `--freeze_backbone_in_grpo` 参数
   - 实现参数冻结逻辑

#### 低优先级（中期规划）

7. ❌ **4.1 多 shot_num 支持**
8. ❌ **4.2 系统性 reward_mode 探索**
9. ❌ **4.3 Hybrid 模式完善**

---

## 4. 建议执行顺序

1. **第一步**：实现 3.1（RCE-only 默认配置）+ 3.2（checkpoint 检查）
2. **第二步**：实现 3.3（vqa_eval_mode + skip_fallback_reward）
3. **第三步**：实现 3.4（rce_use_raw_reward 开关）+ 3.5.2（freeze_backbone）
4. **第四步**：规划中期改进（4.1 - 4.3）

---

**生成时间：** 2025-12-10  
**对比文档：** `LeverPlus_v3_RL_next_steps_2025_12_10.md`







