# 方案五 vs 方案六对比总结

> 日期：2025-12-23  
> 对比：方案五（GRPO_LR=5e-6）vs 方案六（GRPO_LR=5e-5）

---

## 一、训练配置对比

| 配置项 | 方案五 | 方案六 |
|--------|--------|--------|
| **USE_RANK_ADVANTAGE** | false | false |
| **GRPO_LR** | 5e-6 | **5e-5**（提升 10 倍） |
| **KL_BETA** | 0.1 | 0.1 |
| **GRPO_EPOCHS** | 50 | 50 |
| **最优 Checkpoint** | Epoch 2 | Epoch 2 |

---

## 二、训练指标对比

| 指标 | 方案五（Epoch 2） | 方案六（Epoch 2） | 差异 |
|------|------------------|------------------|------|
| **Val Loss** | **5.14132** | 5.15152 | ⬆️ +0.01020（方案五更好） |
| **PPO Loss** | -0.00007 | **0.00005** | ⬆️ 方案六更合理（正值） |
| **KL** | 0.04286 | **0.03565** | ⬇️ 方案六更小（更好） |
| **Adv Std** | 0.0062 | 0.0062 | ➡️ 相同 |
| **Adv Max** | 0.0090 | 0.0090 | ➡️ 相同 |

**关键发现**：
- ✅ **方案五的 Val Loss 更小**（5.14132 vs 5.15152）
- ✅ **方案六的 PPO Loss 更合理**（正值 vs 负值）
- ⚠️ **Adv Std 仍然很小**（两个方案都是 0.0062），学习率增大并没有改善 advantage 信号

---

## 三、推理结果对比（800 samples）

### 3.1 完整对比表

| Shot | Baseline | 方案五 | 方案六 | 方案五 vs Baseline | 方案六 vs Baseline | 方案六 vs 方案五 |
|------|----------|--------|--------|-------------------|-------------------|------------------|
| **1** | 48.55% | **50.15%** | 49.9% | **+1.60%** ⬆️ | +1.35% ⬆️ | **-0.25%** ⬇️ |
| **2** | 47.75% | **48.33%** | 48.23% | **+0.58%** ⬆️ | +0.48% ⬆️ | **-0.10%** ⬇️ |
| **3** | 48.15% | 47.40% | **47.48%** | -0.75% ⬇️ | -0.67% ⬇️ | **+0.08%** ⬆️ |
| **4** | 47.45% | 47.52% | **47.60%** | +0.07% ⬆️ | +0.15% ⬆️ | **+0.08%** ⬆️ |
| **平均** | 47.73% | **47.85%** | 47.80% | **+0.38%** ⬆️ | +0.33% ⬆️ | **-0.05%** ⬇️ |

### 3.2 关键发现

1. **方案五在关键场景表现更好**：
   - Shot 1：方案五 +1.60% vs 方案六 +1.35%（**+0.25%**）
   - Shot 2：方案五 +0.58% vs 方案六 +0.48%（**+0.10%**）

2. **方案六在次要场景略好**：
   - Shot 3：方案六 -0.67% vs 方案五 -0.75%（**+0.08%**）
   - Shot 4：方案六 +0.15% vs 方案五 +0.07%（**+0.08%**）

3. **平均提升方案五更好**：
   - 方案五：**+0.38%**
   - 方案六：+0.33%
   - **差异：-0.05%**（方案五更好）

---

## 四、综合分析

### 4.1 方案五的优势

1. ✅ **平均提升更高**：+0.38% vs +0.33%
2. ✅ **关键场景表现更好**：Shot 1 和 Shot 2 都更好
3. ✅ **Val Loss 更小**：5.14132 vs 5.15152
4. ✅ **训练更稳定**：学习率较小，不容易过拟合

### 4.2 方案六的优势

1. ✅ **PPO Loss 更合理**：正值（0.00005）vs 负值（-0.00007）
2. ✅ **KL 更小**：0.03565 vs 0.04286
3. ✅ **次要场景略好**：Shot 3 和 Shot 4 略好

### 4.3 方案六的问题

1. ⚠️ **平均提升略差**：+0.33% vs +0.38%（-0.05%）
2. ⚠️ **关键场景表现略差**：Shot 1 和 Shot 2 都略差
3. ⚠️ **Val Loss 略差**：5.15152 vs 5.14132（+0.01020）
4. ⚠️ **Adv Std 仍然很小**：学习率增大并没有改善 advantage 信号

---

## 五、结论与建议

### 5.1 最终结论

**方案五是最优方案**：
- ✅ 平均提升更高（+0.38% vs +0.33%）
- ✅ 关键场景表现更好（Shot 1 和 Shot 2）
- ✅ Val Loss 更小（5.14132 vs 5.15152）
- ✅ 训练更稳定

**方案六不推荐**：
- ⚠️ 学习率增大后效果反而略降
- ⚠️ 虽然 PPO Loss 更合理，但推理效果不如方案五
- ⚠️ Adv Std 仍然很小，学习率增大并没有解决根本问题

### 5.2 可能的原因

1. **学习率过大导致训练不稳定**：
   - 虽然 PPO Loss 更合理（正值），但 Val Loss 略差
   - 模型可能更容易过拟合

2. **Adv Std 仍然很小**：
   - 两个方案的 Adv Std 都是 0.0062
   - 学习率增大只是放大了噪声，并没有改善 advantage 信号

3. **根本问题未解决**：
   - Advantage 信号弱的问题没有解决
   - 需要 Reward Shaping 或其他方案来增大 reward 差异

### 5.3 后续建议

1. ✅ **使用方案五作为最终模型**：
   - Checkpoint：`grpo_epoch2_v2format.ckpt`（方案五）
   - 平均提升：+0.38%（vs Baseline）

2. 🔄 **尝试方案 5 + 方案 8**（Reward Shaping）：
   - 让负样本也有梯度信号
   - 可能进一步增大 reward 差异，从而改善 advantage 信号

3. 🔄 **如果继续优化学习率**：
   - 可以尝试中间值（如 2e-5 或 1e-5）
   - 但当前结果看，学习率增大并没有带来预期效果

4. 📊 **继续监控 Adv Std**：
   - 如果仍然很小，可能需要 Reward Shaping 或其他方案
   - 单纯增大学习率无法解决 advantage 信号弱的问题

---

## 六、关键指标汇总

| 指标 | 方案五 | 方案六 | 最优 |
|------|--------|--------|------|
| **平均提升** | **+0.38%** | +0.33% | ✅ 方案五 |
| **Shot 1 提升** | **+1.60%** | +1.35% | ✅ 方案五 |
| **Shot 2 提升** | **+0.58%** | +0.48% | ✅ 方案五 |
| **Val Loss** | **5.14132** | 5.15152 | ✅ 方案五 |
| **PPO Loss** | -0.00007 | **0.00005** | ⚠️ 方案六 |
| **KL** | 0.04286 | **0.03565** | ⚠️ 方案六 |
| **Adv Std** | 0.0062 | 0.0062 | ➡️ 相同 |

**最终推荐**：✅ **方案五**（GRPO_LR=5e-6）

---

*分析时间：2025-12-23*


